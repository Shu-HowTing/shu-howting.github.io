<!doctype html>
<html lang="zh-CN">
<head>
	<meta name="generator" content="Hugo 0.114.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    

    <title>White | Stay foolish, Stay hungry!</title>
    <meta property="og:title" content="White | Stay foolish, Stay hungry!">
    <meta property="og:type" content="website">
    <meta name="Keywords" content="DS, DeepLearning">
    <meta name="description" content="">
    <meta property="og:url" content="https://whiteding.fun/">
    <link rel="shortcut icon" href='/favicon.ico'  type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <link rel="alternate" type="application/rss+xml" href="https://whiteding.fun/index.xml" title="White" />
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
        <link rel="stylesheet" href='/css/douban.css'>
    
        <link rel="stylesheet" href='/css/other.css'>
    

    
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@waline/client@v2/dist/waline.css">
        
    


</head>

<body>
    
<header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <h1>
                        <a id="logo" href="https://whiteding.fun/">
                            White
                        </a>
                    </h1>
                
                <p class="description">Stay foolish, Stay hungry!</p>
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class=" current" href="https://whiteding.fun/"> <i class="fas fa-home"></i> Home</a>
                    
                    <a  href="/categories/" title="&lt;i class=&#39;fas fa-layer-group&#39;&gt;&lt;/i&gt; 分类" > <i class='fas fa-layer-group'></i> 分类</a>
                    
                    
                    <a  href="/tags/" title="&lt;i class=&#39;fas fa-tag&#39;&gt;&lt;/i&gt; 标签" > <i class='fas fa-tag'></i> 标签</a>
                    
                    
                    <a  href="/archives/" title="&lt;i class=&#39;fas fa-archive&#39;&gt;&lt;/i&gt; 归档" > <i class='fas fa-archive'></i> 归档</a>
                    
                    
                    <a  href="/about/" title="&lt;i class=&#39;fas fa-user&#39;&gt;&lt;/i&gt; 关于" > <i class='fas fa-user'></i> 关于</a>
                    
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/recsys/seq_feat/" title="推荐算法中的序列特征处理" target="_blank">推荐算法中的序列特征处理</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2022-02-08T00:00:00Z" class="post-meta meta-date dt-published">
    2022-02-08
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/RecSys' target="_blank">RecSys</a>
  
</div>


        <div class="post-content">
            在推荐领域中，行为序列特征是一种极为重要的特征。近年来，出现了很多有关行为序列特征建模的论文，研究如何将行为序列特征应用到推荐场景中，以更好挖掘用户的历史兴趣。本文将带大家梳理介绍这些论文中提出的方法。 序列特征 序列特征通常表现为时间上的跨度，具有很强的时间先后关系。如何在行为序列中挖掘用户兴趣的多样性以及实效性，是序列特模型研究的重点。 序列特征处理方法 本文将聚焦于$Pooling、attentio……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/recsys/seq_feat/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/deep_learning/transformer%E7%9A%84%E7%BB%86%E8%8A%82%E7%90%86%E8%A7%A3/" title="Transformer的细节" target="_blank">Transformer的细节</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-12-10T00:00:00Z" class="post-meta meta-date dt-published">
    2021-12-10
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/deep-learning' target="_blank">deep learning</a>
  
</div>


        <div class="post-content">
            Transformer中的几个细节讨论 1. 为什么self-attention中需要$/\sqrt{d}$ 在自注意力（self-attention）机制中，将查询（Query, Q）与键（Key, K）相乘之后除以($\sqrt{d}$)，其中d是键向量的维度，这是为了稳定梯度和防止数值不稳定。 具体原因如下： 避免数值过大：在没有缩放的情况下，Q和K的点积结果会随着维度$d$的增加而变得很大。点积的结果会随……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/deep_learning/transformer%E7%9A%84%E7%BB%86%E8%8A%82%E7%90%86%E8%A7%A3/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/machine_learning/softmax%E8%AE%A1%E7%AE%97%E4%BC%98%E5%8C%96/" title="softmax计算优化" target="_blank">softmax计算优化</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-12-01T00:00:00Z" class="post-meta meta-date dt-published">
    2021-12-01
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/machine-learning' target="_blank">machine learning</a>
  
</div>


        <div class="post-content">
            softmax上溢和下溢问题 解决这个问题的方法就是利用softmax的冗余性。我们可以看到对于任意一个数$a$, $x-a$和$x$在$softmax$中的结果都是一样的。 $$ \frac{\exp^{(x-a)}}{\sum_{i=1}^k \exp_i^{(x-a)}}=\frac{\exp ^{(x)} \exp ^{(-a)}}{\exp ^{(-a)} \sum_{i=1}^k \exp _i^{(x)}}=\frac{\exp ^{(x)}}{\sum_{i=1}^k \exp_i^{(x)}} $$ 对于一组输入，我们可以让a=max(x). 这样就可以保证x-a的最大值等于0，也就不会产生上溢的问题。同时，因为$x-a=0$, 所以$exp(0)=1$,分母就不可能为0。 $$ \begin{array}{l} \log \left(\frac{\exp^{(x-a)}}{\sum_{i=1}^k \exp_i^{(x-a)}}\right) &amp;=\log \left(e^{(x-a)}\right)-\log \left(\sum_{i=1}^k \exp_i^{(x-a)}\right) \\ &amp;=(x-a)-\log \left(\sum_{i=1}^k \exp_i^{(x-a)}\right) \end{array} $$……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/machine_learning/softmax%E8%AE%A1%E7%AE%97%E4%BC%98%E5%8C%96/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/deep_learning/transformer%E5%85%A5%E9%97%A8/" title="Transformer模型理解" target="_blank">Transformer模型理解</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-11-15T00:00:00Z" class="post-meta meta-date dt-published">
    2021-11-15
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/deep-learning' target="_blank">deep learning</a>
  
</div>


        <div class="post-content">
            Transformer模型在2017年被google提出，直接基于Self-Attention结构，并且迅速取代了之前NLP任务中常用的RNN神经网络结构，成为主流。本文将探讨关于transformer模型的实现细节 Transformer Encoder Self-attention $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$ Transformer 中token的输入表示$a$由$Word\ Embedding$ 和位置 $Positional\ Encoding$ 相加得到。 Add &amp; Norm Add &amp; Norm 层由 Add 和 Norm 两部分组成，其计算公式如下： $$\textit{LayerNorm}\big(X+\text{MultiHeadAttention}(X)\big)$$ Feed Forward Feed Forward 层比较简单，是一个两层的全连接层，第一……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/deep_learning/transformer%E5%85%A5%E9%97%A8/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/machine_learning/XGBoost%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/" title="XGBoost的原理分析以及实践" target="_blank">XGBoost的原理分析以及实践</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-09-01T00:00:00Z" class="post-meta meta-date dt-published">
    2021-09-01
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/machine-learning' target="_blank">machine learning</a>
  
</div>


        <div class="post-content">
            XGBoost算法 原理 任何机器学习的问题都可以从目标函数(objective function)出发，目标函数的主要由两部分组成 $损失函数+正则项$： $$ Obj(\Theta)=L(\Theta)+\Omega(\Theta) $$ 在这里，当选择树模型为基学习器时，需要正则的对象，或者说需要控制复杂度的对象就是这K颗树,通常树的参数有树的深度，叶子节点的个数，叶子节点值的取值(xgboost里称为权重weight)。 所以，我们的目标函数形式如下： $$ \mathcal{L}=\sum_{i} l\left(\hat{y_i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right) $$ 这里前一半代表预……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/machine_learning/XGBoost%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/machine_learning/%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/" title="模型特征重要性的计算" target="_blank">模型特征重要性的计算</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-07-15T00:00:00Z" class="post-meta meta-date dt-published">
    2021-07-15
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/machine-learning' target="_blank">machine learning</a>
  
</div>


        <div class="post-content">
            深度学习的兴起，使得各种复杂的NN网络应用变得流行。但是，对于这些黑盒的模型，我们一般很难知晓哪些特征对模型的学习比较重要, 即对缺乏特征重要性的解释。这里,我们会介绍一些主流的方法，来计算模型特征的重要性。 Tree_base 树模型的解释性一般要优于NN模型，因为书模型的学习是可解释的，大多数Tree模型也都带有查看特征重要性的接口，以xgboost为例: xgboost如何用于特征选择: 缺点: 无法迁移到NN模型上。……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/machine_learning/%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/machine_learning/FM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" title="FM算法原理" target="_blank">FM算法原理</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-06-14T00:00:00Z" class="post-meta meta-date dt-published">
    2021-06-14
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/machine-learning' target="_blank">machine learning</a>
  
</div>


        <div class="post-content">
            FM的提出 LR为普通的线性模型，优点是复杂度低、方便求解，但缺点也很明显，没有考虑特征之间的交叉，表达能力有限。 $$ y=\omega_0+\sum_{i=1}^n \omega_i x_i $$ FM在线性模型的基础上添加了一个多项式，用于描述特征之间的二阶交叉: $$ y=\omega_0+\sum_{i=1}^n \omega_i x_i+\sum_{i=1}^{n-1} \sum_{j=i+1}^n \omega_{i j} x_i x_j $$ 其中，$n$代表样本的特征数量，$x_i$是第$i$个特征的值， $w_0, w_i, w_{ij}$是模型参数。 问题 参数 $w_{i j}$ 学习困难, 因为对 $w_{i j}$ 进行更新时, 求得的梯度对应为 $x_i x_j$, 当且仅当 $x_i$ 与 $x_j$ 都非0时参数才会得……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/machine_learning/FM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/statistics/CUPED%E6%96%B9%E5%B7%AE%E7%BC%A9%E5%87%8F/" title="CUPED方差缩减" target="_blank">CUPED方差缩减</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-06-06T00:00:00Z" class="post-meta meta-date dt-published">
    2021-06-06
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/statistics' target="_blank">statistics</a>
  
</div>


        <div class="post-content">
            对于成熟的产品来说，大多数的改动带来的提升可能都是微小的， 通常情况下，为提升AB实验的灵敏度，提升AB的显著性，有两种常见做法： 增加流量 增长实验时间 本质上，无论是延长实验时间还是增加流量一方面都是为了增加样本量，因为样本越多，方差越小，p值越显著，越容易检测出一些微小的改进。 如果能合理的通过统计方法降低方差，就可能更快，更少的样本的检测到微小的效果提升! 微软2013年发表过一篇论文，介绍了一种利用……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/statistics/CUPED%E6%96%B9%E5%B7%AE%E7%BC%A9%E5%87%8F/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/deep_learning/softmax%E4%B8%AD%E6%B8%A9%E5%BA%A6%E7%B3%BB%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8/" title="对比损失中温度系数的作用" target="_blank">对比损失中温度系数的作用</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-05-15T00:00:00Z" class="post-meta meta-date dt-published">
    2021-05-15
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/deep-learning' target="_blank">deep learning</a>
  
</div>


        <div class="post-content">
            温度系数 对比损失（Contrastive Loss）中的参数$\tau$是一个神秘的参数，大部分论文都默认采用较小的值来进行自监督对比学习（例如 $\tau = 0.05$），但是很少有文章详细讲解参数$\tau$的作用，本文将详解对比损失中的超参数 ，并借此分析对比学习的核心机制。 首先总结下本文的发现： 对比损失是一个具备困难负样本自发现性质的损失函数，这一性质对于学习高质量的自监督表示是至关重要的。关注困难样本的……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/deep_learning/softmax%E4%B8%AD%E6%B8%A9%E5%BA%A6%E7%B3%BB%E6%95%B0%E7%9A%84%E4%BD%9C%E7%94%A8/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://whiteding.fun/post/statistics/Beta%E5%88%86%E5%B8%83%E5%92%8CThompson%E9%87%87%E6%A0%B7/" title="Beta分布和Thompson采样" target="_blank">Beta分布和Thompson采样</a>
            </h1>
        </header>
        
  <i class="far fa-calendar-alt fa-sm"></i> 
  <time datetime="2021-03-06T00:00:00Z" class="post-meta meta-date dt-published">
    2021-03-06
  </time>


<div class="post-meta meta-category">
  <span>&nbsp;|</span>
  
    <i class="far fa-folder fa-sm"></i> 
    <a href='/categories/statistics' target="_blank">statistics</a>
  
</div>


        <div class="post-content">
            $Beta$分布 $Beta$分布是一个定义在[0,1]区间上的连续概率分布族，它有两个正值参数，称为形状参数，一般用$\alpha$和$\beta$表示 $Beta$分布的概率密度为： $$ f(x ; \alpha, \beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} $$ 随机变量$X$服从参数为$\alpha, \beta$的$beta$分布，一般记作： $$ X \sim \operatorname {Beta} (\alpha, \beta) $$ $Beta$分布的期望： $$ \frac{\alpha}{\alpha + \beta} $$ $Beta$分布的方差： $$ \frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)} $$ $Beta$分布的概率密度图……
        </div>
        <p class="readmore"><a href="https://whiteding.fun/post/statistics/Beta%E5%88%86%E5%B8%83%E5%92%8CThompson%E9%87%87%E6%A0%B7/" target="_blank">阅读全文 <i class="fas fa-chevron-right fa-sm"></i> </a></p>
    </article>
    
    
    



<ol class="page-navigator">
    
    <li class="prev">
        <a href="https://whiteding.fun/">Previous</a>
    </li>
    

    
    <li >
        <a href="https://whiteding.fun/">1</a>
    </li>
    
    <li  class="current">
        <a href="https://whiteding.fun/page/2/">2</a>
    </li>
    
    <li >
        <a href="https://whiteding.fun/page/3/">3</a>
    </li>
    
    <li >
        <a href="https://whiteding.fun/page/4/">4</a>
    </li>
    

    
    <li class="next">
        <a href="https://whiteding.fun/page/3/">Next</a>
    </li>
    
</ol>



</div>

                    <footer id="footer">
    <div>
        &copy; 2024 <a href="https://whiteding.fun/"> By whiteding</a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>












    <script src='/js/douban.js'></script>




    <script src="https://unpkg.com/@waline/client@v2/dist/waline.mjs"></script>

                </div>

                <div id="secondary">
    
    

    <section class="widget">
        
<div class="avatar-widget">
    <img src="/img/avatar3.png" alt="Avatar" class="avatar-image">

    <p class="avatar-slogan">What's past is prologue.</p>
    <div class="widget-list" style="display: flex" >
        
        <li>
            <a class="info-icon" href="mailto:white_ding@163.com" title="email" target="_blank" style="margin-inline:5px">
                <i class="fa fa-envelope-square fa-lg" style="margin-inline:8px"></i>
            </a>
        </li>
        
        <li>
            <a class="info-icon" href="https://www.linkedin.com/in/white-ding-7b5151a7/" title="linkedin" target="_blank" style="margin-inline:5px">
                <i class="fab fa-linkedin fa-lg" style="margin-inline:8px"></i>
            </a>
        </li>
        
        <li>
            <a class="info-icon" href="https://github.com/Shu-HowTing" title="github" target="_blank" style="margin-inline:5px">
                <i class="fab fa-github fa-lg" style="margin-inline:8px"></i>
            </a>
        </li>
        
    </div>
</div>



    </section>

    <section class="widget">
        <h3 class="widget-title"> <i class="far fa-file"></i> Latest articles</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E9%95%BF%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/" title="长序列建模" target="_blank">长序列建模</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E5%BD%93%E6%8E%A8%E8%8D%90%E9%81%87%E5%88%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="当推荐遇到大模型" target="_blank">当推荐遇到大模型</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/Batch%E8%B4%9F%E9%87%87%E6%A0%B7/" title="Batch内负采样" target="_blank">Batch内负采样</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E5%8A%A8%E6%80%81%E6%9D%83%E9%87%8D/" title="动态权重在推荐中的应用" target="_blank">动态权重在推荐中的应用</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/POSO%E5%86%B7%E5%90%AF%E5%8A%A8/" title="POSO冷启动" target="_blank">POSO冷启动</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89/" title="推荐中的特征交叉技术" target="_blank">推荐中的特征交叉技术</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/machine_learning/ordinal_regression/" title="Ordinal Regression" target="_blank">Ordinal Regression</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0/" title="召回模型的评估" target="_blank">召回模型的评估</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E5%A4%9A%E5%85%B4%E8%B6%A3%E5%8F%AC%E5%9B%9E%E6%8E%A8%E8%8D%90/" title="多兴趣召回推荐" target="_blank">多兴趣召回推荐</a>
    </li>
    
    <li>
        <a href="https://whiteding.fun/post/recsys/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E4%BC%98%E5%8C%96/" title="多任务loss优化" target="_blank">多任务loss优化</a>
    </li>
    
</ul>
    </section>

     
    

    <section class="widget">
        <h3 class="widget-title"><a href='/categories/'> <i class="far fa-folder"></i> Categories</a></h3>
<ul class="widget-list">
    
    <li><a href="https://whiteding.fun/categories/RecSys/">RecSys (10)</a></li>
    
    <li><a href="https://whiteding.fun/categories/deep-learning/">deep learning (3)</a></li>
    
    <li><a href="https://whiteding.fun/categories/flink/">flink (1)</a></li>
    
    <li><a href="https://whiteding.fun/categories/kafka/">kafka (1)</a></li>
    
    <li><a href="https://whiteding.fun/categories/machine-learning/">machine learning (5)</a></li>
    
    <li><a href="https://whiteding.fun/categories/spark/">spark (12)</a></li>
    
    <li><a href="https://whiteding.fun/categories/statistics/">statistics (2)</a></li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href='/tags/'> <i class="fas fa-tag"></i> Tags</a></h3>
<div class="tagcloud">
    
    <a href="https://whiteding.fun/tags/Beta%E5%88%86%E5%B8%83/">Beta分布</a>
    
    <a href="https://whiteding.fun/tags/CUPED/">CUPED</a>
    
    <a href="https://whiteding.fun/tags/FM/">FM</a>
    
    <a href="https://whiteding.fun/tags/Regression/">Regression</a>
    
    <a href="https://whiteding.fun/tags/Thompson/">Thompson</a>
    
    <a href="https://whiteding.fun/tags/flink/">flink</a>
    
    <a href="https://whiteding.fun/tags/kafka/">kafka</a>
    
    <a href="https://whiteding.fun/tags/multi-task/">multi-task</a>
    
    <a href="https://whiteding.fun/tags/rank/">rank</a>
    
    <a href="https://whiteding.fun/tags/softmax/">softmax</a>
    
    <a href="https://whiteding.fun/tags/spark/">spark</a>
    
    <a href="https://whiteding.fun/tags/transformer/">transformer</a>
    
    <a href="https://whiteding.fun/tags/xgb/">xgb</a>
    
    <a href="https://whiteding.fun/tags/%E5%86%B7%E5%90%AF%E5%8A%A8/">冷启动</a>
    
    <a href="https://whiteding.fun/tags/%E5%8F%AC%E5%9B%9E/">召回</a>
    
    <a href="https://whiteding.fun/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a>
    
    <a href="https://whiteding.fun/tags/%E5%BA%8F%E5%88%97%E7%89%B9%E5%BE%81/">序列特征</a>
    
    <a href="https://whiteding.fun/tags/%E6%8E%A8%E8%8D%90/">推荐</a>
    
    <a href="https://whiteding.fun/tags/%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89/">特征交叉</a>
    
    <a href="https://whiteding.fun/tags/%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7/">特征重要性</a>
    
    <a href="https://whiteding.fun/tags/%E9%95%BF%E5%BA%8F%E5%88%97/">长序列</a>
    
</div>
    </section>

    
<section class="widget">
    <h3 class="widget-title"> <i class="fas fa-external-link-alt"></i> Links</h3>
    <ul class="widget-list">
        
        <li>
            <a target="_blank" href="https://www.instagram.com/whiteding94/" title="instagram"> <i class="fab fa-instagram fa-lg"></i> Instagram</a>
        </li>
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/ding-shu-hao" title="blog"> <i class="far fa-comment fa-lg"></i> 知乎</a>
        </li>
        
    </ul>
</section>


    
</div>

            </div>
        </div>
    </div>

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</body>

</html>