<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on White</title>
    <link>https://whiteding.fun/post/</link>
    <description>Recent content in Posts on White</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 01 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://whiteding.fun/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>当推荐遇到大模型</title>
      <link>https://whiteding.fun/post/recsys/llm/</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/recsys/llm/</guid>
      <description>自从大语言模型爆火之后，大家对大语言模型（LLM）如何成功应用在推荐系统进行了不少尝试。本文是对目前一些业界工作的调研和总结。 大模型应用范式 经典的推荐架构基本遵循以下范式： 目前, LLM 在推荐系统中的主流应用可以分为两种范式: 一个是作为经典推荐系统的辅助部分，即 LLM+RS。 一个是 LLM 单独作为一个完整的推荐系统，即 LLM AS RS。 本文接下来将分别介绍这两种应用方式。 LLM+RS 传统推荐系统经过多年发展，从召回、排序、重排</description>
    </item>
    <item>
      <title>推荐算法中的序列特征模型</title>
      <link>https://whiteding.fun/post/recsys/seq_feat/</link>
      <pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/recsys/seq_feat/</guid>
      <description>在推荐领域中，行为序列特征是一种极为重要的特征。近年来，出现了很多有关行为序列特征建模的论文，研究如何将行为序列特征应用到推荐场景中，以更好挖掘用户的历史兴趣。本文将带大家梳理介绍这些论文中提出的方法。 序列特征 序列特征通常表现为时间上的跨度，具有很强的时间先后关系。如何在行为序列中挖掘用户兴趣的多样性以及实效性，是序列特模型研究的重点。 序列特征模型 按时间来看，推荐算法中的序列特征模型经历了$Poo</description>
    </item>
    <item>
      <title>Beta分布和Thompson采样</title>
      <link>https://whiteding.fun/post/statistics/Beta%E5%88%86%E5%B8%83%E5%92%8CThompson%E9%87%87%E6%A0%B7/</link>
      <pubDate>Sun, 06 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/statistics/Beta%E5%88%86%E5%B8%83%E5%92%8CThompson%E9%87%87%E6%A0%B7/</guid>
      <description>$Beta$分布 $Beta$分布是一个定义在[0,1]区间上的连续概率分布族，它有两个正值参数，称为形状参数，一般用$\alpha$和$\beta$表示 $Beta$分布的概率密度为： $$ f(x ; \alpha, \beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1} $$ 随机变量$X$服从参数为$\alpha, \beta$的$beta$分布，一般记作： $$ X \sim \operatorname {Beta} (\alpha, \beta) $$ $Beta$分布的期望： $$ \frac{\alpha}{\alpha + \beta} $$ $Beta$分布的方差： $$ \frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)} $$ $Beta$分布的概率密度图</description>
    </item>
    <item>
      <title>cache和persist比较</title>
      <link>https://whiteding.fun/post/spark/cache%E7%AE%97%E5%AD%90%E5%92%8Cpersist%E7%AE%97%E5%AD%90/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/cache%E7%AE%97%E5%AD%90%E5%92%8Cpersist%E7%AE%97%E5%AD%90/</guid>
      <description>Spark中cache和persist的作用 Spark开发高性能的大数据计算作业并不是那么简单。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 有一些代码开发基本的原则，避免创建重复的RDD，尽可能复用同一个RDD，如下，我们可以直接用一个RDD进行多</description>
    </item>
    <item>
      <title>MapReduce原理解析</title>
      <link>https://whiteding.fun/post/spark/MapReduce/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/MapReduce/</guid>
      <description>Hadoop简介 Hadoop就是一个实现了Google云计算系统的开源系统，包括并行计算模型Map/Reduce，分布式文件系统HDFS，以及分布式数据库Hbase，同时Hadoop的相关项目也很丰富，包括ZooKeeper，Pig，Chukwa，Hive，Hbase，Mahout，flume等. 这里详细分解这里面的概念让大家通过这篇文章了解到底是什么hadoop： Map/Reduce： MapRe</description>
    </item>
    <item>
      <title>RDD算子总结</title>
      <link>https://whiteding.fun/post/spark/RDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/RDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93/</guid>
      <description>RDD算子总结 从功能上分： 转换算子(transformer)： lazy执行，生成新的rdd，只有在调用action算子时，才会真正的执行。 如：$map 、flatmap、filter、 union、 join、 ruduceByKey、 cache$ 行动算子(action)： 触发任务执行，产生job，返回值不再是rdd。 如：$count 、collect、top、 take、 reduce$ 从作用上分： 通用的： map、 flatMap、</description>
    </item>
    <item>
      <title>repartition和coalesce区别</title>
      <link>https://whiteding.fun/post/spark/repartition%E5%92%8Ccoalesce%E5%8C%BA%E5%88%AB/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/repartition%E5%92%8Ccoalesce%E5%8C%BA%E5%88%AB/</guid>
      <description>简介 $repartition(numPartitions:Int)$ 和 $coalesce(numPartitions:Int，shuffle:Boolean=false)$ 作用：对RDD的分区进行重新划分，repartition内部调用了coalesce，参数$shuffle=true$ 分析 例：RDD有N个分区，需要重新划分成M个分区 1N小于M 2一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将s</description>
    </item>
    <item>
      <title>Spark map字段处理</title>
      <link>https://whiteding.fun/post/spark/PySpark-%E5%9C%A8%E9%81%87%E5%88%B0map%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/PySpark-%E5%9C%A8%E9%81%87%E5%88%B0map%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86/</guid>
      <description>$PySpark$ 在遇到$map$类型的列的一些处理 在$spark$中，有时会遇到$column$的类型是$array$和$map$类型的，这时候需要将它们转换为多行数据 $Explode\ array\ and\ map\ columns\ to\ rows$ 1import pyspark 2from pyspark.sql import SparkSession 3 4spark = SparkSession.builder.appName(&amp;#39;pyspark-by-examples&amp;#39;).getOrCreate() 5 6arrayData = [ 7 (&amp;#39;James&amp;#39;,[&amp;#39;Java&amp;#39;,&amp;#39;Scala&amp;#39;],{&amp;#39;hair&amp;#39;:&amp;#39;black&amp;#39;,&amp;#39;eye&amp;#39;:&amp;#39;brown&amp;#39;}), 8 (&amp;#39;Michael&amp;#39;,[&amp;#39;Spark&amp;#39;,&amp;#39;Java&amp;#39;,None],{&amp;#39;hair&amp;#39;:&amp;#39;brown&amp;#39;,&amp;#39;eye&amp;#39;:None}), 9 (&amp;#39;Robert&amp;#39;,[&amp;#39;CSharp&amp;#39;,&amp;#39;&amp;#39;],{&amp;#39;hair&amp;#39;:&amp;#39;red&amp;#39;,&amp;#39;eye&amp;#39;:&amp;#39;&amp;#39;}), 10 (&amp;#39;Washington&amp;#39;,None,None), 11 (&amp;#39;Jefferson&amp;#39;,[&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;],{}) ] 12 13df = spark.createDataFrame(data=arrayData, schema = [&amp;#39;name&amp;#39;,&amp;#39;knownLanguages&amp;#39;,&amp;#39;properties&amp;#39;]) 14df.printSchema() 15df.show() 1root 2 |-- name: string (nullable = true) 3 |-- knownLanguages: array (nullable = true) 4 | |-- element: string (containsNull = true) 5 |-- properties: map (nullable = true) 6 | |-- key: string 7 | |-- value: string (valueContainsNull = true) 8 9+----------+--------------+--------------------+ 10| name|knownLanguages| properties| 11+----------+--------------+--------------------+ 12| James| [Java, Scala]|[eye -&amp;gt; brown, ha...| 13| Michael|[Spark, Java,]|[eye -&amp;gt;, hair -&amp;gt; ...| 14| Robert| [CSharp, ]|[eye -&amp;gt; , hair -&amp;gt;...| 15|Washington| null| null| 16| Jefferson| [1, 2]| []| 17+----------+--------------+--------------------+ $explode –</description>
    </item>
    <item>
      <title>Spark RDD入门</title>
      <link>https://whiteding.fun/post/spark/RDD%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/RDD%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</guid>
      <description>RDD简介 RDD&amp;ndash;弹性分布式数据集（Resilient Distributed Dataset）是spark的核心概念。RDD其实就是分布式的元素集合。在Spark中，对数据的所有操作不外乎创建RDD，转化已有的RDD以及调用RDD操作进行求值。而在这一切的背后，spark会自动讲RDD中的数据分发到集群上，并将操作并行化执行。 RDD基础 RDD是一个不可变的分布式对象集合.每个RDD被分为多个分区，这些分区运</description>
    </item>
    <item>
      <title>Spark2.0特性</title>
      <link>https://whiteding.fun/post/spark/spark2.0%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/spark2.0%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/</guid>
      <description>Spark2.0 Spark直接从1.6跨入2.0版本，带来一些新的特性。最大的变化便是SparkSession整合了各种环境。 Spark2.0中引入了$SparkSession$的概念，它为用户提供了一个统一的切入点来使用Spark的各项功能，用户不但可以使用DataFrame和Dataset的各种API，学习Spark的难度也会大大降低。 SparkSession 在Spark的早期版本，SparkContext是进入Spark的切入</description>
    </item>
    <item>
      <title>Spark入门</title>
      <link>https://whiteding.fun/post/spark/spark%E5%85%A5%E9%97%A8/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/spark%E5%85%A5%E9%97%A8/</guid>
      <description>生态： Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的 Spark SQL：提供通过Apache Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。 Spark Streaming：对实时数据流进行处理和控制。Spark</description>
    </item>
    <item>
      <title>Spark各种概念理解</title>
      <link>https://whiteding.fun/post/spark/spark%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/spark%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <description>Spark中的各种概念的理解 Application：通俗讲，用户每次提交的所有的代码为一个application。 Job：一个application可以分为多个job。如何划分job？通俗讲，触发一个final RDD的实际计算（action）为一个job Stage：一个job可以分为多个stage。根据一个job中的RDD的宽依赖和窄依赖关系进行划分 Task：task是最小的基本的计算单位。一般是</description>
    </item>
    <item>
      <title>Spark解析DataFrame中的json字段</title>
      <link>https://whiteding.fun/post/spark/How-to-parse-a-column-of-json-string-in-Pyspark/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/How-to-parse-a-column-of-json-string-in-Pyspark/</guid>
      <description>How to parse a column of json string in Pyspark 在用$spark.sql(\ )$从Table读入数据时，DataFrame的列有时是这样一种类型：json形式的string。此时，我们通常需要去解析这个json string，从而提取我们想要的数据。 数据准备 1# Sample Data Frame 2jstr1 = u&amp;#39;{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:12345,&amp;#34;foo&amp;#34;:&amp;#34;bar&amp;#34;},&amp;#34;body&amp;#34;:{&amp;#34;id&amp;#34;:111000,&amp;#34;name&amp;#34;:&amp;#34;foobar&amp;#34;,&amp;#34;sub_json&amp;#34;:{&amp;#34;id&amp;#34;:54321,&amp;#34;sub_sub_json&amp;#34;:{&amp;#34;col1&amp;#34;:20,&amp;#34;col2&amp;#34;:&amp;#34;somethong&amp;#34;}}}}&amp;#39; 3jstr2 = u&amp;#39;{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:12346,&amp;#34;foo&amp;#34;:&amp;#34;baz&amp;#34;},&amp;#34;body&amp;#34;:{&amp;#34;id&amp;#34;:111002,&amp;#34;name&amp;#34;:&amp;#34;barfoo&amp;#34;,&amp;#34;sub_json&amp;#34;:{&amp;#34;id&amp;#34;:23456,&amp;#34;sub_sub_json&amp;#34;:{&amp;#34;col1&amp;#34;:30,&amp;#34;col2&amp;#34;:&amp;#34;something else&amp;#34;}}}}&amp;#39; 4jstr3 = u&amp;#39;{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:43256,&amp;#34;foo&amp;#34;:&amp;#34;foobaz&amp;#34;},&amp;#34;body&amp;#34;:{&amp;#34;id&amp;#34;:20192,&amp;#34;name&amp;#34;:&amp;#34;bazbar&amp;#34;,&amp;#34;sub_json&amp;#34;:{&amp;#34;id&amp;#34;:39283,&amp;#34;sub_sub_json&amp;#34;:{&amp;#34;col1&amp;#34;:50,&amp;#34;col2&amp;#34;:&amp;#34;another thing&amp;#34;}}}}&amp;#39; 5df = spark.createDataFrame([Row(json=jstr1),Row(json=jstr2),Row(json=jstr3)]) 如上所示，我们模拟一个DataFrame，其中只有一列，列名为json，类型为string。可以看到，json中的值为json格式。我们</description>
    </item>
    <item>
      <title>Spark调优实战</title>
      <link>https://whiteding.fun/post/spark/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</guid>
      <description>Spark性能调优：合理设置并行度 1. Spark的并行度指的是什么？ 并行度其实就是指的是spark作业中,各个stage的同时运行的task的数量,也就代表了spark作业在各个阶段stage的并行度！ $$ 并行度 = executor\_number * executor\_cores $$ 理解： sparkApplication的划分： $job &amp;ndash;&amp;gt; stage &amp;ndash;&amp;gt; task$ 一般每个task一次处理一个分区。 可以将task理解为比赛中的跑道：每轮比赛中，每个跑道上都会有一位运动员(分区，即处理的数据</description>
    </item>
    <item>
      <title>Spark运行内存超出</title>
      <link>https://whiteding.fun/post/spark/%E5%85%B3%E4%BA%8ESpark%E8%BF%90%E8%A1%8C%E4%B8%AD%E5%86%85%E5%AD%98%E8%B6%85%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/spark/%E5%85%B3%E4%BA%8ESpark%E8%BF%90%E8%A1%8C%E4%B8%AD%E5%86%85%E5%AD%98%E8%B6%85%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>Container killed by YARN for exceeding memory limits？ 运行spark脚本时，经常会碰到Container killed by YARN for exceeding memory limits的错误，导致程序运行失败。 这个的意思是指executor的外堆内存超出了。默认情况下，这个值被设置为executor_memory的10%或者384M，以较大者为准，即max(executor_memory*.1, 384M). 解决办法 提高内存开销 减少执行程序内核的数量 增加分区数量 提高驱动程序和执行程序内存 提</description>
    </item>
    <item>
      <title>XGBoost的原理分析以及实践</title>
      <link>https://whiteding.fun/post/machine_learning/XGBoost%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://whiteding.fun/post/machine_learning/XGBoost%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5/</guid>
      <description>XGBoost算法 原理 任何机器学习的问题都可以从目标函数(objective function)出发，目标函数的主要由两部分组成 $损失函数+正则项$： $$ Obj(\Theta)=L(\Theta)+\Omega(\Theta) $$ 在这里，当我选择树模型为基学习器时，我们需要正则的对象，或者说需要控制复杂度的对象就是这K颗树,通常树的参数有树的深度，叶子节点的个数，叶子节点值的取值（xgboost里称为权重weight)。 所以，我们的目标函数形式如下： $$ \mathcal{L}=\sum_{i} l\left(\hat{y_i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right) $$ 这里前一半</description>
    </item>
  </channel>
</rss>
