<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on White</title>
    <link>http://localhost:1313/categories/spark/</link>
    <description>Recent content in Spark on White</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 12 Jun 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark调优实战</title>
      <link>http://localhost:1313/post/spark/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</guid>
      <description>Spark性能调优：合理设置并行度 1. Spark的并行度指的是什么？ 并行度其实就是指的是spark作业中,各个stage的同时运行的task的数量,也就代表了spark作业在各个阶段stage的并行度！ $$ 并行度 = executor\_number * executor\_cores $$ 理解： sparkApplication的划分： $job &amp;ndash;&amp;gt; stage &amp;ndash;&amp;gt; task$ 一般每个task一次处理一个分区。 可以将task理解为比赛中的跑道：每轮比赛中，每个跑道上都会有一位运动员(分区，即处理的数据</description>
    </item>
    <item>
      <title>Spark解析DataFrame中的json字段</title>
      <link>http://localhost:1313/post/spark/Spark%E8%A7%A3%E6%9E%90DataFrame%E4%B8%AD%E7%9A%84json%E5%AD%97%E6%AE%B5/</link>
      <pubDate>Thu, 23 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/Spark%E8%A7%A3%E6%9E%90DataFrame%E4%B8%AD%E7%9A%84json%E5%AD%97%E6%AE%B5/</guid>
      <description>How to parse a column of json string in Pyspark 在用$spark.sql(\ )$从Table读入数据时，DataFrame的列有时是这样一种类型：json形式的string。此时，我们通常需要去解析这个json string，从而提取我们想要的数据。 数据准备 1# Sample Data Frame 2jstr1 = u&amp;#39;{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:12345,&amp;#34;foo&amp;#34;:&amp;#34;bar&amp;#34;},&amp;#34;body&amp;#34;:{&amp;#34;id&amp;#34;:111000,&amp;#34;name&amp;#34;:&amp;#34;foobar&amp;#34;,&amp;#34;sub_json&amp;#34;:{&amp;#34;id&amp;#34;:54321,&amp;#34;sub_sub_json&amp;#34;:{&amp;#34;col1&amp;#34;:20,&amp;#34;col2&amp;#34;:&amp;#34;somethong&amp;#34;}}}}&amp;#39; 3jstr2 = u&amp;#39;{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:12346,&amp;#34;foo&amp;#34;:&amp;#34;baz&amp;#34;},&amp;#34;body&amp;#34;:{&amp;#34;id&amp;#34;:111002,&amp;#34;name&amp;#34;:&amp;#34;barfoo&amp;#34;,&amp;#34;sub_json&amp;#34;:{&amp;#34;id&amp;#34;:23456,&amp;#34;sub_sub_json&amp;#34;:{&amp;#34;col1&amp;#34;:30,&amp;#34;col2&amp;#34;:&amp;#34;something else&amp;#34;}}}}&amp;#39; 4jstr3 = u&amp;#39;{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:43256,&amp;#34;foo&amp;#34;:&amp;#34;foobaz&amp;#34;},&amp;#34;body&amp;#34;:{&amp;#34;id&amp;#34;:20192,&amp;#34;name&amp;#34;:&amp;#34;bazbar&amp;#34;,&amp;#34;sub_json&amp;#34;:{&amp;#34;id&amp;#34;:39283,&amp;#34;sub_sub_json&amp;#34;:{&amp;#34;col1&amp;#34;:50,&amp;#34;col2&amp;#34;:&amp;#34;another thing&amp;#34;}}}}&amp;#39; 5df = spark.createDataFrame([Row(json=jstr1),Row(json=jstr2),Row(json=jstr3)]) 1+--------------------+ 2| json| 3+--------------------+ 4|{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:1...| 5|{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:1...| 6|{&amp;#34;header&amp;#34;:{&amp;#34;id&amp;#34;:4...| 7+--------------------+ 如上所示，我们模拟一个DataFrame，其中只有一列，列名为json，类型为string。可以看到，json中的值为j</description>
    </item>
    <item>
      <title>Spark map字段处理</title>
      <link>http://localhost:1313/post/spark/PySpark-%E5%9C%A8%E9%81%87%E5%88%B0map%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/PySpark-%E5%9C%A8%E9%81%87%E5%88%B0map%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86/</guid>
      <description>$PySpark$ 在遇到$map$类型的列的一些处理 在$spark$中，有时会遇到$column$的类型是$array$和$map$类型的，这时候需要将它们转换为多行数据 $Explode\ array\ and\ map\ columns\ to\ rows$ 1import pyspark 2from pyspark.sql import SparkSession 3 4spark = SparkSession.builder.appName(&amp;#39;pyspark-by-examples&amp;#39;).getOrCreate() 5 6arrayData = [ 7 (&amp;#39;James&amp;#39;,[&amp;#39;Java&amp;#39;,&amp;#39;Scala&amp;#39;],{&amp;#39;hair&amp;#39;:&amp;#39;black&amp;#39;,&amp;#39;eye&amp;#39;:&amp;#39;brown&amp;#39;}), 8 (&amp;#39;Michael&amp;#39;,[&amp;#39;Spark&amp;#39;,&amp;#39;Java&amp;#39;,None],{&amp;#39;hair&amp;#39;:&amp;#39;brown&amp;#39;,&amp;#39;eye&amp;#39;:None}), 9 (&amp;#39;Robert&amp;#39;,[&amp;#39;CSharp&amp;#39;,&amp;#39;&amp;#39;],{&amp;#39;hair&amp;#39;:&amp;#39;red&amp;#39;,&amp;#39;eye&amp;#39;:&amp;#39;&amp;#39;}), 10 (&amp;#39;Washington&amp;#39;,None,None), 11 (&amp;#39;Jefferson&amp;#39;,[&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;],{}) ] 12 13df = spark.createDataFrame(data=arrayData, schema = [&amp;#39;name&amp;#39;,&amp;#39;knownLanguages&amp;#39;,&amp;#39;properties&amp;#39;]) 14df.printSchema() 15df.show() 1root 2 |-- name: string (nullable = true) 3 |-- knownLanguages: array (nullable = true) 4 | |-- element: string (containsNull = true) 5 |-- properties: map (nullable = true) 6 | |-- key: string 7 | |-- value: string (valueContainsNull = true) 8 9+----------+--------------+--------------------+ 10| name|knownLanguages| properties| 11+----------+--------------+--------------------+ 12| James| [Java, Scala]|[eye -&amp;gt; brown, ha...| 13| Michael|[Spark, Java,]|[eye -&amp;gt;, hair -&amp;gt; ...| 14| Robert| [CSharp, ]|[eye -&amp;gt; , hair -&amp;gt;...| 15|Washington| null| null| 16| Jefferson| [1, 2]| []| 17+----------+--------------+--------------------+ $explode –</description>
    </item>
    <item>
      <title>cache和persist比较</title>
      <link>http://localhost:1313/post/spark/cache%E7%AE%97%E5%AD%90%E5%92%8Cpersist%E7%AE%97%E5%AD%90/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/cache%E7%AE%97%E5%AD%90%E5%92%8Cpersist%E7%AE%97%E5%AD%90/</guid>
      <description>Spark中cache和persist的作用 Spark开发高性能的大数据计算作业并不是那么简单。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 有一些代码开发基本的原则，避免创建重复的RDD，尽可能复用同一个RDD，如下，我们可以直接用一个RDD进行多</description>
    </item>
    <item>
      <title>Spark运行内存超出</title>
      <link>http://localhost:1313/post/spark/%E5%85%B3%E4%BA%8ESpark%E8%BF%90%E8%A1%8C%E4%B8%AD%E5%86%85%E5%AD%98%E8%B6%85%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/%E5%85%B3%E4%BA%8ESpark%E8%BF%90%E8%A1%8C%E4%B8%AD%E5%86%85%E5%AD%98%E8%B6%85%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>Container killed by YARN for exceeding memory limits？ 运行spark脚本时，经常会碰到Container killed by YARN for exceeding memory limits的错误，导致程序运行失败。 这个的意思是指executor的外堆内存超出了。默认情况下，这个值被设置为executor_memory的10%或者384M，以较大者为准，即max(executor_memory*.1, 384M). 解决办法 提高内存开销 减少执行程序内核的数量 增加分区数量 提高驱动程序和执行程序内存 提</description>
    </item>
    <item>
      <title>repartition和coalesce区别</title>
      <link>http://localhost:1313/post/spark/repartition%E5%92%8Ccoalesce%E5%8C%BA%E5%88%AB/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/repartition%E5%92%8Ccoalesce%E5%8C%BA%E5%88%AB/</guid>
      <description>简介 $repartition(numPartitions:Int)$ 和 $coalesce(numPartitions:Int，shuffle:Boolean=false)$ 作用：对RDD的分区进行重新划分，repartition内部调用了coalesce，参数$shuffle=true$ 分析 例：RDD有N个分区，需要重新划分成M个分区 N小于M 一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shu</description>
    </item>
    <item>
      <title>RDD算子总结</title>
      <link>http://localhost:1313/post/spark/RDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93/</link>
      <pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/RDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93/</guid>
      <description>RDD算子总结 从功能上分： 转换算子(transformer)： lazy执行，生成新的rdd，只有在调用action算子时，才会真正的执行。 如：map 、flatmap、filter、 union、 join、 ruduceByKey、 cache 行动算子(action)： 触发任务执行，产生job，返回值不再是rdd。 如：count 、collect、top、 take、 reduce 从作用上分： 通用的： map、 flatMap、 di</description>
    </item>
    <item>
      <title>Spark RDD入门</title>
      <link>http://localhost:1313/post/spark/RDD%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/RDD%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</guid>
      <description>RDD简介 RDD&amp;ndash;弹性分布式数据集（Resilient Distributed Dataset）是spark的核心概念。RDD其实就是分布式的元素集合。在Spark中，对数据的所有操作不外乎创建RDD，转化已有的RDD以及调用RDD操作进行求值。而在这一切的背后，spark会自动讲RDD中的数据分发到集群上，并将操作并行化执行。 RDD基础 RDD是一个不可变的分布式对象集合.每个RDD被分为多个分区，这些分区运</description>
    </item>
    <item>
      <title>Spark2.0新特性</title>
      <link>http://localhost:1313/post/spark/spark2.0%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/spark2.0%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7/</guid>
      <description>Spark2.0 Spark直接从1.6跨入2.0版本，带来一些新的特性。最大的变化便是SparkSession整合了各种环境。 Spark2.0中引入了SparkSession的概念，它为用户提供了一个统一的切入点来使用Spark的各项功能，用户不但可以使用DataFrame和Dataset的各种API，学习Spark的难度也会大大降低。 SparkSession 在Spark的早期版本，SparkContext是进入Spark的切入点。</description>
    </item>
    <item>
      <title>Spark各种概念理解</title>
      <link>http://localhost:1313/post/spark/spark%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/spark%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <description>Spark中的各种概念的理解 Application：通俗讲，用户每次提交的所有的代码为一个application。 Job：一个application可以分为多个job。如何划分job？通俗讲，触发一个final RDD的实际计算（action）为一个job Stage：一个job可以分为多个stage。根据一个job中的RDD的宽依赖和窄依赖关系进行划分 Task：task是最小的基本的计算单位。一般是</description>
    </item>
    <item>
      <title>Spark入门</title>
      <link>http://localhost:1313/post/spark/spark%E5%85%A5%E9%97%A8/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/spark%E5%85%A5%E9%97%A8/</guid>
      <description>生态： Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的 Spark SQL：提供通过Apache Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。 Spark Streaming：对实时数据流进行处理和控制。Spark</description>
    </item>
    <item>
      <title>MapReduce原理解析</title>
      <link>http://localhost:1313/post/spark/MapReduce/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/spark/MapReduce/</guid>
      <description>Hadoop简介 Hadoop就是一个实现了Google云计算系统的开源系统，包括并行计算模型Map/Reduce，分布式文件系统HDFS，以及分布式数据库Hbase，同时Hadoop的相关项目也很丰富，包括ZooKeeper，Pig，Chukwa，Hive，Hbase，Mahout，flume等. 这里详细分解这里面的概念让大家通过这篇文章了解到底是什么hadoop： Map/Reduce： MapRe</description>
    </item>
  </channel>
</rss>
