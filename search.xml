<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[XGBoost的原理分析以及实践]]></title>
    <url>%2F2021%2F09%2F01%2FMachineLearning%2FXGBoost%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%E4%BB%A5%E5%8F%8A%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[XGBoost的原理分析以及实践原理任何机器学习的问题都可以从目标函数(objective function)出发，目标函数的主要由两部分组成 $损失函数+正则项$： Obj(\Theta)=L(\Theta)+\Omega(\Theta)在这里，当我选择树模型为基学习器时，我们需要正则的对象，或者说需要控制复杂度的对象就是这K颗树,通常树的参数有树的深度，叶子节点的个数，叶子节点值的取值（xgboost里称为权重weight)。 所以，我们的目标函数形式如下： \mathcal{L}=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)这里前一半代表预测值与真实$label$之间的误差，$i$代表的是每一个训练样本。后一半是正则项，$f_k$代表训练生成的每一颗树。 对一个目标函数，我们最理想的方法就选择一个优化方法算法去一步步的迭代的学习出参数。但是这里的参数是一颗颗的树，没有办法通过这种方式来学习。既然如此，我们可以利用Boosting的思想来解决这个问题，我们把学习的过程分解成先学第一颗树，然后基于第一棵树学习第二颗树。也就是说： \begin{array}{l} \hat{y}_{i}^{0}= C \\ \hat{y}_{i}^{1}=\hat{y}_{i}^{0}+f_{1}\left(x_{i}\right) \\ \hat{y}_{i}^{2}=\hat{y}_{i}^{1}+f_{2}\left(x_{i}\right) \\ \hat{y}_{i}^{K}=\hat{y}_{i}^{K-1}+f_{K}\left(x_{i}\right) \end{array}所以，对于第K次的目标函数为： \mathcal{L}^{(K)}=\sum_{i} l\left(y_{i}, \hat{y}_{i}^{K}\right)+\Omega\left(f_{K}\right)+\text {constant}\\ \mathcal{L}^{(K)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(K-1)}+f_{K}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{K}\right)根据二阶泰勒展开式： f(x+\Delta x)=f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2} 令： f(x) = L(y_i, \hat{y}_{i}^{K-1}) \\ \Delta x = f_K(x_i)对损失函数二阶展开： \mathcal{L}^{(K)} \simeq \sum_{i} L\left(y_{i}, \hat{y}_{i}^{K-1}+f_{K}\left(x_{i}\right)\right)=\sum_{i}\left[L\left(y_{i}, \hat{y}_{i}^{K-1}\right)+L^{\prime}\left(y_{i}, \hat{y}_{i}^{K-1}\right) f_{K}\left(x_{i}\right)+\frac{1}{2} L^{\prime \prime}\left(y_{i}, \hat{y}_{i}^{K-1}\right) f_{K}^{2}\left(x_{i}\right)\right]令： \begin{array}{l} g_{i}=L^{\prime}\left(y_{i}, \hat{y}_{i}^{K-1}\right) \\ h_{i}=L^{\prime \prime}\left(y_{i}, \hat{y}_{i}^{K-1}\right) \end{array}则进一步得到损失函数为： \mathcal{L}^{(K)} = \sum_{i}\left[L\left(y_{i}, \hat{y}_{i}^{K-1}\right)+g_{i} f_{K}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{K}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{K}\right)+\text { constant }一棵树其实可以由一片区域以及若干个叶子节点来表达。而同时，构建一颗树也是为了找到每个节点的区域以及叶子节点的值 f(x)=\left \{\begin{array}{ll} 0.8 & x =10 \end{array}\right.就说可以有如下映射的关系$f_K(x)=w_{q(x)}$。其中$q(x)$为叶子节点的编号（从左往右编，1，2，3···)。$w$是叶子节点的取值。也就说对于任意一个样本$x$,其最后会落在树的某个叶子节点上，其值为$w_{q(x)}$ 既然一棵树可以用叶子节点来表达，上面的正则项，我们可以对叶子节点值进行惩罚(正则)，比如取L2正则，以及我们控制一下叶子节点的个数T，那么正则项有： \Omega\left(f_{K}\right)=\frac{1}{2} \lambda \sum_{j}^{T}\left\|w_{j}\right\|_{2}+\gamma T 其实正则为什么可以控制模型复杂度呢？有很多角度可以看这个问题，最直观就是，我们为了使得目标函数最小，自然正则项也要小，正则项要小，叶子节点个数T要小（叶子节点个数少，树就简单）。而为什么要对叶子节点的值进行L2正则，这个可以参考一下LR里面进行正则的原因，简单的说就是LR没有加正则，整个w的参数空间是无限大的，只有加了正则之后，才会把w的解规范在一个范围内。（对此困惑的话可以跑一个不带正则的LR，每次出来的权重w都不一样，但是loss都是一样的，加了L2正则后，每次得到的w都是一样的） 目标函数（移除常数项后）就可以改写成这样（用叶子节点表达）： \sum_{i}\left[g_{i} w_{q\left(x_{i}\right)} + \frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\frac{1}{2} \lambda \sum_{j}^{T}\left\|w_{j}\right\|_{2}+\gamma T \\ =\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T令： \begin{aligned} G_{j} &=\sum_{\left(i \in I_{j}\right)} g_{i} \\ H_{j} &=\sum_{\left(i \in I_{j}\right)} h_{i} \end{aligned} \sum_{j=1}^{T}\left[G_{j} w_{j}+\frac{1}{2}\left(H_{j}+\lambda\right) w_{j}^{2}\right]+\gamma T对$w_j$求导，然后带入极值点，可以得到一个极值 w^{*}=-\frac{G_{j}}{H_{j}+\lambda} \\ \mathcal{L} = -\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T到这里，我们一直都是在围绕目标函数进行分析，这个到底是为什么呢？这个主要是为了后面我们寻找$f(x)$，也就是建树的过程。 具体来说，我们回忆一下建树的时候需要做什么，建树的时候最关键的一步就是选择一个分裂的准则，也就如何评价分裂的质量。比如在GBDT的介绍里，我们可以选择MSE，MAE来评价我们的分裂的质量，但是，我们所选择的分裂准则似乎不总是和我们的损失函数有关，因为这种选择是启发式的。比如，在分类任务里面，损失函数可以选择logloss，分裂准确选择MSE，这样看来，似乎分裂的好坏和我们的损失并没有直接挂钩。 但是，在xgboost里面，我们的分裂准则是直接与损失函数挂钩的准则，这个也是xgboost和GBDT一个很不一样的地方。 具体来说，$XGBoost$选择这个准则，计算增益$Gain$ {Gain}=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_L+G_{R}\right)^{2}}{\left(H_{L}+H_{R}\right)+\lambda}\right]-\gamma为什么?其实选择这个作为准则的原因很简单也很直观。我们这样考虑。由损失函数的最终表达式知道，对于一个结点，假设不分裂的话, 此时该节点损失为: -\frac{\left(G_L+G_{R}\right)^{2}}{\left(H_{L}+H_{R}\right)+\lambda}分裂之后左右子节点总损失为： -\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}\right]既然要分裂的时候，我们当然是选择分裂成左右子节点后，损失减少的最多, 即找到分裂点，使得: \max \left(\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_L+G_{R}\right)^{2}}{\left(H_{L}+H_{R}\right)+\lambda}\right]\right)那么$\gamma$的作用是什么呢？利用$\gamma$可以控制树的复杂度，进一步来说，利用$\gamma$来作为阈值，只有大于$\gamma$时候才选择分裂。这个其实起到预剪枝的作用。 寻找分裂点算法 缺失值的处理 可以看到内层循环里面有两个for，第一个for是从把特征取值从小到大排序，然后从小到大进行扫描，这个时候在计算$G_R$的时候是用总的$G$减去$G_LG_R$时候是用总的$G$减去$G_L$，$H_R$也是同样用总的$H$减去$H_L$,这意味着把空缺样本归到了右子结点。 第二个for相反过来，把空缺样本归到了左子结点。只要比较这两次最大增益出现在第一个for中还是第二个for中就可以知道对于空缺值的分裂方向，这就是xgboost如何学习空缺值的思想。 特征重要性一般我们调用xgb库的get_fscore()。但其实xgboost里面有三个指标用于对特征进行评价，而get_fscore()只是其中一个指标weight。这个指标大部分玩家都很熟悉，其代表着某个特征被选作分裂的次数。 而xgboost还提供了另外两个指标，一个叫gain，一个叫cover。可以利用get_score()来选择。 那么gain是指什么呢？其代表着某个特征的平均增益。比如，特征x1被选了6次作为分裂的特征，每次的增益假如为Gain1,Gain2,…Gain6，那么其平均增益为$(Gain1+Gain2+…Gain3)/6$ 实践 ID x1 x2 y 1 1 -5 0 2 2 5 0 3 3 -2 1 4 1 2 1 5 2 0 1 6 6 -5 1 7 7 5 1 8 6 -2 0 9 7 2 0 10 6 0 1 11 8 -5 1 12 9 5 1 13 10 -2 0 14 8 2 0 15 9 0 1 导数公式由于后面需要用到logloss的一阶导数以及二阶导数，这里先简单推导一下： L_{i}=-\left[y_{i} \cdot \textit{log} \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \textit{log} \left(1-p_{i}\right)\right]其中： p_i=\sigma(\hat y_i)=\frac{1}{1+e^{\hat {-y_i}}} \begin{aligned} \frac{\partial L_{i}}{\partial\hat {y_i}} &= \frac{\partial L_{i}}{\partial p_{i}} \cdot \frac{\partial p_{i}}{\partial \hat {y_i}} \\ &=\sigma\left(\hat {y_i}\right)-y_{i} \end{aligned}即： g_i = \sigma\left(\hat {y_i}\right)-y_{i}同理二阶导数： h_{i}=\sigma (\hat {y_i}) *\left(1-\sigma (\hat {y_i})\right)建立第一颗树(k=1)根据公式： \max \left(\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_L+G_{R}\right)^{2}}{\left(H_{L}+H_{R}\right)+\lambda}\right]\right)在结点处把样本分成左子结点和右子结点两个集合。分别求两个集合的$H_L,H_R, G_L, G_R$，然后计算增益$Gain$ 但是这里你可能碰到了一个问题，那就是第一颗树的时候每个样本的预测的概率值$\sigma (\hat {y_i})$是多少？ 这里和GBDT一样，应该说和所有的Boosting算法一样，都需要一个初始值。而在xgboost里面，对于分类任务只需要初始化为(0,1)中的任意一个数都可以。具体来说就是参数base_score。（其默认值是0.5) 这里我们也设base_score=0.5(即$\hat{y}_{i}^{0}= 0$)。然后我们就可以计算每个样本的一阶导数值和二阶导数值了 \begin{array}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|} \hline { ID } & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\ \hline { g_i } & 0.5 & 0.5 & -0.5 & -0.5 & -0.5 & -0.5 & -0.5 & 0.5 & 0.5 & -0.5 & -0.5 & -0.5 & 0.5 & 0.5 & -0.5 \\ \hline { h_i } & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 \\ \hline \end{array}那么把样本如何分成两个集合呢？这里就是上面说到的选取一个最佳的特征以及分裂点使得GainGain最大。 比如说对于特征$x_1$，一共有[1, 2, 3, 6, 7, 8, 9, 10]8种取值。可以得到以下这么多划分方式: x_1 < 2 \ \ \ x_1]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka概述]]></title>
    <url>%2F2021%2F07%2F26%2FKafka%2FKafka%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Kafka概述 Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 1. 消息队列(MQ)1.1 优点 解耦 削封 缓冲 异步通信 1.2 两种模式 点对点(一对一，消费者主动拉取数据，消息收到后消息清除）) ​ 消息生产者生产消息发送到Queue中，然后消息消费者主动从Queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布/订阅模式（一对多，消费者消费数据之后不会清除消息） 消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 发布/订阅模式既可以通过队列向消费者推送(类似于微信公众号); 也可以消费者去主动拉取(kafka),这样可以由消费者自己决定消费的速度，但同时，要维护一个长轮询，去监听队列中是否有新消息到达，造成一定的资源浪费。 2. Kafka的基础架构 Producer ：消息生产者，就是向kafka broker 发消息的客户端； Consumer ：消息消费者，向kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 Broker ：一台kafka 服务器就是一个broker。一个集群由多个broker 组成。一个broker 可以容纳多个topic。 Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic； Partition：为了实现扩展性，一个非常大的topic 可以分布到多个broker（即服务器）上，一个topic 可以分为多个partition，每个partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition 数据不丢失.且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader. follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower。 3. Kafka的常用指令12345678910111213141516171819202122#创建topic(副本数不能大于broker的个数) --partitions:分区数 --replication-factor：副本数(不能大于brokers的个数)bin/kafka-topics.sh --create --topic white --bootstrap-server hadoop101:9092 --partitions 3 --replication-factor 3# 查看topicbin/kafka-topics.sh --list --bootstrap-server hadoop101:9092 #列出可用的topicsbin/kafka-topics.sh --describe --bootstrap-server hadoop101:9092 --topic white #查看具体topic的信息# 删除topicbin/kafka-topics.sh --delete --bootstrap-server hadoop101:9092 --topic white #标记删除，过一段时间kafka会自己删除# 修改topicbin/kafka-topics.sh --alter --bootstrap-server hadoop101:9092 --topic white --partitions 6 #修改topic的分区数# 生产者bin/kafka-console-producer.sh --topic white --broker-list hadoop101:9092 # 消费者bin/kafka-console-consumer.sh --topic white --bootstrap-server hadoop101:9092# 从头开始消费bin/kafka-console-consumer.sh --topic white --bootstrap-server hadoop101:9092 --from-beginning# 查看log数据# ***.index文件(offset + 物理位置)bin/kafka-dump-log.sh --files /opt/module/kafka-2.4.1/datas/white-0/00000000000000000000.index --print-data-log# ***.log文件(真实的数据)bin/kafka-dump-log.sh --files /opt/module/kafka-2.4.1/datas/white-0/00000000000000000000.log --print-data-log]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink流处理API]]></title>
    <url>%2F2021%2F06%2F30%2FFlink%2FFLink%E6%B5%81%E5%A4%84%E7%90%86API%2F</url>
    <content type="text"><![CDATA[Flink 流处理API 1. Environment1.1 getExecutionEnvironment 创建一个执行环境，表示当前执行程序的上下文。如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。 12ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); 1.2 createLocalEnvironment1LocalStreamEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1); 1.3 createRemoteEnvironment1StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("jobmanage-hostname", 6123,"YOURPATH//WordCount.jar"); 2. Source2.1 集合1234567891011121314151617StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(1);// 从集合里读取数据DataStreamSource&lt;SensorReading&gt; dataStream = env.fromCollection(Arrays.asList( new SensorReading("sensor_1", 1547718199L, 35.8), new SensorReading("sensor_6", 1547718201L, 15.4), new SensorReading("sensor_7", 1547718202L, 6.7), new SensorReading("sensor_10", 1547718205L, 38.1)));DataStreamSource&lt;Integer&gt; integerDataStreamSource = env.fromElements(1, 2, 4, 8);// 打印输出dataStream.print("data");integerDataStreamSource.print("int");env.execute(); 2.2 文件123String path = "src/main/resources/sensor.txt";DataStreamSource&lt;String&gt; dataStream = env.readTextFile(path); 2.3 kafka消息队列1234567891011Properties properties = new Properties();properties.setProperty("bootstrap.servers", "localhost:9092");properties.setProperty("group.id", "consumer-group");properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");properties.setProperty("auto.offset.reset", "latest");//从kafka读取文件DataStreamSource&lt;String&gt; sensor = env.addSource(new FlinkKafkaConsumer011&lt;String&gt;("sensor", new SimpleStringSchema(), properties)); 2.4 自定义Source 除了以上的source数据来源，我们还可以自定义source。需要做的，只是传入一个SourceFunction就可以。具体调用如下： 1DataStreamSource&lt;SensorReading&gt; dataStream = env.addSource(new MySourceFucntion()); 我们希望可以随机生成传感器数据，MySensorSource具体的代码实现如下： 1234567891011121314151617181920212223242526272829public static class MySourceFucntion implements SourceFunction&lt;SensorReading&gt;&#123; private boolean running = true; @Override public void run(SourceContext&lt;SensorReading&gt; ctx) throws Exception &#123; HashMap&lt;String, Double&gt; sensorTempMap = new HashMap&lt;&gt;(10); // 设置10个传感器的初始温度 Random random = new Random(); for (int i = 0; i &lt; 10; i++) &#123; sensorTempMap.put("sensor_" + (i+1), 60 + random.nextGaussian() * 20); &#125; while(running)&#123; for (String sensorId : sensorTempMap.keySet() ) &#123; // 在当前温度基础上随机波动 double newTemp = sensorTempMap.get(sensorId) + random.nextGaussian(); sensorTempMap.put(sensorId, newTemp); ctx.collect(new SensorReading(sensorId, System.currentTimeMillis(), newTemp)); &#125; // 控制输出频率 Thread.sleep(1000L); &#125; &#125; @Override public void cancel() &#123; running = false; &#125;&#125; 3. Transform3.1 map 1234567DataStreamSource&lt;String&gt; inputStream = env.readTextFile(path);...DataStream&lt;Integer&gt; mapStram = inputStream.map(new MapFunction&lt;String, Integer&gt;() &#123; public Integer map(String value) throws Exception &#123; return value.length(); &#125; &#125;); 3.2 flatMap123456789// 2. flatmap，按逗号分字段DataStream&lt;String&gt; flatMapStream = inputStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; String[] fields = value.split(","); for( String field: fields ) out.collect(field); &#125;&#125;); 3.3 filter 1DataStream&lt;String&gt; filterStream = inputStream.filter(line -&gt; line.startsWith("sensor_1")); 3.4 keyBy DataStream→KeyedStream：逻辑地将一个流拆分成不相交的分区，每个分区包含具有相同key的元素，在内部以hash的形式实现的. 3.5 滚动聚合算子(Rolling Aggregation)​ 这些算子可以针对$KeyedStream$的每一个支流做聚合。 sum() min() max() minBy() Maxby() 3.6 Reduce KeyedStream →DataStream：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。 1234567891011121314151617181920DataStreamSource&lt;String&gt; inputStream = env.readTextFile(path); DataStream&lt;SensorReading&gt; mapStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(","); return new SensorReading(fields[0], Long.parseLong(fields[1]), Double.parseDouble(fields[2])); &#125;); KeyedStream&lt;SensorReading, Tuple&gt; keyedStream = mapStream.keyBy("id"); SingleOutputStreamOperator&lt;SensorReading&gt; reduceStream = keyedStream.reduce(new ReduceFunction&lt;SensorReading&gt;() &#123; @Override public SensorReading reduce(SensorReading value1, SensorReading value2) throws Exception &#123; return new SensorReading(value1.getId(), value2.getTimestamp(), Math.max(value1.getTemperature(), value2.getTemperature())); &#125; &#125;); //SensorReading&#123;id='sensor_1', timestamp=1547718207, temperature=36.3&#125; //SensorReading&#123;id='sensor_1', timestamp=1547718209, temperature=36.3&#125; reduceStream.print(); env.execute(); 3.7 Split/Select DataStream → SplitStream：根据某些特征把一个$DataStream$拆分成两个或者多个$DataStream$. SplitStream→DataStream：从一个SplitStream中获取一个或者多个DataStream. 1234567891011121314151617//传感器数据按照温度高低（以 30 度为界），拆分 成两个流 。DataStreamSource&lt;String&gt; inputStream = env.readTextFile(path);DataStream&lt;SensorReading&gt; dataStream = inputStream.map(line -&gt; &#123; String[] fields = line.split(","); return new SensorReading(fields[0], Long.parseLong(fields[1]), Double.parseDouble(fields[2]));&#125;);// 分流，按照温度值30度为界分为两条流SplitStream&lt;SensorReading&gt; splitStream = dataStream.split(new OutputSelector&lt;SensorReading&gt;() &#123; @Override public Iterable&lt;String&gt; select(SensorReading value) &#123; return (value.getTemperature() &gt; 30) ? Collections.singletonList("high") : Collections.singletonList("low"); &#125;&#125;);DataStream&lt;SensorReading&gt; highStream = splitStream.select("high");DataStream&lt;SensorReading&gt; lowStream = splitStream.select("low"); 3.8 Connect和CoMap DataStream,DataStream → ConnectedStreams：连接两个保持他们类型的数据流，两个数据流被Connect之后，只是被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。 ConnectedStreams → DataStream：作用于$ConnectedStreams$上，功能与map和flatMap一样，对ConnectedStreams中的每一个Stream分别进行map和flatMap处理 1234567891011121314151617181920// 2. 合流 connect，将高温流转换成二元组类型，与低温流连接合并之后，输出状态信息DataStream&lt;Tuple2&lt;String, Double&gt;&gt; warningStream = highStream.map(new MapFunction&lt;SensorReading, Tuple2&lt;String, Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(SensorReading value) throws Exception &#123; return new Tuple2&lt;&gt;(value.getId(), value.getTemperature()); &#125; &#125;);ConnectedStreams&lt;Tuple2&lt;String, Double&gt;, SensorReading&gt; connectedStreams = warningStream.connect(lowStream);SingleOutputStreamOperator&lt;Object&gt; resultStream = connectedStreams.map(new CoMapFunction&lt;Tuple2&lt;String, Double&gt;, SensorReading, Object&gt;() &#123; @Override public Object map1(Tuple2&lt;String, Double&gt; value) throws Exception &#123; return new Tuple3&lt;&gt;(value.f0, value.f1, "high temperature warning"); &#125; @Override public Object map2(SensorReading value) throws Exception &#123; return new Tuple2&lt;&gt;(value.getId(), "normal"); &#125; &#125;); 3.9 Union DataStream → DataStream：对两个或者两个以上的$DataStream$进行union操作，产生一个包含所有$DataStream$元素的新$DataStream$。 Union之前两个流的类型必须是一样，Connect可以不一样，在之后的coMap中再去调整成为一样的。 Connect只能操作两个流，Union可以操作多个 1highStream.union(lowStream, allStream); 总览]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合框架(JCF)]]></title>
    <url>%2F2021%2F06%2F20%2FJava%2FJavaCollectionFramwork%2F</url>
    <content type="text"><![CDATA[Java集合框架(JCF) java的集合框架可以用如下图表示： 具体来讲，有如下关系： 1234567891011121314151617181920|----Collection接口：单列集合，用来存储一个一个的对象 |----List接口：存储有序的、可重复的数据。(jdk1.2) |----ArrayList:作为List接口的主要实现类；线程不安全的，效率高；底层使用Object[] elementData存储(jdk1.2) |----LinkedList:对于频繁的插入、删除操作，使用此类效率比ArrayList高；底层使用双向链表存储(jdk1.2) |----Vector:作为List接口的古老实现类；线程安全的，效率低；底层使用Object[] elementData存储(jdk1.0) |----Stack |----Set接口：存储无序的、不可重复的数据。 |----HashSet:作为Set接口的主要实现类；线程不安全的；可以存储null值 |----LinkedHashSet:作为HashSet的子类；遍历其内部数据时，可以按照添加的顺序遍历对于频繁的遍历操作，LinkedHashSet效率高于HashSet. |----TreeSet:可以按照添加对象的指定属性，进行排序. |----Queue队列 |----PriorityQueue |----Deque |----Map接口：双列集合，用来存储一对(key - value)一对的数据。jdk1.2 |----HashMap:作为Map的主要实现类；线程不安全的，效率高；存储null的key和value。 jdk1.2 |----LinkedHashMap:保证在遍历map元素时，可以按照添加的顺序实现遍历。jdk1.4 |----TreeMap:保证按照添加的key-value对进行排序，实现排序遍历。此时考虑key的自然排序或定制排序,底层使用红黑树 |----Hashtable:Hashtable:作为古老的实现类；线程安全的，效率低；不能存储null的key和value jdk1.0 |----Properties:常用来处理配置文件。key和value都是String类型]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础3]]></title>
    <url>%2F2021%2F06%2F01%2FJava%2FJava%E5%9F%BA%E7%A1%803%2F</url>
    <content type="text"><![CDATA[变量Java基本数据类型基本数据类型是CPU可以直接进行运算的类型。Java定义了以下几种基本数据类型： 整数类型：byte，short，int，long 浮点数类型：float，double 字符类型：char 布尔类型：boolean Java定义的这些基本数据类型有什么区别呢？要了解这些区别，我们就必须简单了解一下计算机内存的基本结构。 计算机内存的最小存储单元是字节（byte），一个字节就是一个8位二进制数，即8个bit。它的二进制表示范围从00000000~11111111，换算成十进制是0~255，换算成十六进制是00~ff。 内存单元从0开始编号，称为内存地址。每个内存单元可以看作一间房间，内存地址就是门牌号。 1234 0 1 2 3 4 5 6 ...┌───┬───┬───┬───┬───┬───┬───┐│ │ │ │ │ │ │ │...└───┴───┴───┴───┴───┴───┴───┘ 一个字节是1byte，1024字节是1K，1024K是1M，1024M是1G，1024G是1T。一个拥有4T内存的计算机的字节数量就是： 123454T = 4 x 1024G = 4 x 1024 x 1024M = 4 x 1024 x 1024 x 1024K = 4 x 1024 x 1024 x 1024 x 1024byte = 4398046511104byte 不同的数据类型占用的字节数不一样。我们看一下Java基本数据类型占用的字节数： 123456789101112131415161718192021 ┌───┐ byte │ │ └───┘ ┌───┬───┐ short │ │ │ └───┴───┘ ┌───┬───┬───┬───┐ int │ │ │ │ │ └───┴───┴───┴───┘ ┌───┬───┬───┬───┬───┬───┬───┬───┐ long │ │ │ │ │ │ │ │ │ └───┴───┴───┴───┴───┴───┴───┴───┘ ┌───┬───┬───┬───┐ float │ │ │ │ │ └───┴───┴───┴───┘ ┌───┬───┬───┬───┬───┬───┬───┬───┐double │ │ │ │ │ │ │ │ │ └───┴───┴───┴───┴───┴───┴───┴───┘ ┌───┬───┐ char │ │ │ └───┴───┘ 整型对于整型类型，Java只定义了带符号的整型，因此，最高位的bit表示符号位（0表示正数，1表示负数）。各种整型能表示的最大范围如下： byte：-128 ~ 127 short: -32768 ~ 32767 int: -2147483648 ~ 2147483647 long: -9223372036854775808 ~ 9223372036854775807 12345678910public class Main &#123; public static void main(String[] args) &#123; int i = 2147483647; int i2 = -2147483648; int i3 = 2_000_000_000; // 加下划线更容易识别 int i4 = 0xff0000; // 十六进制表示的16711680 int i5 = 0b1000000000; // 二进制表示的512 long l = 9000000000000000000L; // long型的结尾需要加L &#125;&#125; 浮点型浮点类型的数就是小数，因为小数用科学计数法表示的时候，小数点是可以“浮动”的，如1234.5可以表示成12.345x102，也可以表示成1.2345x103，所以称为浮点数。 下面是定义浮点数的例子： 12345float f1 = 3.14f;float f2 = 3.14e38f; // 科学计数法表示的3.14x10^38double d = 1.79e308;double d2 = -1.79e308;double d3 = 4.9e-324; // 科学计数法表示的4.9x10^-324 对于float类型，需要加上f后缀。 浮点数可表示的范围非常大，float类型可最大表示3.4x10e38，而double类型可最大表示1.79x10e308。 布尔类型布尔类型boolean只有true和false两个值，布尔类型总是关系运算的计算结果： 12345boolean b1 = true;boolean b2 = false;boolean isGreater = 5 &gt; 3; // 计算结果为trueint age = 12;boolean isAdult = age &gt;= 18; // 计算结果为false Java语言对布尔类型的存储并没有做规定，因为理论上存储布尔类型只需要1 bit，但是通常JVM内部会把boolean表示为4字节整数。 字符类型字符类型char表示一个字符。Java的char类型除了可表示标准的ASCII外，还可以表示一个Unicode字符： 123456789// 字符类型public class Main &#123; public static void main(String[] args) &#123; char a = 'A'; char zh = '中'; System.out.println(a); System.out.println(zh); &#125;&#125; Run 注意char类型使用单引号&#39;，且仅有一个字符，要和双引号&quot;的字符串类型区分开。 引用类型除了上述基本类型的变量，剩下的都是引用类型。例如，引用类型最常用的就是String字符串： 1String s = "hello"; 引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置，后续我们介绍类的概念时会详细讨论。 常量定义变量的时候，如果加上final修饰符，这个变量就变成了常量： 1234final double PI = 3.14; // PI是一个常量double r = 5.0;double area = PI * r * r;PI = 300; // compile error! 常量在定义时进行初始化后就不可再次赋值，再次赋值会导致编译错误。 常量的作用是用有意义的变量名来避免魔术数字（$Magic\ number$），例如，不要在代码中到处写3.14，而是定义一个常量。如果将来需要提高计算精度，我们只需要在常量的定义处修改，例如，改成3.1416，而不必在所有地方替换3.14。 根据习惯，常量名通常全部大写。 var关键字有些时候，类型的名字太长，写起来比较麻烦。例如： 1StringBuilder sb = new StringBuilder(); 这个时候，如果想省略变量类型，可以使用var关键字(Java SE 10特性)： 1var sb = new StringBuilder(); 编译器会根据赋值语句自动推断出变量sb的类型是StringBuilder。对编译器来说，语句： 1var sb = new StringBuilder(); 实际上会自动变成： 1StringBuilder sb = new StringBuilder(); 因此，使用var定义变量，仅仅是少写了变量类型而已。 变量的作用范围在Java中，多行语句用{ }括起来。很多控制语句，例如条件判断和循环，都以{ }作为它们自身的范围，例如： 1234567891011if (...) &#123; // if开始 ... while (...) &#123; // while 开始 ... if (...) &#123; // if开始 ... &#125; // if结束 ... &#125; // while结束 ...&#125; // if结束 只要正确地嵌套这些{ }，编译器就能识别出语句块的开始和结束。而在语句块中定义的变量，它有一个作用域，就是从定义处开始，到语句块结束。超出了作用域引用这些变量，编译器会报错。举个例子： 123456789101112131415161718192021&#123; ... int i = 0; // 变量i从这里开始定义 ... &#123; ... int x = 1; // 变量x从这里开始定义 ... &#123; ... String s = "hello"; // 变量s从这里开始定义 ... &#125; // 变量s作用域到此结束 ... // 注意，这是一个新的变量s，它和上面的变量同名， // 但是因为作用域不同，它们是两个不同的变量: String s = "hi"; ... &#125; // 变量x和s作用域到此结束 ...&#125; // 变量i作用域到此结束 定义变量时，要遵循作用域最小化原则，尽量将变量定义在尽可能小的作用域，并且，不要重复使用变量名]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis笔记]]></title>
    <url>%2F2021%2F01%2F08%2FRedis%2FRedis%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Redis入门笔记redis 是一款 NoSQL 数据库，是最常见的一款非关系型数据库，主要使用 key - value 的形式存储数据，和 mysql 不同，redis 并不会直接把数据存储到硬盘中，而是存储在内存中，也正是这样的设定让 redis 的存取操作特别的快。 Redis 的下载及安装redis 的下载首先要去 redis 的官网下载他的压缩包，官网 [ 中文 ] 直连地址如下： 1http://www.redis.cn/ 进入官网后直接下载即可，然后远程连接 服务器/虚拟机 将下载好的文件上传至 /usr/local 目录下 1234# 进入到usr/local目录cd /usr/local# 上传文件后，执行解压命令tar -zxvf redis-5.0.5.tar.gz redis 的安装在执行安装命令之前，首选需要安装 gcc 的依赖，如已安装请忽略 1yum -y install gcc 然后切换到 redis 目录下开始执行安装命令 1234# 切换目录到redis下cd redis-5.0.5# 执行安装命令make &amp;&amp; make install 这样一来，安装就顺利结束了，如果提示 -bash: make: command not found 则代表 gcc 没有装，需要重新安装 gcc 环境 redis 的配置实际上这个时候 redis 已经可以正常运行了，运行 redis 的命令为 redis-server，但是当我们运行的时候会发现，redis 直接将窗口锁定了，我们不能进行任何的操作，一旦解锁也就意味着 redis 停止工作了 1234567891011121314151617181920212223242519196:C 15 Jun 2020 22:13:31.574 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo19196:C 15 Jun 2020 22:13:31.574 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=19196, just started19196:C 15 Jun 2020 22:13:31.574 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 5.0.8 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 19196 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 19196:M 15 Jun 2020 22:13:31.575 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.19196:M 15 Jun 2020 22:13:31.575 # Server initialized.................. 这时我们需要对配置文件做一些修改 12345678## 建议弄个备份，玩儿备份就好，redis 的配置文件还是很重要的# 复制配置文件cp redis.conf /root/redis.confcd /root# 编辑配置文件信息vim redis.conf# 修改为daemonize为yes，可以后台运行 redis，否则会锁定命令行daemonize yes 运行 redis 并创建连接首先我们来运行服务端 redis-sever，需要注意的是这里要指定配置文件进行启动，否则还是会以默认配置执行，还是会锁定当前窗口 12345678# 默认执行，会锁定窗口redis-server# 指定配置文件执行，咱们配置了后台运行，不会锁定窗口redis-server redis.conf## 出现下面这堆东西就代表执行成功了 ##19203:C 15 Jun 2020 22:15:09.851 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo19203:C 15 Jun 2020 22:15:09.851 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=19203, just started19203:C 15 Jun 2020 22:15:09.851 # Configuration loaded 接下来让我们启动客户端 redis-cli 123456# 如果直接输入则默认连接本地的6379端口redis-cli127.0.0.1:6379&gt;# 也可以通过参数来指定目标IP地址或端口号redis-cli -h 127.0.0.1 -p 6379127.0.0.1:6379&gt; 尝试输入一个 ping 命令，测试服务是否正常工作，如果返回 PONG 就证明一切正常 12127.0.0.1:6379&gt; PINGPONG 关闭 redis如果不时用任何配置让他默认启动的话，直接 Ctrl + C 就可以关闭 redis，那么后台运行应该怎么关？ 1234# 连接到指定的服务后，执行关闭命令即可127.0.0.1:6379&gt; shutdown# 也可以在创建连接之前直接关闭，支持远程关闭redis-cli shutdown 设置访问密码我们刚刚成功连接到了 redis 服务器，但是这仅仅是本地的，我们目前并不能够远程 ip 地址直接访问，因为我们的 redis 默认开启了保护模式，将来我们的项目是要跑在网络上要用 java 代码操作的，想要远程连接必须要先设置密码！ 临时设置密码为了可以远程操作，我们可以设置一个临时的密码，连接上 redis 客户端，命令如下： 12345678910111213141516# 设置密码127.0.0.1:6379&gt; config set requirepass 123456OK# 设置密码是实时的，需要登录才可以进行操作127.0.0.1:6379&gt; keys *(error) NOAUTH Authentication required.# 验证redis密码127.0.0.1:6379&gt; auth 123456OK# 获取密码127.0.0.1:6379&gt; config get requirepass1) "requirepass"2) "123456"# 取消之前设置的密码，留空即可127.0.0.1:6379&gt; config set requirepass ''OK 这样一个临时的密码就设置成功了，只要 redis 始终保持正常运行，秘密是不会失效的 设置永久性密码如果觉得临时密码并不能够满足你的安全感，那么可以通过修改配置文件来设置密码： 12# 在redis.conf中找到requirepass然后在后面追加密码，重启redis即可requirepass 你的密码 配置文件讲解学习 redis 最好是跟着官方文档走，记笔记只是为了让自己更加熟练，推荐 redis 中文网： 1https://www.redis.net.cn/order/ 刚刚在安装 redis 的时候简单修改了一下 daemonize 属性的值，让他可以不锁定当前命令行且后台运行，redis 有很多需要了解的常规性配置，这里大概记个笔记： redis 的配置文件中注释非常多，官方很友好的将模块划分开来，这里就记几个我个人觉得常用的配置 ################## NETWORK ######################## 网络 1234#### 绑定客户端IP# 如果需要远程连接redis的话，要么在这里添加你的IP地址，要么将这句话注释掉# 注释掉之后，所有的IP就都可以访问了bind 127.0.0.1 123#### 保护模式# redis没有设置访问密码的情况下，如果开启了保护模式就不能远程连接了protected-mode yes 12#### 启动端口号，不多赘述port 6379 1234#### 客户端超时时间/s# 当客户端连接服务端的时候，超出 n 秒没有任何操作就自动切断连接# 设置为 0 代表无超时时间timeout 0 ################## GENERAL ######################## 常用 1234#### 守护进程# 这个就是之前设置的后台运行# 守护进程就是Windows系统中的服务，将redis以服务的形式运行daemonize yes 123#### PID文件# 如果开启了守护进程后台运行redis，那么这个就一定要有pidfile /var/run/redis_6379.pid 1234#### 日志级别# 记录执行日志时可选择的记录级别，选项分别有：# debug，verbose，notice，warning，具体描述配置文件中有，一般不用设置这个loglevel notice 12#### 日志文件名称logfile "" 123#### 默认数据库个数# 可以在这里手动对数据库数量进行设置，我就改成5个啦，闲的没事儿databases 16 ################## SECURITY ######################## 安全 1234#### 密码# 安全中最常用的就是密码设置了，访问数据库必须要密码，不然就乱套了# 默认是注释的，也就是没有密码requirepass foobared Redis 基础命令redis 中也有库的概念，初始数量为 16 个，可以在配置文件中进行修改，他是按照从 0 开始的递增顺序命名的，和 mysql 不同，redis 不支持为某个数据库单独命名，也不支持为每个数据库设置单独的访问密码，默认我们启动 redis 时使用的是第 0 个数据库。 ==切换数据库==应该怎么操作呢？只需要执行 select 命令 1234# 当我们使用select切换到其他数据库的时候，端口号后面会提示当前使用数据库 [1]，默认库就不会提示127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; 在我们学习 mysql 的时候，需要通过使用 sql 语句对数据进行 CRUD，而在 redis 我们需要记的是命令. 验证登录密码 12127.0.0.1:6379&gt; auth 123456OK 数据的简单 CRUD 123456789101112# set命令，因为key是唯一的，所以set既是添加，也是修改，如果有空格需要用双引号引起来127.0.0.1:6379&gt; set user zhangOK# get命令，就是查询喽127.0.0.1:6379&gt; get user"zhang"# del命令，就是删除喽127.0.0.1:6379&gt; del user(integer) 1# 删除之后查不到了127.0.0.1:6379&gt; get user(nil) 查看当前库数据总计数 12127.0.0.1:6379[1]&gt; dbsize(integer) 4 查看当前库所有的 key，* 代表通配符 12345127.0.0.1:6379[1]&gt; keys *1) "user"2) "sex"3) "name"4) "age" 清空当前数据库 12127.0.0.1:6379[1]&gt; flushdbOK 清空所有数据库 12127.0.0.1:6379&gt; flushallOK 查看当前库是否存在该 key，只要不返回 0 就代表存在，exists 是否存在 12127.0.0.1:6379&gt; exists name(integer) 1 将当前库的数据移动到其他库 move 移动，name 为 key，1 为目标数据库 12127.0.0.1:6379&gt; move name 1(integer) 1 设置消亡时间 ( 过期时间 )，expire 单位/秒，ttl 查询 123456789101112131415161718# 设置user127.0.0.1:6379&gt; set user zhanghanzheOK# 设置user的消亡时间为10秒127.0.0.1:6379&gt; expire user 10(integer) 1# 一开始的时候还是可以获取的 127.0.0.1:6379&gt; get user"zhanghanzhe"# 查询过期时间，还剩3秒127.0.0.1:6379&gt; tll user(integer) 3# 这会变-2了127.0.0.1:6379&gt; tll user(integer) -2# 获取不到了127.0.0.1:6379&gt; get user(nil) 查看 key 的数据类型 12127.0.0.1:6379&gt; type namestring 五大基本类型在上面我们介绍了最后一个命令，是一个 type 命令，使用它可以查看指定 key 的数据类型，这里接触了一个新的知识，就是数据类型，redis 中有有多少数据类型呢？ String 字符串string 就是字符串啦，我们使用简单的 set 命令设置的数据，如果没有指定，默认就是 string 类型的，就像 java 的 api 一样，string 类型有自己专属的命令 string 类型的命令大多是都是以 str 开头的 字符串操作就是之前学习的 get set 命令，就不多赘述了 组合命令 返回并修改 12# 组合命令，修改新的value同时返回修改前的valuegetset name hanzhe 数据的批量操作 123456127.0.0.1:6379&gt; mset user zhang name hanzhe age 21OK127.0.0.1:6379&gt; mget user name age1) "zhang"2) "hanzhe"3) "21" 带有验证的 setnx 12345678# 同样是设置值，因为key是唯一的，所以他在设置之前会有一个效验，如果已存在就不修改127.0.0.1:6379&gt; set name zhang(integer) 1127.0.0.1:6379&gt; setnx name hanzhe(integer) 0# 批量操作也是一样，因为原子性的原因，他们要么全都成功，要么全都失败。127.0.0.1:6379&gt; msetnx view 123 name hanzhe(integer) 0 追加字符 append 12345678127.0.0.1:6379&gt; set name zhangOK127.0.0.1:6379&gt; append name " "(integer) 6127.0.0.1:6379&gt; append name hanzhe(integer) 12127.0.0.1:6379&gt; get name"zhang hanzhe" 返回字符串长度 strlen 12127.0.0.1:6379&gt; strlen name(integer) 12 自增，自减 incr decr 在 java 中，如果想要对字符串 +1 需要转换类型，而在 redis 中他们自动为我们进行了处理 1234567891011121314127.0.0.1:6379&gt; set view 1OK# 自增 +1127.0.0.1:6379&gt; incr view(integer) 2# 自减 -1127.0.0.1:6379&gt; decr view(integer) 1# 自增，指定步长为3127.0.0.1:6379&gt; incrby view 3(integer) 4# 自减，指定步长为2127.0.0.1:6379&gt; decrby view 2(integer) 2 截取字符串 getrange，==截取是将获取的结果进行截取，并不会对 key 本身造成改变== 12345127.0.0.1:6379&gt; set name zhanghanzheOK# 从下标0开始，截取到4，共截取了5位，包头包尾，如果是0到-1则代表获取全部的字符串127.0.0.1:6379&gt; getrange name 0 4"zhang" 替换字符串 setrange 12345# 从下标5开始替换，直至替换结束，这个会对name进行修改。127.0.0.1:6379&gt; setrange name 5 chunde(integer) 11127.0.0.1:6379&gt; get name"zhangchunde" 设置消亡时间 ( 过期时间 ) setex 单位/秒 之前我们使用过一个叫 expire 的命令，也是用来设置消亡时间的，但是他执行设置已存在的 key，而这个命令可以在设置变量的同时设置消亡时间 12127.0.0.1:6379&gt; setex name 10 zhang #设置name的值为"zhang" 10s后过期OK List 列表集合list 是用来存储 string 的一个列表, ==redis 的列表是双向的==，list 默认从右向左延伸，也就是每次添加元素都会添加在左面，==list 列表允许重复元素的出现== list中的命令，大多数都是以小写字母 l 开头的 创建-添加元素 lpush，rpush lpush 默认从左侧添加，如需从右侧添加可以使用 r 开头声明 123456127.0.0.1:6379&gt; lpush name han(integer) 1127.0.0.1:6379&gt; lpush name zhang(integer) 2127.0.0.1:6379&gt; rpush name zhe(integer) 3 查看列表 lrange 12345# 查看之前创建的list，仅支持从左侧查看127.0.0.1:6379&gt; lrange name 0 -11) "zhang"2) "han"3) "zhe" 添加，追加元素 2 之前介绍了 push 命令可以添加元素，通过 l 开头或者 r 开头可以控制左侧添加还是右侧添加，其实添加元素还可以使用 linsert 命令，和 push 不同的是， push 是在首位添加，而 linsert 是从 左面开始在第一个目标 key 的前后添加 123456789101112127.0.0.1:6379&gt; lpush name han(integer) 1# 在name中的han前面添加zhang127.0.0.1:6379&gt; linsert name before han zhang(integer) 2# 在name中的han后面添加zhe127.0.0.1:6379&gt; linsert name after han zhe(integer) 3127.0.0.1:6379&gt; lrange name 0 -11) "zhang"2) "han"3) "zhe" 修改指定下标的 value 1234567891011127.0.0.1:6379&gt; lrange name 0 -11) "zhe"2) "han"3) "zhe"# 修改下标0的值为zhang127.0.0.1:6379&gt; lset name 0 zhangOK127.0.0.1:6379&gt; lrange name 0 -11) "zhang"2) "han"3) "zhe" 移除首尾 pop，移除多个 lrem 12345678910111213# 移除左侧第一个，返回被移除的数据127.0.0.1:6379&gt; lpop name"zhang"# 移除右侧第一个，返回被移除的数据127.0.0.1:6379&gt; rpop name"zhe"127.0.0.1:6379&gt; lpush name one(integer) 4127.0.0.1:6379&gt; rpush name one one(integer) 6# 移除多个元素，仅支持从左侧移除，2数量，one为具体要移除的值，精确匹配，不支持通配符127.0.0.1:6379&gt; lrem name 2 one(integer) 2 返回长度 llen 1234567127.0.0.1:6379&gt; lpush name han zhang(integer) 2127.0.0.1:6379&gt; rpush name zhe(integer) 3# 获取集合长度127.0.0.1:6379&gt; llen name(integer) 3 获取指定下标的value 123456127.0.0.1:6379&gt; lrange name 0 -11) "zhang"2) "han"3) "zhe"127.0.0.1:6379&gt; lindex name 0"zhang" 截取列表 ltrim 123456789101112127.0.0.1:6379&gt; lrange list 0 -11) "five"2) "four"3) "three"4) "two"5) "one"# 从下标1开始截取到2，会修改变量中的值127.0.0.1:6379&gt; ltrim list 1 2OK127.0.0.1:6379&gt; lrange list 0 -11) "four"2) "three" 获取排序结果 sort，不会影响本体，字母排序/alpha，倒序排序/desc 1234567891011121314127.0.0.1:6379&gt; lpush num 10 30 20 40(integer) 4# 默认从小到大127.0.0.1:6379&gt; sort num1) "10"2) "20"3) "30"4) "40"# 可以从大到小127.0.0.1:6379&gt; sort num desc1) "40"2) "30"3) "20"4) "10" 1234567# 字母排序127.0.0.1:6379&gt; lpush En python java switch(integer) 3127.0.0.1:6379&gt; sort En alpha1) "java"2) "python"3) "switch" 组合命令：移动元素 1234567891011127.0.0.1:6379&gt; lrange name 0 -11) "han"2) "zhe"3) "zhang"# 移除目标元素并添加到目标位置127.0.0.1:6379&gt; rpoplpush name name"zhang"127.0.0.1:6379&gt; lrange name 0 -11) "zhang"2) "han"3) "zhe" Set 无序集合set 集合是 string 类型的 ==无序集合==，且==不允许存入重复数据==，如果重复存入相同的值会报错 还是老规矩，set 集合的命令特点，就是以 s 开头 set 集合的 CRUD 12345# 添加 sadd127.0.0.1:6379&gt; sadd code java(integer) 1127.0.0.1:6379&gt; sadd code python switch(integer) 2 12345# 查看 smembers，这里就可以看出，set集合是无序的127.0.0.1:6379&gt; smembers code1) "python"2) "java"3) "switch" 123# 删除元素127.0.0.1:6379&gt; srem code switch(integer) 1 查看目标 set 集合长度 scard 12127.0.0.1:6379&gt; scard code(integer) 3 检查 set 集合中是否包含指定的值 sismember，存在返回 1，不存在返回 0 12127.0.0.1:6379&gt; sismember code java(integer) 1 随机性的获取和移除 12345678910127.0.0.1:6379&gt; sadd num 1 2 3 4 5 6 7 8 9(integer) 9# 随机获取一个值127.0.0.1:6379&gt; srandmember num 11) "7"127.0.0.1:6379&gt; srandmember num 11) "3"# 随机移除一个值127.0.0.1:6379&gt; spop num 11) "2" 获取多个集合中的 ==差集==，==交集==，==并集== 1234567## 前置条件--拥有三个set集合127.0.0.1:6379&gt; sadd key1 1 2 3 4(integer) 4127.0.0.1:6379&gt; sadd key2 3 4 5 6(integer) 4127.0.0.1:6379&gt; sadd key3 1 6 7 8(integer) 4 差集 sdiff，取出两个集合中不同的元素 12345678# 以第一个key为主，依次与每个key取差集127.0.0.1:6379&gt; sdiff key1 key21) "1"2) "2"127.0.0.1:6379&gt; sdiff key1 key2 key31) "2"127.0.0.1:6379&gt; sdiff key2 key1 key31) "5" 交集 sinter，取出两个集合中相同的元素 12345678# 以第一个key为主，依次与每个key取交集127.0.0.1:6379&gt; sinter key1 key21) "3"2) "4"127.0.0.1:6379&gt; sinter key1 key31) "1"127.0.0.1:6379&gt; sinter key1 key2 key3(empty list or set) 并集 sunion，取出两个集合中所有的元素 1234567891011121314151617# 以第一个key为主，依次与每个key取并集127.0.0.1:6379&gt; sunion key1 key21) "1"2) "2"3) "3"4) "4"5) "5"6) "6"127.0.0.1:6379&gt; sunion key1 key2 key31) "1"2) "2"3) "3"4) "4"5) "5"6) "6"7) "7"8) "8" Hash 散列集合hash 类型就像 map，是由一个个的 key-value 组成的对象，而且 ==key 不能重复==，存入相同的 key 和 value 后，最后一个存入的会覆盖之前存入的结果 老规矩，几乎所有 hash 的指令都以 h 开头 添加，批量添加，hash 没有修改，覆盖就是修改 1234127.0.0.1:6379&gt; hset user name zhanghanzhe(integer) 1127.0.0.1:6379&gt; hmset user sex nan age 21OK 获取，批量获取 1234567# 通过key获取value127.0.0.1:6379&gt; hget user name"zhanghanzhe"127.0.0.1:6379&gt; hmget user name sex age1) "zhanghanzhe"2) "nan"3) "21" 12345678910# 获取所有key127.0.0.1:6379&gt; hkeys user1) "name"2) "sex"3) "age"# 获取所有value127.0.0.1:6379&gt; hvals user1) "zhanghanzhe"2) "nan"3) "21" 12345678# 获取所有的key和value127.0.0.1:6379&gt; hgetall user1) "name"2) "zhanghanzhe"3) "sex"4) "nan"5) "age"6) "21" 删除指令 1234127.0.0.1:6379&gt; hdel user name(integer) 1127.0.0.1:6379&gt; hdel user sex age(integer) 2 自增和自减 123456127.0.0.1:6379&gt; hset map key1 5(integer) 1127.0.0.1:6379&gt; hincrby map key1 2(integer) 7127.0.0.1:6379&gt; hincrby map key1 -3(integer) 4 判断是否存在，存在返回 1，不存在返回 2 12127.0.0.1:6379&gt; hexists map key1(integer) 1 Zset 有序集合zset 有序集合和 set 集合类似，都是不允许重复的值出现，只不过相比于 set 集合，zset 多了一个排序的功能，在添加值得时候需要给他一个 分值，分值 可以重复但 value 不允许重复，默认按照分值从小到大排序 老规矩，几乎所有 zset 的指令都以 z开头 添加元素 zadd，第一个是分值，第二个是 value 1234127.0.0.1:6379&gt; zadd user 1 zhang(integer) 1127.0.0.1:6379&gt; zadd user 2 han 3 zhe(integer) 2 删除元素 12127.0.0.1:6379&gt; zrem user zhang han zhe(integer) 3 查看集合 123456789101112131415161718# 正常查看集合127.0.0.1:6379&gt; zrange user 0 -11) "zhang"2) "han"3) "zhe"# 倒序查看集合127.0.0.1:6379&gt; zrevrange user 0 -11) "zhe"2) "han"3) "zhang"# 查看带有分值的集合127.0.0.1:6379&gt; zrange user 0 -1 withscores1) "zhang"2) "1"3) "han"4) "2"5) "zhe"6) "3" 获取集合内元素的数量 zcard zcount 这里需要注意：==- inf 代表无穷小，+ inf 代表无穷大== 12345678910127.0.0.1:6379&gt; zadd user 10 zhang 12 wang 18 li 20 zhao 28 bai 30 guo(integer) 6# 获取当前集合数量127.0.0.1:6379&gt; zcard user(integer) 6# 获取当前集合分值符合区间内的数量127.0.0.1:6379&gt; zcount user -inf +inf(integer) 6127.0.0.1:6379&gt; zcount user 10 20(integer) 4 三大特殊类型地理位置Geospatial 类型用来存储有关经纬度的地理位置信息，默认经度第一位，维度第二位，redis 中针对经纬度的存储范围有一定的限制： 经度有效范围：-180 ~ 180 维度有效范围：-85.05112878 ~ 85.05112878 老规矩，几乎所有指令都是以 geo开头的 添加地理位置信息 geoadd 在添加经纬度的时候如果超出了范围会报错 1234127.0.0.1:6379&gt; geoadd point 126.64 45.75 hei(integer) 1127.0.0.1:6379&gt; geoadd point 125.32 43.88 ji 123.42 41.79 liao(integer) 2 查看指定地区的经纬度 ( 已存入的 )，同理，添加也是修改 12345678910127.0.0.1:6379&gt; geopos point ji1) 1) "125.32000154256820679" 2) "43.87999897829567431"127.0.0.1:6379&gt; geopos point hei ji liao1) 1) "126.64000242948532104" 2) "45.74999965248261447"2) 1) "125.32000154256820679" 2) "43.87999897829567431"3) 1) "123.41999977827072144" 2) "41.78999971580505246" 计算两地的距离 geodist 使用 geodist 获取距离是通过经纬度计算出来的 直线距离，并不是路途距离，默认获取单位是米，可以通过制定后缀来设置获取的单位： m/米，km/千米，mi英里，ft英尺 12345# 获取吉林到黑龙江的直线距离127.0.0.1:6379&gt; geodist point hei ji"232604.0065"127.0.0.1:6379&gt; geodist point hei ji km"232.6040" 雷达方式获取目标经纬度附近的地理位置 georadius 雷达方式，以经纬度为中心，距离为半径按照圆形扫描，类似微信附近的人功能 withdist距离km，withdist经纬度，count 1 显示第一个符合要求的 1234# 返回经度125维度43附近300km的位置信息127.0.0.1:6379&gt; georadius point 125 43 300 km1) "ji"2) "liao" 123456# 多要求获取127.0.0.1:6379&gt; georadius point 125 43 300 km withdist withcoord count 11) 1) "ji" 2) "101.2331" 3) 1) "125.32000154256820679" 2) "43.87999897829567431" 1234# 使用已存在的地理位置进行获取127.0.0.1:6379&gt; georadiusbymember point ji 250 km1) "ji"2) "hei" 移除元素 Geospatial 类型比较特殊，他没有给咱们提供移除的指令，但是 Geospatial 的底层是基于 zset 实现的，我们可以通过 zset 来移除指定的 key 即可。 12345678127.0.0.1:6379&gt; zrange point 0 -11) "liao"2) "ji"3) "hei"127.0.0.1:6379&gt; zrem point hei(integer) 1127.0.0.1:6379&gt; geopos point hei1) (nil) 基数统计hyperloglog 是一种专门用来统计基数的类型，当然其中的元素不允许重复，相比于 set 集合，例如计算网站访问量时，可以交给 hyperloglog 进行处理，他的内存占用是固定的 12KB。 但是因为他仅仅是计算基数的类型，所以并不能像 set 集合一样，获取到元素的具体的值，而且该类型有一定的误差，如果对精准度没有太大要求，那么推荐使用 hyperloglog 老规矩，几乎所有指令都是以 pf开头的 添加元素 pfadd 1234127.0.0.1:6379&gt; pfadd num1 1 2 3 4 5(integer) 1127.0.0.1:6379&gt; pfadd num2 3 4 5 6 7(integer) 1 查看元素个数 12127.0.0.1:6379&gt; pfcount num1(integer) 5 将两个集合合并为一个集合 1234127.0.0.1:6379&gt; pfmerge num3 num1 num2OK127.0.0.1:6379&gt; pfcount num3(integer) 7 进制存储bitmap 是基于二进制进行存储的，二进制只有 0 和 1 两个值，可以分别用来代表两种相对不同的状态，例如 打卡 &lt;=&gt; 未打卡，可以抽象的理解为 boolean 类型中的 true 和 false 的感觉。 命令几乎都以 bit 结尾 添加元素 [ 0 代表未打卡，1 代表打卡 ] 12345678127.0.0.1:6379&gt; setbit sign 0 1(integer) 1127.0.0.1:6379&gt; setbit sign 1 1(integer) 0127.0.0.1:6379&gt; setbit sign 2 0(integer) 1127.0.0.1:6379&gt; setbit sign 3 1(integer) 1 查看某个状态 — 返回 1 代表打卡，0 代表未打卡 12127.0.0.1:6379&gt; getbit sign 1(integer) 1 查看多少人符合条件(即有多少个1) 12127.0.0.1:6379&gt; bitcount sign(integer) 3 Redis 的事务事务就是一组命令的集合，将平时多次执行的命令放在一起，然后按照命令顺序依次执行，而且执行过程中不会被干扰，事务执行结束后不会保留，也就意味着每次执行事务都需要重新创建，可得出 redis 事务的三个特点：顺序性，排他性，一次性。 还有几点需要注意：==redis 的事务中，没有原子性和隔离性的概念，也不包含回滚==，所有命令在加入事务的时候，并没有直接执行，而是被放在了执行的队列中，也就不存在隔离性。 事务涉及到的关键字：开启/multi，执行/exec，放弃/discard，监视/watch，关闭监视/unwatch 使用事务 事务的执行 1234567891011121314# 当输入multi的时候就表示事务开始了，当使用exec的时候，就表示要执行了127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 v1QUEUED127.0.0.1:6379&gt; mset k2 v2 k3 v3QUEUED127.0.0.1:6379&gt; exec1) OK2) OK127.0.0.1:6379&gt; mget k1 k2 k31) "v1"2) "v2"3) "v3" 123456789# 当我事务添加中途，不想执行了，那么可以使用discard命令来放弃当前事务127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 zhangQUEUED127.0.0.1:6379&gt; discardOK127.0.0.1:6379&gt; get k1"v1" 事务的异常处理机制 和 java 有点类似，redis 针对事务也分所谓的 编译时异常 和 运行时异常，只不过这里的 编译时异常 指的是命令是否正确，针对不同的异常，redis 事务处理的方式也不一致 命令错误的处理方式 123456789101112# 当我在执行命令的时候，命令输入有误，这时整个事务都不会执行127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set user zhangQUEUED127.0.0.1:6379&gt; sett name hanzhe(error) ERR unknown command `sett`, with args beginning with: `name`, `hanzhe`, 127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors.# 因为事务报错，所以user并没有存进去，哪怕他是第一条命令127.0.0.1:6379&gt; get user(nil) 逻辑错误的处理方式 12345678910111213141516171819# 自增的变量是字符而不是数字127.0.0.1:6379&gt; set money zhangOK# 然后我们再来执行事务127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; incrby money 3000QUEUED127.0.0.1:6379&gt; decrby money 188QUEUED127.0.0.1:6379&gt; set user zhangQUEUED127.0.0.1:6379&gt; exec # 他会直接提示前两句执行失败，但是第三句执行成功1) (error) ERR value is not an integer or out of range2) (error) ERR value is not an integer or out of range3) OK# 这里也是可以获取到的，哪怕他是最后一条，不符合原子性127.0.0.1:6379&gt; get user"zhang" 因为 redis 的事务管理并不严格，所以 redis 的事务又被人戏称 伪事务 乐观锁在我们执行事务的时候，如果是处在多线程的环境下，我通过事务对某个数据进行改变，但是在我命令缓存完成还没有执行的时候，另一条线程进来对这个数据进行了修改，那么就会发生难以想象的改变。 模拟举例说明： 1234567## 线程1 启动事务127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; incrby money 3000QUEUED127.0.0.1:6379&gt; decrby money 188 QUEUED 123## 线程2 中途插入127.0.0.1:6379&gt; set money 200000(integer) 200100 1234## 线程1 这时刚刚提交事务127.0.0.1:6379&gt; exec1) (integer) 2030002) (integer) 202812 可以发现，在事务执行的过程中数据被改变了，结果也造成影响了 什么是乐观锁？ 关于锁，有两个概念，一个是 悲观锁，还有一个是 乐观锁，悲观锁类似 java 中的多线程锁 synchronized，将整个方法上锁，无论是否有多条线程访问都会工作，在安全的前提下影响了性能，而 乐观锁 是在指令添加前将被操作的那个 key 监视起来，如果在执行的时候发现目标 key 发生了改变，那么就将当前事务取消不执行。 1234567891011## 线程1# 在启动事务之前，监视目标key127.0.0.1:6379&gt; watch moneyOK# 启动事务127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; incrby money 3000QUEUED127.0.0.1:6379&gt; decrby money 188QUEUED 123## 线程2 中途插入127.0.0.1:6379&gt; incrby money 200000(integer) 200100 123## 线程1 这时刚刚提交事务127.0.0.1:6379&gt; exec(nil) 可以发现，被监视的 key 发生改变后，事务执行就被中断了，那么之后应该如何处理呢？ 乐观锁善后 12345678# 1.关闭当前事务127.0.0.1:6379&gt; discardOK# 关闭监控127.0.0.1:6379&gt; unwatchOK## 重新监控，再走一遍事务逻辑.....## 在执行前在进行比对.... 持久化保存策略我们都是到，redis 是操作内存的数据库，正因为这样的特点才让他的存取速度特别快，但是在内存中存放的数据，当我们重新启动服务器的时候就会丢失，这个时候就需要接触到 持久化 这个技术了，持久化就是把当前进程数据生成快照保存到硬盘的过程。 redis 的持久化操作分为两种，分别为 RDB 和 AOF，两种方式只能同时使用一种，redis 使用的 默认持久化方式为 RDB， RDB 方式RDB ( Redis DataBase ) 持久方式，会单起一条线程，在指定的时间间隔内将内存中的数据以二进制形式写入、临时文件中，写入成功后默认存放到 redis 的安装目录下的 dump.rdb 文件中，如果你使用自己的配置启动的 redis，那么 dump.rdb 会和你的配置文件同级。 RDB 的持久化有两种触发机制，一种是手动命令持久化，一种是自动持久化。 手动持久化 save，bgsave 1234127.0.0.1:6379&gt; saveOK127.0.0.1:6379&gt; bgsaveBackground saving started save 命令会阻塞当前 redis 服务器，期间不能正常提供服务，这一现象直至数据保存完毕后恢复正常。 bgsave 会执行 fork 子进程负责持久化操作，在创建子进程的时候会有短暂的阻塞，时间很短。 除开这种主动的持久化之外，一些其他的命令也会完成持久化的操作，例如：flushall，shutdown 等等。 自动持久化 配置文件中的 SNAPSHOTTING 模块就是用来做 RDB 持久化的，里面有这样几句配置命令： 12345678910111213141516################################ SNAPSHOTTING ################################ 快照# save 时间/s 数据修改次数# 例如 save 900 1，如果数据只修改了一次，那么就900秒持久化一次save 900 1save 300 10save 60 10000# 当RDB最后一次保存失败后，是否停止接受数据，默认yes (否则没人知道它坏了)stop-writes-on-bgsave-error yes# 是否以压缩形式保存，默认为yesrdbcompression yes# 是否效验数据完整性，默认为yesrdbchecksum yes# 持久化保存文件名dbfilename dump.rdb# 持久化文件保存路径dir ./ 一般我们不需要该他的配置文件，预设的就够用了。 #####AOF 方式同样还是在配置文件中，APPEND ONLY MODE 模块就是负责 AOF 持久化的，和 RDB 不同， AOF 的原理是将所有曾经使用过的存入操作的命令都记录下来，存放到 appendonly.aof 文件中，是可以看个模糊的大概的。 AOF 持久化配置 AOF 也可以手动触发，只需要执行 bgrewriteaof 命令即可，但是一般都用配置文件管理，配置信息如下： 123456789101112############################## APPEND ONLY MODE ############################### 仅附加模式# 是否开启AOF，默认为noappendonly no# 指定更新条件，可以选择三个值： # no：每次修改都同步，数据完整性强，性能偏低 # always：每秒同步一次，数据完整性较好 # everysec：让操作系统自己同步数据，消耗资源最低appendfsync everysec# 百分比，如果数据文件大小占据了指定百分比，会触发重写。auto-aof-rewrite-percentage 100# 必须满足最小大小才可以重写auto-aof-rewrite-min-size 64mb 比较RDB和AOFRDB：RDB 是二进制文件，按照时间进行数据同步，每次同步都会执行 fork 操作，如果为了追求数据数据完整性不停的同步，会极大的影响 redis 工作效率，更==适合做定期备份，用于灾难恢复== AOF：AOF 通过记录命令实现持久化，通过控制参数可以精确到秒级。 # 有关于持久化策略，以后再细学 文件损坏修复当我们的数据文件损坏导致 redis 无法启动的时候，我们可以尝试运行 redis 的修复工具 12redis-check-aof --fix 配置文件redis-check-rdb --fix 配置文件 # 有关于持久化策略，以后再细学 Jedis整合使用所有程序均是以 maven 为基础搭建的 Java整合使用 java 项目整合 redis 进行操作，首先要添加 maven 依赖 正常操作 redis 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 添加依赖完成后，便可以直接通过 java 代码创建连接并操作数据库了 12345678910111213141516public class Demo1 &#123; public static void main(String[] args) &#123; // 创建连接 Jedis redis = new Jedis("IP地址", 6379); // 验证连接密码 redis.auth("你的密码"); // 清空当前数据库 redis.flushDB(); // 正常的增删改查 System.out.println(redis.ping()); System.out.println(redis.set("user", "zhang")); System.out.println(redis.get("user")); // 关闭连接 redis.close(); &#125;&#125; 直接创建 jedis 对象就可以创建远程连接，而且实例内的方法和 redis 的指令用法等等几乎一模一样，非常好用。 redis 连接池 一般来说 jedis 已经可以很好的操作 redis 数据库了，但是在项目中如果频繁的创建和关闭连接，是很耗费服务器资源的，所以这里可以使用 jedis 的连接池进行操作 123456789101112131415// 使用默认的连接池操作public class JedisPoolDemo &#123; public static void main(String[] args) &#123; // 创建jedis连接池对象 JedisPool pool = new JedisPool("IP地址", 6379); // 从连接池中获取连接 Jedis redis = pool.getResource(); // 正常操作jedis redis.auth("密码"); redis.set("name", "hanzhe"); System.out.println(redis.get("name")); // 关闭连接 redis.close(); &#125;&#125; 123456789101112131415161718// 使用自定义配置的连接池public class JedisPoolDemo &#123; public static void main(String[] args) &#123; // 创建连接池配置对象 JedisPoolConfig poolConfig = new JedisPoolConfig(); // 设置最大连接数 poolConfig.setMaxTotal(50); // 设置最大空闲连接数 poolConfig.setMaxIdle(10); // 按照指定的配置创建连接 JedisPool pool = new JedisPool(poolConfig, "IP地址", 6379); // 获取连接后面的操作就是一样的了 Jedis redis = pool.getResource(); // ........ // 关闭连接 redis.close(); &#125;&#125; 检查 redis 数据库 1234567127.0.0.1:6379&gt; keys *1) "name"2) "user"127.0.0.1:6379&gt; mget user name1) "zhang"2) "hanzhe"127.0.0.1:6379&gt; 经过检查发现，java 操作 redis 不存在其他的问题，整合基本完成。 控制台打印警告问题 123SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. 如果控制台打印如上的警报信息，可以引入 slf4j 的 maven 依赖进行解决 12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt; Springboot整合springboot 整合同样需要引入对应的 maven 依赖，和 jedis 有些许的不同，springboot 中封装的类并不可以直接调用类似命令的函数，而是对他们进行了二次封装 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 修改配置文件 12345678910# IP地址spring.redis.host=IP地址# 端口spring.redis.port=6379# 访问密码spring.redis.password=密码# 最大连接数spring.redis.pool.max-active=50# 最大空闲连接数spring.redis.pool.max-idle=10 简单操作数据库 操作数据库，需要注入 RedisTemplate 对象进行操作 123456789101112@SpringBootTestclass Redis2SpringbootApplicationTests &#123; // 自动注入对象 @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() &#123; redisTemplate.opsForValue().set("user", "zhang"); // 可以正常读取存入的 user 信息 System.out.println(redisTemplate.opsForValue().get("user")); &#125;&#125; RedisTemplate 将数据类型对应的指令函数分别命名为 opsForValue()，opsForList()，opsForSet()，opsForHash()，opsForHyperLogLog() ，opsForZSet() 高级操作数据库 上面的方法封装了各种数据类型的简单操作，接下来就是一些高级的操作了。 123456789101112131415161718@SpringBootTestclass Redis2SpringbootApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() &#123; // 获取当前连接 RedisConnectionFactory factory = redisTemplate.getConnectionFactory(); RedisConnection connection = factory.getConnection(); // ping测试，清空库，停止服务，关闭连接等等高级操作 System.out.println(connection.ping()); connection.flushDb(); connection.flushAll(); connection.shutdown(); connection.close(); // 开启事务等等相关设置也在这里 &#125;&#125; 乱码问题 123// Java 代码redisTemplate.opsForValue().set("name", "zhang");System.out.println(redisTemplate.opsForValue().get("name")); 123# redis 数据库127.0.0.1:6379&gt; keys *1) "\xac\xed\x00\x05t\x00\x04name" 我们通过 springboot 向 redis 中插入一个字符串，发现存进去的字符串存在乱码问题，这时候我们可以通过使用 RedisTemplate 的子类 StringRedisTemplate 进行操作 12345678910@SpringBootTestclass Redis2SpringbootApplicationTests &#123; @Autowired private StringRedisTemplate string; @Test void contextLoads() &#123; string.opsForValue().set("name", "zhang"); System.out.println(string.opsForValue().get("name")); &#125;&#125; 123456# name 也可以正常存取127.0.0.1:6379&gt; keys *1) "name"2) "\xac\xed\x00\x05t\x00\x04name"127.0.0.1:6379&gt; get name"zhang" 自定义 RedisTemplate 字符串可以通过 StringRedisTemplate 来解决问题，但是存入其他类型的还是会出现问题，这时可自己创建一个 RedisTemplate 来代替原本的类工作。代码来自互联网 1.首先要引入 maven 依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt;&lt;/dependency&gt; 2.创建配置类 123456789101112131415161718192021222324@Configurationpublic class RedisUtil &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate(); template.setConnectionFactory(factory); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); // key采用String的序列化方式 template.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 template.setHashKeySerializer(stringRedisSerializer); // value序列化方式采用jackson template.setValueSerializer(jackson2JsonRedisSerializer); // hash的value序列化方式采用jackson template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; &#125;&#125; 这个时候自定义的 就已经完成了，现在再来测试一遍是否乱码 12345678910@SpringBootTestclass Redis2SpringbootApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() &#123; redisTemplate.opsForValue().set("name", "zhang"); System.out.println(redisTemplate.opsForValue().get("name")); &#125;&#125; 12345# 乱码问题已经解决，这个特殊符号是字符串的转义127.0.0.1:6379&gt; keys *1) "name"127.0.0.1:6379&gt; get name"\"zhang\"" 通过序列化将对象存储到 redis 中 123456// implements Serializable 序列化接口一定要实现public class Person implements Serializable &#123; private String username; private String password; /* 省略get-set-toString方法 */&#125; 1234567891011121314151617@SpringBootTestclass Redis2SpringbootApplicationTests &#123; @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() &#123; Person p = new Person(); p.setUsername("张涵哲"); p.setPassword("zhang"); redisTemplate.opsForValue().set("user", p); System.out.println(redisTemplate.opsForValue().get("user")); &#125;&#125;/* 可以正常读取 Person&#123;username='张涵哲', password='zhang'&#125;*/ 订阅与发布redis 的订阅发布是一种通讯模式，分别为 发送者/sub 和 订阅者/pub 两种身份，发布者负责发送一些信息，然后由订阅者接收，涉及到的命令也非常少 最常用：简单使用订阅和发布完成交互 subscribe，publish 123456# 订阅者 当我订阅s1频道的时候，命令行就会被锁定，静等s1频道发送信息127.0.0.1:6379&gt; subscribe s1Reading messages... (press Ctrl-C to quit)1) "subscribe"2) "s1"3) (integer) 1 123# 发布者 只需要在固定的频道发送消息即可，不用考虑订阅者状态127.0.0.1:6379&gt; publish s1 haha(integer) 1 1234# 订阅者 命令行状态发生改变：1) "message" # 提示接受到了消息2) "s1" # 如果订阅了多个，在这里区分发布者频道3) "haha" # 频道发送的消息 主从复制在我们的项目越做越大的情况下，一个 redis 服务可能已经不支持我们的读写效率了，这个时候我们需要配置多个服务器，在每个服务器上都配置 redis 的环境，让他们分别为一个程序提供服务，这种工作方式被称之为集群 在多个服务器中选中一台服务器为主机 ( Master )，其他为从机 ( Slave )，主机负责写入数据 ( set.. )，而从机负责读取数据 ( get )，为了多个服务器之间数据同步的问题，所以有了主从复制的技术。 这里可以通过复制多个配置文件，修改端口号来实现 伪集群，设置 6379 为主机，6380 为从机，后面简称为 79 和 80 通过命令实现选择 79 为主机，连接主机输入 info replication 命令即可查看当前机器配置状态 123456789101112127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:0master_replid:49e04dfecc70d4b55f17798d80b1d77ac4289c12master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 第二行位置显示 role:master，代表当前端口是主机，==redis 默认每台机器都是主机==，配置主从复制只需要配置从机就可以了。 配置从机 slaveof 认主，只需要找到目标 Redis 服务器作为自己的主人就可以了，这样一来从机就可以获取到主机的数据了。 1234567891011121314151617181920212223# 80，81 端口127.0.0.1:6380&gt; slaveof localhost 6379OK127.0.0.1:6380&gt; info replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:upmaster_last_io_seconds_ago:7master_sync_in_progress:0slave_repl_offset:434slave_priority:100slave_read_only:1connected_slaves:0master_replid:5ec4f58da9447b03e682d76edb61cbf5e4cf89a2master_replid2:0000000000000000000000000000000000000000master_repl_offset:434second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:434]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql的join实现]]></title>
    <url>%2F2020%2F08%2F14%2FMySQL%2FMySql%E7%9A%84join%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[$Nested-Loop\ Join$ $MySql$只支持一种join算法：$Nested-Loop Join$（嵌套循环连接），但$Nested-Loop\ Join$有三种变种：$Simple\ Nested-Loop\ Join，Index\ Nested-Loop\ Join，Block\ Nested-Loop Join$(简单-索引-缓冲区) $Simple\ Nested-Loop\ Join：$如图，$R$为驱动表，$S$为匹配表，可以看到从r中分别取出$r_1、r_2、……、r_n$去匹配$S$表的左右列，然后再合并数据，对$S$表进行了$r*n$次访问，对数据库开销大: $Index\ Nested-Loop\ Join$（索引嵌套）：在查询时，如果被驱动表$S$的关联字段是有索引的，那么查询就要快很多 $Block\ Nested-Loop\ Join:$ 如果有索引，会选取第二种方式进行join，但如果join列没有索引，就会采用$Block\ Nested-Loop\ Join$。可以看到中间有个$join\ buffer$缓冲区，是将驱动表的所有join相关的列(这里不仅仅指用来join的列，还包含select的列)都先缓存到join buffer中，然后批量与匹配表进行匹配，将第一种多次比较合并为一次，降低了非驱动表$S$的访问频率。默认情况下$join_buffer_size=256K$，在查找的时候$MySQL$会将所有的需要的列缓存到$join\ buffer$当中。在一个有N个JOIN关联的SQL当中会在执行时候分配N-1个$join\ buffer$。 具体做法如下： 先从表$R$中取出满足条件的数据，放入$join\ buffer$中 遍历表$S$,每次取出一条数据与$join\ buffer$中的数据进行关联字段的比较。满足条件的进入结果集 如果$join\ buffer$中一次性放不下所有满足条件的数据，那么以上两步需要重复进行]]></content>
      <categories>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[$Spark$解析$DataFrame$中的$json\ string$字段]]></title>
    <url>%2F2020%2F08%2F03%2FSpark%2FHow%20to%20parse%20a%20column%20of%20json%20string%20in%20Pyspark%2F</url>
    <content type="text"><![CDATA[How to parse a column of json string in Pyspark 在用$spark.sql(\ )$从Table读入数据时，DataFrame的列有时是这样一种类型：json形式的string。此时，我们通常需要去解析这个json string，从而提取我们想要的数据。 数据准备12345# Sample Data Framejstr1 = u'&#123;"header":&#123;"id":12345,"foo":"bar"&#125;,"body":&#123;"id":111000,"name":"foobar","sub_json":&#123;"id":54321,"sub_sub_json":&#123;"col1":20,"col2":"somethong"&#125;&#125;&#125;&#125;'jstr2 = u'&#123;"header":&#123;"id":12346,"foo":"baz"&#125;,"body":&#123;"id":111002,"name":"barfoo","sub_json":&#123;"id":23456,"sub_sub_json":&#123;"col1":30,"col2":"something else"&#125;&#125;&#125;&#125;'jstr3 = u'&#123;"header":&#123;"id":43256,"foo":"foobaz"&#125;,"body":&#123;"id":20192,"name":"bazbar","sub_json":&#123;"id":39283,"sub_sub_json":&#123;"col1":50,"col2":"another thing"&#125;&#125;&#125;&#125;'df = spark.createDataFrame([Row(json=jstr1),Row(json=jstr2),Row(json=jstr3)]) 如上所示，我们模拟一个DataFrame，其中只有一列，列名为json，类型为string。可以看到，json中的值为json格式。我们如何从中取出我们关心的值，形成一个单独的列呢？例如：$df[‘header’][‘id’]$. from_json函数123456from pyspark import Rowfrom pyspark.sql.functions import from_json, coljson_schema = spark.read.json(df.select('json').rdd.map(lambda row: row.json)).schemadf_json = df.withColumn('json', from_json(col('json'), json_schema))print(json_schema) $Result:$ 123456789101112root |-- body: struct (nullable = true) | |-- id: long (nullable = true) | |-- name: string (nullable = true) | |-- sub_json: struct (nullable = true) | | |-- id: long (nullable = true) | | |-- sub_sub_json: struct (nullable = true) | | | |-- col1: long (nullable = true) | | | |-- col2: string (nullable = true) |-- header: struct (nullable = true) | |-- foo: string (nullable = true) | |-- id: long (nullable = true) 1df_json.select(col('json.header.id').alias('id')).show() $Result:$ 1234567+-----+| id|+-----+|12345||12346||43256|+-----+ 1df_json.select(col('json.header.id').alias('id'), col('json.body.name').alias('name')).show() $Result:$ 1234567+-----+-------+| id| name|+-----+-------+|12345| foobar||12346| barfoo||43256| bazbar|+-----+-------+ 参考链接]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础2]]></title>
    <url>%2F2020%2F07%2F25%2FJava%2FJava%E5%9F%BA%E7%A1%802%2F</url>
    <content type="text"><![CDATA[Java程序的结构首先，我们来看一段完整的Java程序： 123456789101112/** * 可以用来自动创建文档的注释 */public class Hello &#123; public static void main(String[] args) &#123; // 向屏幕输出文本: System.out.println("Hello, world!"); /* 多行注释开始 注释内容 注释结束 */ &#125;&#125; // class定义结束 $class$关键字因为Java是面向对象的语言，一个程序的基本单位就是class，class是关键字，这里定义的class名字就是Hello： 类名要求： 类名必须以英文字母开头，后接字母，数字和下划线的组合 习惯以大写字母开头 注意到public是访问修饰符，表示该class是公开的。 不写public，也能正确编译，但是这个类将无法从命令行执行。 Java中的命名规范 包名：多单词组成时，所有的字母都小写：xxxyyyzzz 类名、接口名：多单词组成时，所有单词的首字母大写：XxxYyyZzz 变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词的首字母大写：xxxYyyZzz 常量名：所有字母都大写，多单词时每个单词用下划线链接，XXX_YYY_ZZZ 方法在class内部，可以定义若干方法（method）： 12345public class Hello &#123; public static void main(String[] args) &#123; // 方法名是main // 方法代码... &#125; // 方法定义结束&#125; 在方法内部，语句才是真正的执行代码。Java的每一行语句必须以分号结。这里的方法名是main，返回值是void，表示没有任何返回值。 我们注意到public除了可以修饰class外，也可以修饰方法。而关键字static是另一个修饰符，它表示静态方法，后面我们会讲解方法的类型，目前，我们只需要知道，Java入口程序规定的方法必须是静态方法，方法名必须为main，括号内的参数必须是String数组。 注释Java有3种注释，第一种是单行注释，以双斜线开头，直到这一行的结尾结束： 1// 这是注释... 而多行注释以/*星号开头，以*/结束，可以有多行： 12345/*这是注释blablabla...这也是注释*/ 最后一种是文档注释: 12345678910/** * 可以用来自动创建文档的注释 * * @auther liaoxuefeng */public class Hello &#123; public static void main(String[] args) &#123; System.out.println("Hello, world!"); &#125;&#125; 这种特殊的多行注释需要写在类和方法的定义处，可以用于自动创建文档。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础1]]></title>
    <url>%2F2020%2F07%2F17%2FJava%2FJava%E5%9F%BA%E7%A1%801%2F</url>
    <content type="text"><![CDATA[Java简介 Java最早是由SUN公司（已被Oracle收购）的詹姆斯·高斯林（高司令，人称Java之父）在上个世纪90年代初开发的一种编程语言，最初被命名为Oak，目标是针对小型家电设备的嵌入式应用，结果市场没啥反响。谁料到互联网的崛起，让Oak重新焕发了生机，于是SUN公司改造了Oak，在1995年以Java的名称正式发布，原因是Oak已经被人注册了，因此SUN注册了Java这个商标。随着互联网的高速发展，Java逐渐成为最重要的网络编程语言。 Java介于编译型语言和解释型语言之间。编译型语言如C、C++，代码是直接编译成机器码执行，但是不同的平台（x86、ARM等）CPU的指令集不同，因此，需要编译出每一种平台的对应机器码。解释型语言如Python、Ruby没有这个问题，可以由解释器直接加载源码然后运行，代价是运行效率太低。而Java是将代码编译成一种“字节码”，它类似于抽象的CPU指令，然后，针对不同平台编写虚拟机，不同平台的虚拟机负责加载字节码并执行，这样就实现了“一次编写，到处运行”的效果。当然，这是针对Java开发者而言。对于虚拟机，需要为每个平台分别开发。为了保证不同平台、不同公司开发的虚拟机都能正确执行Java字节码，SUN公司制定了一系列的Java虚拟机规范。从实践的角度看，JVM的兼容性做得非常好，低版本的Java字节码完全可以正常运行在高版本的JVM上。 Java体系Java一共分为三个体系，分别应用于不同的场景; $JavaSE(J2SE)$ — （Java2 Platform Standard Edition，java平台标准版） $JavaEE(J2EE)$ — (Java 2 Platform Enterprise Edition，java平台企业版)​ $JavaME(J2ME)$ — (Java 2 Platform Micro Edition，java平台微型版) 补充：2005年6月，JavaOne大会召开，SUN公司公开Java SE 6。此时，Java的各种版本已经更名以取消其中的数字”2”：J2EE更名为Java EE, J2SE更名为Java SE，J2ME更名为Java ME。 三者之间的关系： 简单来说，Java SE就是标准版，包含标准的JVM和标准库，而Java EE是企业版，它只是在Java SE的基础上加上了大量的API和库，以便方便开发Web应用、数据库、消息服务等，Java EE的应用使用的虚拟机和Java SE完全相同。 Java ME就和Java SE不同，它是一个针对嵌入式设备的瘦身版，Java SE的标准库无法在Java ME上使用，Java ME的虚拟机也是“瘦身版”。 毫无疑问，Java SE是整个Java平台的核心，而Java EE是进一步学习Web应用所必须的。我们熟悉的Spring等框架都是Java EE开源生态系统的一部分。不幸的是，Java ME从来没有真正流行起来，反而是Android开发成为了移动平台的标准之一，因此，没有特殊需求，不建议学习Java ME。 Java学习路线图如下： 首先要学习Java SE，掌握Java语言本身、Java核心开发技术以及Java标准库的使用； 如果继续学习Java EE，那么Spring框架、数据库开发、分布式架构就是需要学习的； 如果要学习大数据开发，那么Hadoop、Spark、Flink这些大数据平台就是需要学习的，他们都基于Java或Scala开发； 如果想要学习移动开发，那么就深入Android平台，掌握Android App开发。 Java版本 版本 时间 1.0 1995 1.2 1998 1.3 2000 1.4 2002 1.5 / 5.0 2004 1.6 / 6.0 2005 1.7 / 7.0 2011 1.8 / 8.0 2014 1.9 / 9.0 2017/9 10 2018/3 11 2018/9 补充： 2004年9月30日18:00PM，J2SE1.5发布，成为Java语言发展史上的又一里程碑。为了表示该版本的重要性，J2SE1.5更名为Java SE 5.0 2005年6月，JavaOne大会召开，SUN公司公开Java SE 6。此时，Java的各种版本已经更名，以取消其中的数字”2”：J2EE更名为Java EE，J2SE更名为Java SE，J2ME更名为Java ME 2009年04月20日，甲骨文74亿美元收购Sun。取得java的版权。 JRE/JVM/JDK JDK：Java Develpment Kit java 开发工具 JRE：Java Runtime Environment java运行时环境 JVM：java Virtual Machine java 虚拟机 简单地说，JRE就是运行Java字节码的虚拟机。但是，如果只有Java源码，要编译成Java字节码，就需要JDK，因为JDK除了包含JRE，还提供了编译器、调试器等开发工具。 三者关系如下： 1234567891011 ┌─ ┌──────────────────────────────────┐ │ │ Compiler, debugger, etc. │ │ └──────────────────────────────────┘JDK ┌─ ┌──────────────────────────────────┐ │ │ │ │ │ JRE │ JVM + Runtime Library │ │ │ │ │ └─ └─ └──────────────────────────────────┘ ┌───────┐┌───────┐┌───────┐┌───────┐ │Windows││ Linux ││ macOS ││others │ └───────┘└───────┘└───────┘└───────┘ JDK、JRE和JVM的区别与相互之间的联系]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beta分布和Thompson采样]]></title>
    <url>%2F2020%2F07%2F16%2FStatistics%2FBeta%E5%88%86%E5%B8%83%E5%92%8CThompson%E9%87%87%E6%A0%B7%2F</url>
    <content type="text"><![CDATA[$Beta$分布$Beta$分布是一个定义在[0,1]区间上的连续概率分布族，它有两个正值参数，称为形状参数，一般用$\alpha$和$\beta$表示 $Beta$分布的概率密度为： f(x ; \alpha, \beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}随机变量$X$服从参数为$\alpha, \beta$的$beta$分布，一般记作： X \sim \operatorname \\{Beta} (\alpha, \beta)$Beta$分布的期望： \frac{\alpha}{\alpha + \beta}$Beta$分布的方差： \frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}$Beta$分布的概率密度图: 从$Beta$分布的概率密度函数的图形我们可以看出，$Beta$分布有很多种形状，但都是在$0~1$区间内，因此$Beta$分布可以描述各种$0~1$区间内的形状（事件）。因此，它特别适合为某件事发生或者成功的概率建模。同时，当$α=1，β=1$的时候，它就是一个均匀分布。 123456789101112131415from scipy.stats import beta import matplotlib.pyplot as plt import numpy as np x = np.linspace(0, 1, 100) a_array = [1, 2, 4, 8] b_array = [1, 2, 4, 8] for i, a in enumerate(a_array): for j, b in enumerate(b_array): plt.plot(x, beta.pdf(x, a, b), lw=1, alpha=0.6, label='a='+str(a)+',b='+str(b)) plt.legend(frameon=False) plt.show() 贝塔分布主要有 $α$和 $β$两个参数，这两个参数决定了分布的形状，从上图及其均值和方差的公式可以看出： $α/(α+β)$也就是均值，其越大，概率密度分布的中心位置越靠近1，依据此概率分布产生的随机数也多说都靠近1，反之则都靠近0。 $α+β$越大，则分布越窄，也就是集中度越高，这样产生的随机数更接近中心位置，从方差公式上也能看出来。 案例$Beta$分布可以看作是一个概率的概率分布(如硬币正面朝上的概率的分布)，当我们不知道一个东西的具体概率是多少时，它给出了所有概率出现的可能性大小，可以理解为概率的概率分布—贝叶斯的思维。 以棒球为例子： 熟悉棒球运动的都知道有一个指标就是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。 现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。 对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。 接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取$α=81,β=219$ 之所以取这两个参数是因为： beta分布的均值是$\frac{\alpha}{\alpha+\beta}=\frac{81}{81+219}=0.27$ 从图中可以看到这个分布主要落在了$(0.2,0.35)$间，这是从经验中得出的合理的范围 在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率。也就是说beta分布可以看作一个概率的概率分布。 那么有了先验信息后，现在我们考虑一个运动员只打一次球，那么他现在的数据就是1中-1击。这时候我们就可以更新我们的分布了，让这个曲线做一些移动去适应我们的新信息。beta分布在数学上就给我们提供了这一性质，他与二项分布是共轭先验的。所谓共轭先验就是先验分布是beta分布，而后验分布同样是beta分布。结果很简单： \operatorname{Beta}\left(\alpha_{0}+\text { hits }, \beta_{0}+\text { misses }\right)如果我们得到了更多的数据，假设一共打了300次，其中击中了100次，200次没击中，那么这一新分布就是：$\operatorname{beta}(81+100,219+200)$ 可以看出，曲线更窄而且往右移动了（击球率更高），由此我们对于运动员的击球率有了更好的了解。新的贝塔分布的期望值为0.303，比直接计算$100/(100+200)=0.333$要低，是比赛季开始时的预计0.27要高，所以贝塔分布能够抛出掉一些偶然因素，比直接计算击球率更能客观反映球员的击球水平。 $Beta$分布和二项分布二项分布 P(\text {data} \mid \theta) \propto \theta^{z}(1-\theta)^{N-z}贝叶斯定理 P(\theta \mid d a t a)=\frac{P(d a t a \mid \theta) P(\theta)}{P(d a t a)} \propto P(d a t a \mid \theta) P(\theta)$P(\theta)$为$beta$分布时： \\P(\theta) = \operatorname{Beta}(a, b)=\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a, b)} \propto \theta^{a-1}(1-\theta)^{b-1}则此时的后验分布为: \begin{array}{c} P(\theta \mid d a t a) \propto \theta^{z}(1-\theta)^{N-z} * \theta^{a-1}(1-\theta)^{b-1} \\ \propto \theta^{a+z-1}(1-\theta)^{b+N-z-1} \end{array}令$a′=a+z,b′=b+N−z$： P(\theta \mid \text { data })=\frac{\theta^{a^{\prime}-1}(1-\theta)^{b^{\prime}-1}}{B\left(a^{\prime}, b^{\prime}\right)}可见，后验分布依然是一个$Beta$分布。所以，我们将$Beta$分布和二项分布称之为共轭分布 $Thompson$采样 $Thompson$采样的背后原理正是上述所讲的$Beta$分布，将$Beta$分布的 $\alpha$参数看成是推荐后用户点击的次数，把分布的 $\beta$ 参数看成是推荐后用户未点击的次数，则汤普森采样过程如下： 1、取出每一个候选对应的参数 a 和 b； 2、为每个候选用 a 和 b 作为参数，用贝塔分布产生一个随机数； 3、按照随机数排序，输出最大值对应的候选； 4、观察用户反馈，如果用户点击则将对应候选的 a 加 1，否则 b 加 1； $Thompson$采样为什么有效呢？ 如果一个候选被选中的次数很多，也就是 $a+b$ 很大了，它的分布会很窄，换句话说这个候选的收益已经非常确定了，就是说不管分布中心接近0还是1都几乎比较确定了。用它产生随机数，基本上就在中心位置附近，接近平均收益。 如果一个候选不但 a+b 很大，即分布很窄，而且 $a/(a+b) $也很大，接近 1，那就确定这是个好的候选项，平均收益很好，每次选择很占优势，就进入利用阶段。反之则有可能平均分布比较接近与0，几乎再无出头之日。 如果一个候选的 $a+b$ 很小，分布很宽，也就是没有被选择太多次，说明这个候选是好是坏还不太确定，那么分布就是跳跃的，这次可能好，下次就可能坏，也就是还有机会存在，没有完全抛弃。那么用它产生随机数就有可能得到一个较大的随机数，在排序时被优先输出，这就起到了前面说的探索作用。 代码实现： 1choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins))]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>Thompson</tag>
        <tag>Beta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一颗B+ Tree能存储多少条数据]]></title>
    <url>%2F2020%2F07%2F12%2FMySQL%2FInnoDB%E5%BC%95%E6%93%8E%E7%9A%84%E4%B8%80%E9%A2%97B%2B%20Tree%E8%83%BD%E5%AD%98%E5%82%A8%E5%A4%9A%E5%B0%91%E6%9D%A1%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[InnoDB中一颗B+ Tree能存储多少条数据磁盘的结构 在回答这个问题之前，我们先来复习一下计算机机械磁盘的结构，如下图： 磁头（head）：主要就是读取磁盘表面磁方向和改变其方向，每个盘面有一个磁头，它极其贴近地悬浮在盘面上，但是绝对不与盘面接触，否则会损坏磁头和盘面； 磁道（track）：磁道是单个盘面上的同心圆，当磁盘旋转时，磁头若保持在一个位置上，则每个磁头都会在磁盘表面划出一个圆形轨迹，这些圆形轨迹就叫做磁道，一个盘面上的磁道可以有成千上万个。相邻磁道之间并不是紧挨着的，这是因为磁化单元相隔太近时磁性会产生相互影响，同时也为磁头的读写带来困难。 柱面（cylinder）：在有多个盘片构成的盘组中，由不同盘片的面，但处于同一半径圆的多个磁道组成的一个圆柱面。 扇区（sector）：磁盘上的每个磁道被等分为若干个弧段，这些弧段便是硬盘的扇区$Sector$。硬盘的第一个扇区，叫做引导扇区。扇区是被间隙$gap$分割的圆的片段，间隙未被磁化成0或者1。注意，扇区是读写磁盘最基本的单位，如果一个扇区因为某种原因被破坏，那么整个扇区的数据都会受影响。一个机械硬盘扇区的容量一般为512字节 单盘面结构图： 磁盘容量 Megatron747磁盘是一个典型的vintage-2008的大容量驱动器，它具有以下特性：8个圆盘，16个盘面，每个盘面有65536个磁道，每个磁道（平均）有256个扇区，每个扇区可以存储4096个字节（byte） 那整个磁盘容量的算法是： 16个盘面，乘以65536个磁道，乘以256个扇区，再乘以4096字节，即1665536256*4096=2^40 byte，也就是1TB的容量。 InnoDB存储引擎 在计算机中，磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）的最小单元是块，一个块的大小是4k，而对于InnoDB存储引擎也有自己的最小储存单元，页（Page），一个页的大小是16K。 InnoDB的所有数据文件（后缀为ibd的文件），大小始终都是16384（16k）的整数倍。 磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。 ​ 考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 事实1： 不同容量的存储器，访问速度差异悬殊。 磁盘(ms级别) &lt;&lt; 内存(ns级别)， 100000倍 若内存访问需要1s，则一次外存访问需要一天 为了避免1次外存访问，宁愿访问内存100次…所以将最常用的数据存储在最快的存储器中 事实2 ： 从磁盘中读 1 B，与读写 1KB 的时间成本几乎一样 从以上数据中可以总结出一个道理，索引查询的数据主要受限于硬盘的I/O速度，查询I/O次数越少，速度越快。 $B+\ Tree$ 在InnoDB存储引擎中，每一个page对应B+ Tree的一个节点(Node) 根节点：InnoDB树的根节点由索引字段值和指针组成 叶节点：InnoDB的叶节点由索引字段值和对应的完整记录(row)组成 一般来说，B+ Tree的高度为3 现在，我们回到最开始的问题：一颗InnoDB的B+树大概存储多少条数据？ 假设索引为主键索引，主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，这样一共14字节。 这样根节点大约可以存放的指针个数约为：$16384/14=1170$。即意味着内部节点一共有大约1170个。 对于每一个内部节点来说(上图第二层)，大约含有$16384/14=1170$个指针，意味着每一个内部节点对应大约1170个叶子节点。则一共有大约$1170*1170$个叶子节点 叶子节点，假设一条数据的大小为$1K$， 则每个叶子节点大约存放$16k / 1k =16$条数据。 至此，我们可以算出颗InnoDB的B+树大约能存数据为： 11170*1170*16=21902400 答案为：两千多万条！ 也就是说，对于一个B+ Tree，从两千万条的数据中查找一条数据，只需要进行3次$I/O$.效率极大的得到了提升！！]]></content>
      <categories>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
        <tag>B+ Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[$PySpark$在遇到$map$类型的列的一些处理]]></title>
    <url>%2F2020%2F07%2F10%2FSpark%2FPySpark%20%E5%9C%A8%E9%81%87%E5%88%B0map%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[$PySpark$ 在遇到$map$类型的列的一些处理 在$spark$中，有时会遇到$column$的类型是$array$和$map$类型的，这时候需要将它们转换为多行数据 $Explode\ array\ and\ map\ columns\ to\ rows$123456789101112131415import pysparkfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()arrayData = [ ('James',['Java','Scala'],&#123;'hair':'black','eye':'brown'&#125;), ('Michael',['Spark','Java',None],&#123;'hair':'brown','eye':None&#125;), ('Robert',['CSharp',''],&#123;'hair':'red','eye':''&#125;), ('Washington',None,None), ('Jefferson',['1','2'],&#123;&#125;) ]df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])df.printSchema()df.show() 1234567891011121314151617root |-- name: string (nullable = true) |-- knownLanguages: array (nullable = true) | |-- element: string (containsNull = true) |-- properties: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true)+----------+--------------+--------------------+| name|knownLanguages| properties|+----------+--------------+--------------------+| James| [Java, Scala]|[eye -&gt; brown, ha...|| Michael|[Spark, Java,]|[eye -&gt;, hair -&gt; ...|| Robert| [CSharp, ]|[eye -&gt; , hair -&gt;...||Washington| null| null|| Jefferson| [1, 2]| []|+----------+--------------+--------------------+ $explode – array\ column\ example$ $PySpark\ function$ explode(e: Column) is used to explode or create array or map columns to rows. When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. When a map is passed, it creates two new columns one for key and one for value and each element in map split into the rows. $spark$提供$explode$函数explode(e: Column)， 当传入的column是array类型时，它会新建一个列，默认列名为col；当传入的column是map类型时，则会新建两个列，一个列为key，另一个为value 1234from pyspark.sql.functions import explodedf3 = df.select(df.name, explode(df.knownLanguages))df3.printSchema()df3.show() $output$123456789101112131415161718root |-- name: string (nullable = true) |-- col: string (nullable = true)+---------+------+| name| col|+---------+------+| James| Java|| James| Scala|| Michael| Spark|| Michael| Java|| Michael| null|| Robert|CSharp|| Robert| ||Jefferson| 1||Jefferson| 2|+---------+------+ 注意： Washington对应的$knownLanguages$字段是null，explode会忽略这种值，可以看到，结果集里并没有Washington的记录，如果需要保留，使用explode_outer函数 $explode – map\ column\ example$1234from pyspark.sql.functions import explodedf3 = df.select(df.name,explode(df.properties))df3.printSchema()df3.show() $output$123456789101112131415root |-- name: string (nullable = true) |-- key: string (nullable = false) |-- value: string (nullable = true)+-------+----+-----+| name| key|value|+-------+----+-----+| James| eye|brown|| James|hair|black||Michael| eye| null||Michael|hair|brown|| Robert| eye| || Robert|hair| red|+-------+----+-----+ $How\ to\ covert\ Map\ into\ multiple\ columns$ 有时候需要把$Map$类型的$colum$n进行以$key$为列名，$value$为列值的处理。如下： 12345from pyspark.sql import functions as Fdf.select(F.col("name"), F.col("properties").getItem("hair").alias("hair_color"), F.col("properties").getItem("eye").alias("eye_color")).show() $output$123456789+----------+----------+---------+| name|hair_color|eye_color|+----------+----------+---------+| James| black| brown|| Michael| brown| null|| Robert| red| ||Washington| null| null|| Jefferson| null| null|+----------+----------+---------+]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的分区]]></title>
    <url>%2F2020%2F06%2F29%2FMySQL%2FMySql%E7%9A%84%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1. 什么是表分区？表分区，是指根据一定规则，将数据库中的一张表分解成多个更小的，容易管理的部分。从逻辑上看，只有一张表，但是底层却是由多个物理分区组成。 2. 表分区与分表的区别分表：指的是通过一定规则，将一张表分解成多张不同的表。比如将用户订单记录根据时间成多个表。分表与分区的区别在于：分区从逻辑上来讲只有一张表，而分表则是将一张表分解成多张表。 3. 表分区有什么好处？ 分区表的数据可以分布在不同的物理设备上，从而高效地利用多个硬件设备。 和单个磁盘或者文件系统相比，可以存储更多数据。 优化查询。在where语句中包含分区条件时，可以只扫描一个或多个分区表来提高查询效率;涉及sum和count语句时，也可以在多个分区上并行处理，最后汇总结果。 分区表更容易维护。例如：想批量删除大量数据可以清除整个分区。 可以使用分区表来避免某些特殊的瓶颈，例如InnoDB的单个索引的互斥访问，ext3问价你系统的inode锁竞争等。 4. MySQL支持的分区类型有哪些？ RANGE分区：按照数据的区间范围分区 LIST分区：按照List中的值分区，与RANGE的区别是，range分区的区间范围值是连续的。 HASH分区 KEY分区A、 HASH分区根据$MOD$（分区键）的值，把数据行存储到表的不同分区中，键值必须为INT类型的值，或者转换为INT类型进行HASH函数运算。12CREATE TABLE `order` (***)PARTITION BY HASH(order_id) PARTITIONS 4; B、RANGE分区根据分区表键值的范围把数据行存储到表的不同分区中，默认情况下使用VALUES LESS THAN属性，也即[0,100)。 适用分区键为日期或者时间类型，数据分布均衡，容易归档。 1234567CREATE TABLE `order` (***)PARTITION BY RANGE(product_id) (PARTITION p0 VALUES LESS THAN (1000),PARTITION p1 VALUES LESS THAN (2000),PARTITION p2 VALUES LESS THAN (3000),PARTITION p3 VALUES LESS THAN MAXVALUE); 注意: 每个分区都是按顺序定义的，从最低到最高。 当插入的记录中对应的分区键的值不在分区定义的范围中的时候，插入语句会失败。 Range分区中，分区键的值如果是NULL，将被作为一个最小值来处理。 C、LIST分区按分区键取值的列表进行分区，每一行数据须找到对的分区列表，否则数据插入失败。12345CREATE TABLE `order` (***)PARTITION BY LIST(partner_id) (PARTITION p0 VALUES IN (1,3,5,7,9),PARTITION p1 VALUES IN (2,4,6,8,10)); 注意： MYSQL的分区字段，必须包含在主键字段内如果有主键，那么分区字段必须是主键(之一);如果没有主键，则可以指定任意字段作为分区字段。]]></content>
      <categories>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务的的四大特性]]></title>
    <url>%2F2020%2F06%2F09%2FMySQL%2F%E4%BA%8B%E5%8A%A1%E7%9A%84ACID%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[事务的定义 数据库事务(事务)是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。 事务的四大特性—ACID 原子性(Atomicity):事务包含的操作全部成功或者全部失败 一致性(Consistency):是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 隔离性(Isolation): 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 持久性(Durability):是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 以A给B转账1000元为例子: 如何同时保证上述交易中，A账户总金额减少1000，B账户-总金额增加1000？ A A账户如果同时在和C账户交易(T2)，如何让这两笔交易互不影响？ I 如果交易完成时数据库突然崩溃，如何保证交易数据成功保存在数据库中？ D 如何在支持大量交易的同时，保证数据的合法性(没有钱凭空产生或消失) ？ C 一致性是基础，也是最终目的，其他三个特性（原子性、隔离性和持久性）都是为了保证一致性的 原子性和一致性事务中的操作必须作为一个整体,全部进行,或者一起回滚.就像原子最为物质的最小单位一样,无法再细分下去.比如:A给B转账,那么A-100和B+100必须全部进行. 但是,保证原子性的情况下是否就能满足一致性?答案是:否 例如，事务1需要将100元转入帐号A：先读取帐号A的值，然后在这个值上加上100。但是，在这两个操作之间，另一个事务2修改了帐号A的值，为它增加了100元。那么最后的结果应该是A增加了200元。但事实上，事务1最终完成后，帐号A只增加了100元，因为事务2的修改结果被事务1覆盖掉了。问题根源在于没有满足隔离性]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用函数构建抽象]]></title>
    <url>%2F2020%2F06%2F02%2FSCIP%2F%E4%BD%BF%E7%94%A8%E5%87%BD%E6%95%B0%E6%9E%84%E5%BB%BA%E6%8A%BD%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[Chapter1: 使用函数构建抽象高阶函数作为一般方法的函数 sum_naturals() 1234567&gt;&gt;&gt; def sum_naturals(n): total, k = 0, 1 while k &lt;= n: total, k = total + k, k + 1 return total&gt;&gt;&gt; sum_naturals(100)5050 sum_cube() 1234567&gt;&gt;&gt; def sum_cubes(n): total, k = 0, 1 while k &lt;= n: total, k = total + pow(k, 3), k + 1 return total&gt;&gt;&gt; sum_cubes(100)25502500 sum_pi() \pi=\frac{8}{1 \cdot 3}+\frac{8}{5 \cdot 7}+\frac{8}{9 \cdot 11}+\dots1234567&gt;&gt;&gt; def pi_sum(n): total, k = 0, 1 while k &lt;= n: total, k = total + 8 / (k * (k + 2)), k + 4 return total&gt;&gt;&gt; pi_sum(100)3.121594652591009 这三个函数在背后都具有相同模式。它们大部分相同，只是名字、用于计算被加项的k的函数，以及提供k的下一个值的函数不同。我们可以通过向相同的模板中填充槽位来生成每个函数： 12345def &lt;name&gt;(n): total, k = 0, 1 while k &lt;= n: total, k = total + &lt;term&gt;(k), &lt;next&gt;(k) return total 对于以上三个函数我们可以抽象成以下函数: 12345&gt;&gt;&gt; def summation(n, term, next): total, k = 0, 1 while k &lt;= n: total, k = total + term(k), next(k) return total 要注意summation接受上界n，以及函数term和next作为参数。我们可以像任何函数那样使用summation，它简洁地表达了求和。 sum_cube() 12345678&gt;&gt;&gt; def cube(k): return pow(k, 3)&gt;&gt;&gt; def successor(k): return k + 1&gt;&gt;&gt; def sum_cubes(n): return summation(n, cube, successor)&gt;&gt;&gt; sum_cubes(3) 36 sum_pi() 1234567891011def pi_term(k): return 8 / (k * (k + 2))def pi_next(k): return k+4def sum_pi(n): return summation(n, pi, next)&gt;&gt;&gt; pi_sum(1e6) 3.1415906535898936 计算黄金分割比(gold ratio) \phi = 1 + \frac{1}{\phi}123456&gt;&gt;&gt; def golden_update(guess): return 1/guess + 1&gt;&gt;&gt; def golden_test(guess): return near(guess, square, successor)&gt;&gt;&gt; iter_improve(golden_update, golden_test)1.6180371352785146 $Lambda$ 表达式 Python 中，我们可以使用 Lambda 表达式凭空创建函数，它会求值为匿名函数。Lambda 表达式是函数体具有单个返回表达式的函数，不允许出现赋值和控制语句。12 lambda x : f(g(x))&quot;A function that takes x and returns f(g(x))&quot; 1234561 def compose1(f, g):2 return lambda x: f(g(x))3 4 f = compose1(lambda x: x * x,5 lambda y: y + 1)6 result = f(12) 抽象和一等函数 作为程序员，我们应该留意识别程序中低级抽象的机会，在它们之上构建，并泛化它们来创建更加强大的抽象。这并不是说，一个人应该总是尽可能以最抽象的方式来编程；专家级程序员知道如何选择合适于他们任务的抽象级别。但是能够基于这些抽象来思考，以便我们在新的上下文中能使用它们十分重要。高阶函数的重要性是，它允许我们更加明显地将这些抽象表达为编程语言中的元素，使它们能够处理其它的计算元素。 通常，编程语言会限制操作计算元素的途径。带有最少限制的元素被称为具有一等地位。一些一等元素的“权利和特权”是： 它们可以绑定到名称。 它们可以作为参数向函数传递。 它们可以作为函数的返回值返回。 它们可以包含在数据结构中。 Python 总是给予函数一等地位，所产生的表现力的收益是巨大的。另一方面，控制结构不能做到：你不能像使用sum那样将if传给一个函数。 补充阅读如何正确理解Python函数是第一类对象]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>SICP</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[repartition和coalesce区别]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Frepartition%E5%92%8Ccoalesce%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[简介$repartition(numPartitions:Int)$ 和 $coalesce(numPartitions:Int，shuffle:Boolean=false)$ 作用：对RDD的分区进行重新划分，repartition内部调用了coalesce，参数shuffle=true分析例：RDD有N个分区，需要重新划分成M个分区12345678N小于M一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。N大于M且和M相差不多假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。N大于M且和M相差悬殊这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们在同一个Stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。 总结：返回一个减少到numPartitions个分区的新RDD，这会导致窄依赖例如：你将1000个分区转换成100个分区，这个过程不会发生shuffle，相反如果10个分区转换成100个分区将会发生shuffle。然而如果你想大幅度合并分区，例如所有partition合并成一个分区，这会导致计算在少数几个集群节点上进行（言外之意：并行度不够）。‘为了避免这种情况，你可以将第二个shuffle参数传递一个true，这样会在重新分区过程中多一步shuffle，这意味着上游的分区可以并行运行。 总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的partition数变多的。 进一步理解123N大于M且和M相差悬殊将shuffle设置为false，父子RDD是窄依赖关系，他们在同一个Stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以将shuffle设置为true 每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！！1234和repartition有所区别的是，coalesce并不是一个shuffle算子。也就说coalesce不会触发shuffle操作，它是包含在当前的stage中的。由于，每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的。就会引起这样一种现象：由于coalesce算子的存在，必然导致运算后的partition数目的减少。也就是说当前的stage的并行的task数目(并行度)会降低。每个task计算的数据(分区)会加大。可以将shuffle设置为true来触发shuffle，从而不会降低当前stage的task数量(并行度)。 由图可见，在coalesce操作之前，每个rdd有四个partition，如果没有soalesce操作,当前stage的并行度为4.但是由于coalesce操作的存在，导致分区数变为2。所以整个stage的并行度为2，在实际运行时，excutor只会为当前的stage启动2个核。也就是说输入的4个partition会被分布到两个task上运行，每个task上分到两个partion。每个task运算的数据量变大，运行速度就会被拖慢。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark运行内存超出]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2F%E5%85%B3%E4%BA%8ESpark%E8%BF%90%E8%A1%8C%E4%B8%AD%E5%86%85%E5%AD%98%E8%B6%85%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Container killed by YARN for exceeding memory limits？运行spark脚本时，经常会碰到Container killed by YARN for exceeding memory limits的错误，导致程序运行失败。 这个的意思是指executor的外堆内存超出了。默认情况下，这个值被设置为executor_memory的10%或者384M，以较大者为准，即max(executor_memory*.1, 384M). 解决办法 提高内存开销 减少执行程序内核的数量 增加分区数量 提高驱动程序和执行程序内存 提高内存开销 即直接指定堆外内存的大小1spark.conf.set(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;4g&quot;) 减少执行程序内核的数量 这可减少执行程序可以执行的最大任务数量，从而减少所需的内存量。 增加分区数量 要增加分区数量，请为原始弹性分布式数据集增加 spark.default.parallelism 的值，或执行.repartition() 操作。增加分区数量可减少每个分区所需的内存量。 提高驱动程序和执行程序内存 即通过增大executor.memory的值来增大堆外内存,但是可以看到，由于乘了10%，所以提升其实很有限。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark并行度理解]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FSpark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%9A%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE%E5%B9%B6%E8%A1%8C%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[Spark性能调优：合理设置并行度1. Spark的并行度指的是什么？并行度其实就是指的是spark作业中,各个stage的同时运行的task的数量,也就代表了spark作业在各个阶段stage的并行度！ 并行度 = executor\_number * executor\_cores理解：sparkApplication的划分： $job —&gt; stage —&gt; task$一般每个task一次处理一个分区。 可以将task理解为比赛中的跑道：每轮比赛中，每个跑道上都会有一位运动员(分区，即处理的数据)，并行度就是跑道的数量，一轮比赛就可以理解为一个stage。 2.如果不调节并行度，导致并行度过低会怎么样？假设现在已经在spark-submit脚本里面，给我们的spark作业分配了足够多的资源，比如有50个 executor，每个executor 有10G内存，每个 executor 有3个cpu core，基本已经达到了集群或者yarn队列的资源上限。 如果 task 没有设置，或者设置的很少，比如就设置了100个 task。现在50个 executor，每个executor 有3个cpu core，也就是说，你的Application任何一个 stage 运行的时候都有总数在150个 cpu core，可以并行运行。但是你现在只有100个task，平均分配一下，每个executor 分配到2个task，那么同时在运行的task只有100个，每个executor只会并行运行2个task，每个executor剩下的一个 cpu core 就浪费掉了。 你的资源虽然分配足够了，但是问题是，并行度没有与资源相匹配，导致你分配下去的资源都浪费掉了。 合理的并行度的设置，应该是要设置的足够大，大到可以完全合理的利用你的集群资源。比如上面的例子，总共集群有150个cpu core，可以并行运行150个task。那么就应该将你的Application 的并行度至少设置成150才能完全有效的利用你的集群资源，让150个task并行执行，而且task增加到150个以后，既可以同时并行运行，还可以让每个task要处理的数据量变少。比如总共150G的数据要处理，如果是100个task，每个task计算1.5G的数据，现在增加到150个task可以并行运行，而且每个task主要处理1G的数据就可以。 很简单的道理，只要合理设置并行度，就可以完全充分利用你的集群计算资源，并且减少每个task要处理的数据量，最终，就是提升你的整个Spark作业的性能和运行速度。 3. 如何去提高并行度？(1) task数量，至少设置成与spark Application 的总cpu core 数量相同（最理性情况，150个core，分配150task，一起运行，差不多同一时间运行完毕） 官方推荐，task数量，设置成spark Application 总cpu core数量的2~3倍 ，比如150个cpu core ，基本设置 task数量为 300~ 500， 与理性情况不同的，有些task 会运行快一点，比如50s 就完了，有些task 可能会慢一点，要一分半才运行完，所以如果你的task数量，刚好设置的跟cpu core 数量相同，可能会导致资源的浪费，因为 比如150task ，10个先运行完了，剩余140个还在运行，但是这个时候，就有10个cpu core空闲出来了，导致浪费。如果设置2~3倍，那么一个task运行完以后，另外一个task马上补上来，尽量让cpu core不要空闲。同时尽量提升spark运行效率和速度。提升性能。 (2) 如何设置一个Spark Application的并行度？123456对于RDD来说：可以通过设置spark.default.parallelism 参数来决定shuffle操作之后的partition数目，默认是没有值的，如果设置了值,比如说100，是在shuffle的过程才会起作用new SparkConf().set(“spark.default.parallelism”, “500”)对于sparksql来说：可以通过设置spark.sql.shuffle.partitions参数，默认值为200; (3) repartition算子repartiton算子通过传入想要的分区数目，来改变分区数。注意：repartion是一个shuffle算子 (4) 在算子中加入指定的参数，来指定分区数目]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark中的各种概念]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Fspark%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spark中的各种概念的理解Application：通俗讲，用户每次提交的所有的代码为一个application。 Job：一个application可以分为多个job。如何划分job？通俗讲，触发一个final RDD的实际计算（action）为一个job Stage：一个job可以分为多个stage。根据一个job中的RDD的宽依赖和窄依赖关系进行划分 Task：task是最小的基本的计算单位。一般是一个RDD的一个分区（partition）为一个task，大约是128M 并行度：是指指令并行执行的最大条数。在指令流水中，同时执行多条指令称为指令并行。 理论上：每一个stage下有多少的分区，就有多少的task，task的数量就是我们任务的最大的并行度。 （一般情况下，我们一个task运行的时候，使用一个cores） 实际上：最大的并行度，取决于我们的application任务运行时使用的executor拥有的cores的数量。 spark 基本概念解析 Spark运行流程 Application 首先被 Driver 构建 DAG 图并分解成 Stage。 然后 Driver 向 Cluster Manager 申请资源。 Cluster Manager 向某些 Work Node 发送征召信号。 被征召的 Work Node 启动 Executor 进程响应征召，并向 Driver 申请任务。 Driver 分配 Task 给 Work Node。 Executor 以 Stage 为单位执行 Task，期间 Driver 进行监控。 Driver 收到 Executor 任务完成的信号后向 Cluster Manager 发送注销信号。 Cluster Manager 向 Work Node 发送释放资源信号。 Work Node 对应 Executor 停止运行。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cache和persist比较]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Fcache%E7%AE%97%E5%AD%90%E5%92%8Cpersist%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[Spark中cache和persist的作用Spark开发高性能的大数据计算作业并不是那么简单。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 有一些代码开发基本的原则，避免创建重复的RDD，尽可能复用同一个RDD，如下，我们可以直接用一个RDD进行多种操作：123val rdd1 = sc.textFile("xxx")rdd1.collectrdd1.show 但是Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。对于上面的代码，sc.textFile("xxx")会执行两次，这种方式的性能是很差的。 因此对于这种情况，我的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 持久化如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。123val rdd1 = sc.textFile("xxx").cacherdd1.collectrdd1.show cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。 此时再对rdd1执行两次算子操作时，只有在第一次算子时，才会将这个rdd1从源头处计算一次。第二次执行算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。1234/** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */def cache(): this.type = persist() 通过源码可以看出cache()是persist()的简化方式，调用persist的无参版本，也就是调用persist(StorageLevel.MEMORY_ONLY)，cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中 persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。 1234/** * Persist this RDD with the default storage level (`MEMORY_ONLY`).*/def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) 默认缓存级别是StorageLevel.MEMORY\_ONLY.也就是说cache的默认级别就是MEMORY\_ONLY DataFrame的cache和persist的区别官网和上的教程说的都是RDD,但是没有讲df的缓存，通过源码发现df和rdd还是不太一样的:1234567891011121314151617181920212223242526272829/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */def cache(): this.type = persist()/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */def persist(): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this) this&#125;def persist(newLevel: StorageLevel): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this&#125;def cacheQuery( query: Dataset[_], tableName: Option[String] = None, storageLevel: StorageLevel = MEMORY_AND_DISK): Unit = writeLock 可以到cache()依然调用的persist()，但是persist调用cacheQuery，而cacheQuery的默认存储级别为MEMORY_AND_DISK，这点和rdd是不一样的。 RDD的缓存级别每个持久化的 RDD 可以使用不同的存储级别进行缓存，例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。这些存储级别通过传递一个 StorageLevel 对象给 persist() 方法进行设置。详细的存储级别介绍如下： MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。 MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。 MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。 MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。 DISK_ONLY : 只在磁盘上缓存 RDD。 MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。 OFF_HEAP（实验中）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。 注意，在 Python 中，缓存的对象总是使用 Pickle 进行序列化，所以在 Python 中不关心你选择的是哪一种序列化级别。python 中的存储级别包括 MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY 和 DISK_ONLY_2 。 持久化策略的选择 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 使用Cache注意下面三点（1）cache之后一定不能立即有其它算子，不能直接去接算子。因为在实际工作的时候，cache后有算子的话，它每次都会重新触发这个计算过程。 （2）cache不是一个action，运行它的时候没有执行一个作业。 （3）cache缓存如何让它释放缓存：unpersist，它是立即执行的。persist是lazy级别的（没有计算）unpersist时eager级别的。意味着unpersist如果定义在action算子之前，则cache失效]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD算子总结]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FRDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[RDD算子总结从功能上分：转换算子(transformer)： lazy执行，生成新的rdd，只有在调用action算子时，才会真正的执行。如：map 、flatmap、filter、 union、 join、 ruduceByKey、 cache 行动算子(action)： 触发任务执行，产生job，返回值不再是rdd。如：count 、collect、top、 take、 reduce 从作用上分：通用的： map、 flatMap、 distinct、 union 作用于RDD[K,V]： mapValues、 reduceByKey、 groupByKey、 sortByKey、 转换算子是否有shuffleshuffle类: reduceByKey、 groupByKey、 groupBy、 join、 distinct、 repartition 非shuffle类: map、 filter、 union、flatMap、 coalesce Spark算子使用案例总结]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD基础入门]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FRDD%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[RDD简介 RDD—弹性分布式数据集（Resilient Distributed Dataset）是spark的核心概念。RDD其实就是分布式的元素集合。在Spark中，对数据的所有操作不外乎创建RDD，转化已有的RDD以及调用RDD操作进行求值。而在这一切的背后，spark会自动讲RDD中的数据分发到集群上，并将操作并行化执行。 RDD基础 RDD是一个不可变的分布式对象集合.每个RDD被分为多个分区，这些分区运行在集群中的不同节点上。RDD可以包含Python、Java、Scala中任意类型的对象。 每个spark程序无外乎都是下面的流程: 1.从外部数据创建输入RDD 2.使用诸如filter()这样的操作对RDD进行转化，定义新的RDD 3.告诉spark对需要被重用的中间RDD执行persisit()操作 4.使用行动操作(count(),first())触发一次并行计算，spark并不会立马执行，而是优化后再执行 1、创建RDD Spark提供了两种方法创建RDD的方法： 读取外部数据集 在驱动器程序中对一个集合进行并行化 12345#parallelize方法将集合转化为rddlines = sc.parallelize([&quot;pandas&quot;, &quot;i like pandas&quot;])#textFile方法lines=sc.textFile(&quot;README.md&quot;) 2、RDD操作spark支持两种操作：转化操作，行动操作 转化操作 转化操作执行时返回新的RDD的操作，转化出来的RDD是惰性求值的，只有在行动中用到这些RDD时才会被计算，许多转化操作只会操作RDD中的一个元素，并不是所有的转化操作都是这样 比如提取日志文件的错误信息 12345inputRDD=sc.testFile(&quot;log.txt&quot;)errorsRDD=inputRDD.filter(lambda x:&quot;error&quot; in x)warningsRDD = inputRDD.filter(lambda x: &quot;warning&quot; in x)# 将errorsRDD和warningsRDD合并成一个RDDbadLinesRDD = errorsRDD.union(warningsRDD) 通过转化操作，从已有的RDD中派生新的RDD，spark会使用谱系图记录这些RDD的依赖关系，spark在需要用这些信息的时候按需计算每个RDD，也可以依靠谱系图在丢失数据的情况下恢复丢失的数据 行动操作 行动操作需要实际的输出，它会强制执行哪些求值必须用到的RDD转化操作 示例：对badLinesRDD进行计数操作，并且打印前十条记录 1234print &quot;Input had &quot; + badLinesRDD.count() + &quot; concerning lines&quot;# take(num) 从RDD中取出num个元素for line in badLinesRDD.take(10): print line 这里使用了take()取出少量的数据集，也可以使用collect()函数获取整个RDD中的数据，但是使用collect需要注意内存是否够用。如果数据集特别大的时候，我们需要把数据写到诸如HDFS之类的分布式存储系统，当调用一个新的行动操作的时候整个RDD会从头计算，我们要将中间结果持久化 惰性求值 RDD的转化操作都是多心求值的，这意味着在被调用行动操作之前Spark不会开始计算。 惰性求值意味着我们对RDD调用转化操作（例如map()）时，操作不会立即执行，相反，Spark会在内部记录下所要求执行的操作的相关信息。我们不应该把RDD看作放着特定数据的数据集，而最好把每个RDD看作我们通过转化操作构建出来的、记录如何计算数据的指令列表。把数据读到RDD的操作也是惰性的，因此，当我们调用sc.textFile()时，数据并没有读取进来，而是在必要时才会读取。 3、函数传递1234567#使用lambda方法传递word = rdd.filter(lambda s: &quot;error&quot; in s)#定义一个函数然后传递def containsError(s): return &quot;error&quot; in sword = rdd.filter(containsError) 传递函数时要小心，python会在不经意间把函数所在的对象也序列化传出，有时如果传递的类里包含了python不知道如何序列化输出的对象，也可能导致程序失败。 如下是一个错误的函数传递示例； 1234567891011class SearchFunctions(object): def __init__(self, query): self.query = query def isMatch(self, s): return self.query in s def getMatchesFunctionReference(self, rdd): # 问题：在&quot;self.isMatch&quot;中引用了整个self return rdd.filter(self.isMatch) def getMatchesMemberReference(self, rdd): # 问题：在&quot;self.query&quot;中引用了整个self return rdd.filter(lambda x: self.query in x) 正确做法: 12345class WordFunctions(object): def getMatchesNoReference(self, rdd): # 安全：只把需要的字段提取到局部变量中 query = self.query return rdd.filter(lambda x: query in x) 4、RDD操作 常见的转化操作 常见的行动操作 5、持久化(缓存) 如前所述， SparkRDD是惰性求值的，而有时我们希望能多次使用同一个 RDD。如果简单地对 RDD 调用行动操作， Spark 每次都会重算 RDD 以及它的所有依赖。这在迭代算法中消耗格外大，因为迭代算法常常会多次使用同一组数据。 如下就是先对 RDD 作一次计数、再把该 RDD 输出的一个小例子。 123val result = input.map(x =&gt; x*x)println(result.count())println(result.collect().mkString(",")) 为了避免多次计算同一个 RDD，可以让 Spark 对数据进行持久化。当我们让 Spark 持久化存储一个 RDD 时，计算出 RDD 的节点会分别保存它们所求出的分区数据。如果一个有持久化数据的节点发生故障， Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。 出于不同的目的，我们可以为 RDD 选择不同的持久化级别（如表 3-6 所示）。在 Scala和 Java 中，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。在 Python 中，我们会始终序列化要持久化存储的数据，所以持久化级别默认值就是以序列化后的对象存储在 JVM 堆空间中。当我们把数据写到磁盘或者堆外存储上时，也总是使用序列化后的数据。 1234val result = input.map(x =&gt; x * x)result.persist(StorageLevel.DISK_ONLY)println(result.count())println(result.collect().mkString(",")) 如果要缓存的数据太多， 内存中放不下， Spark 会自动利用最近最少使用（ LRU）的缓存策略把最老的分区从内存中移除。 对于仅把数据存放在内存中的缓存级别，下一次要用到已经被移除的分区时， 这些分区就需要重新计算。但是对于使用内存与磁盘的缓存级别的分区来说，被移除的分区都会写入磁盘。不论哪一种情况，都不必担心你的作业因为缓存了太多数据而被打断。 不过，缓存不必要的数据会导致有用的数据被移出内存，带来更多重算的时间开销]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce介绍]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FMapReduce%2F</url>
    <content type="text"><![CDATA[Hadoop简介 Hadoop就是一个实现了Google云计算系统的开源系统，包括并行计算模型Map/Reduce，分布式文件系统HDFS，以及分布式数据库Hbase，同时Hadoop的相关项目也很丰富，包括ZooKeeper，Pig，Chukwa，Hive，Hbase，Mahout，flume等. 这里详细分解这里面的概念让大家通过这篇文章了解到底是什么hadoop： Map/Reduce： MapReduce是hadoop的核心组件之一，hadoop要分布式包括两部分，一是分布式文件系统hdfs,一部是分布式计算框架，就是mapreduce,缺一不可，也就是说，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程。 Mapreduce是一种编程模型，是一种编程方法，抽象理论。 下面是一个关于一个程序员是如何个妻子讲解什么是MapReduce？文章很长请耐心的看。 123456789101112131415161718192021222324252627282930313233343536373839404142434445我问妻子：“你真的想要弄懂什么是MapReduce？” 她很坚定的回答说“是的”。 因此我问道：我： 你是如何准备洋葱辣椒酱的？（以下并非准确食谱，请勿在家尝试）妻子： 我会取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机里研磨。这样就能得到洋葱辣椒酱了。妻子： 但这和MapReduce有什么关系？我： 你等一下。让我来编一个完整的情节，这样你肯定可以在15分钟内弄懂MapReduce.妻子： 好吧。我：现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？妻子： 我会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。我： 没错，让我们把MapReduce的概念应用到食谱上。Map和Reduce其实是两种操作，我来给你详细讲解下。Map（映射）: 把洋葱、番茄、辣椒和大蒜切碎，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。 同样的，你把辣椒，大蒜和番茄一一地拿给Map，你也会得到各种碎块。 所以，当你在切像洋葱这样的蔬菜时，你执行就是一个Map操作。 Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把坏洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉坏洋葱而不会生产出任何的坏洋葱块。Reduce（化简）:在这一阶段，你将各种蔬菜碎都放入研磨机里进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将map操作的蔬菜碎聚集在了一起。妻子： 所以，这就是MapReduce?我： 你可以说是，也可以说不是。 其实这只是MapReduce的一部分，MapReduce的强大在于分布式计算。妻子： 分布式计算？ 那是什么？请给我解释下吧。我： 没问题。我： 假设你参加了一个辣椒酱比赛并且你的食谱赢得了最佳辣椒酱奖。得奖之后，辣椒酱食谱大受欢迎，于是你想要开始出售自制品牌的辣椒酱。假设你每天需要生产10000瓶辣椒酱，你会怎么办呢？妻子： 我会找一个能为我大量提供原料的供应商。我：是的..就是那样的。那你能否独自完成制作呢？也就是说，独自将原料都切碎？ 仅仅一部研磨机又是否能满足需要？而且现在，我们还需要供应不同种类的辣椒酱，像洋葱辣椒酱、青椒辣椒酱、番茄辣椒酱等等。妻子： 当然不能了，我会雇佣更多的工人来切蔬菜。我还需要更多的研磨机，这样我就可以更快地生产辣椒酱了。我：没错，所以现在你就不得不分配工作了，你将需要几个人一起切蔬菜。每个人都要处理满满一袋的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断的从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块、和蒜蓉等等。妻子：但是我怎么会制造出不同种类的番茄酱呢？我：现在你会看到MapReduce遗漏的阶段—搅拌阶段。MapReduce将所有输出的蔬菜碎都搅拌在了一起，这些蔬菜碎都是在以key为基础的 map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，就像洋葱一样。 所以全部的洋葱keys都会搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移到标记着番茄的研磨器里，并制造出番茄辣椒酱。 上面都是从理论上来说明什么是MapReduce，那么咱们在MapReduce产生的过程和代码的角度来理解这个问题。 如果想统计下过去10年计算机论文出现最多的几个单词，看看大家都在研究些什么，那收集好论文后，该怎么办呢？ 方法一： 我可以写一个小程序，把所有论文按顺序遍历一遍，统计每一个遇到的单词的出现次数，最后就可以知道哪几个单词最热门了。 这种方法在数据集比较小时，是非常有效的，而且实现最简单，用来解决这个问题很合适。 方法二：写一个多线程程序，并发遍历论文. 这个问题理论上是可以高度并发的，因为统计一个文件时不会影响统计另一个文件。当我们的机器是多核或者多处理器，方法二肯定比方法一高效。但是写一个多线程程序要比方法一困难多了，我们必须自己同步共享数据，比如要防止两个线程重复统计文件。 方法三： 把作业交给多个计算机去完成。我们可以使用方法一的程序，部署到N台机器上去，然后把论文集分成N份，一台机器跑一个作业。这个方法跑得足够快，但是部署起来很麻烦，我们要人工把程序copy到别的机器，要人工把论文集分开，最痛苦的是还要把N个运行结果进行整合（当然我们也可以再写一个程序）。 方法四： 让MapReduce来帮帮我们吧！ MapReduce本质上就是方法三，但是如何拆分文件集，如何copy程序，如何整合结果这些都是框架定义好的。我们只要定义好这个任务（用户程序），其它都交给MapReduce。 map函数和reduce函数 123map函数： 接受一个键值对（key-value pair），产生一组中间键值对。MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。 reduce函数： 接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。 在统计词频的例子里，map函数接受的键是文件名，值是文件的内容，map逐个遍历单词，每遇到一个单词w ，就产生一个中间键值对，这表示单词w咱又找到了一个；MapReduce将键相同（都是单词w）的键值对传给reduce函数，这样reduce函数接受的键就是单词w，值是一串”1”（最基本的实现是这样，但可以优化），个数等于键为w的键值对的个数，然后将这些“1”累加就得到单词w的出现次数。最后这些单词的出现次数会被写到用户定义的位置，存储在底层的分布式存储系统（GFS或HDFS）。 执行过程 图1展示了我们的实现中MapReduce操作的整体流程。当用户程序调用MapReduce函数时，会发生下面一系列动作（图1中的标号与下面列表顺序相同）： 12345678910111213141516171819201. 用户程序中的MapReduce库首先将输入文件切分为M块，每块的大小从16MB到64MB（用户可通过一个可选参数控制此大小）。然后MapReduce库会在一个集群的若干台机器上启动程序的多个副本。2. 程序的各个副本中有一个是特殊的——主节点，其它的则是工作节点。主节点将M个map任务和R个reduce任务分配给空闲的工作节点，每个节点一项任务。3. 被分配map任务的工作节点读取对应的输入区块内容。它从输入数据中解析出key/value对，然后将每个对传递给用户定义的map函数。由map函数产生的中间key/value对都缓存在内存中。4. 缓存的数据对会被周期性的由划分函数分成R块，并写入本地磁盘中。这些缓存对在本地磁盘中的位置会被传回给主节点，主节点负责将这些位置再传给reduce工作节点。5. 当一个reduce工作节点得到了主节点的这些位置通知后，它使用RPC调用去读map工作节点的本地磁盘中的缓存数据。当reduce工作节点读取完了所有的中间数据，它会将这些数据按中间key排序，这样相同key的数据就被排列在一起了。同一个reduce任务经常会分到有着不同key的数据，因此这个排序很有必要。如果中间数据数量过多，不能全部载入内存，则会使用外部排序。6. reduce工作节点遍历排序好的中间数据，并将遇到的每个中间key和与它关联的一组中间value传递给用户的reduce函数。reduce函数的输出会写到由reduce划分过程划分出来的最终输出文件的末尾。7. 当所有的map和reduce任务都完成后，主节点唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码中。 成功完成后，MapReduce执行的输出都在R个输出文件中（每个reduce任务产生一个，文件名由用户指定）。通常用户不需要合并这R个输出文件——他们经常会把这些文件当作另一个MapReduce调用的输入，或是用于另一个可以处理分成多个文件输入的分布式应用。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.0的新特性]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Fspark2.0%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Spark2.0Spark直接从1.6跨入2.0版本，带来一些新的特性。最大的变化便是SparkSession整合了各种环境。 Spark2.0中引入了SparkSession的概念，它为用户提供了一个统一的切入点来使用Spark的各项功能，用户不但可以使用DataFrame和Dataset的各种API，学习Spark的难度也会大大降低。 SparkSession在Spark的早期版本，SparkContext是进入Spark的切入点。我们都知道RDD是Spark中重要的API，然而它的创建和操作得使用sparkContext提供的API；对于RDD之外的其他东西，我们需要使用其他的Context。比如对于流处理来说，我们得使用StreamingContext；对于SQL得使用sqlContext；而对于hive得使用HiveContext。然而DataSet和Dataframe提供的API逐渐称为新的标准API，我们需要一个切入点来构建它们，所以在 Spark 2.0中我们引入了一个新的切入点(entry point)：SparkSession SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 之前的写法：123456789from pyspark import SparkContext, SparkConffrom pyspark.sql import SQLContextconf = SparkConf().setMaster("local[*]").setAppName("PySparkShell")sc = SparkContext(conf=conf)sqlContest = SQLContext(sc)spark = SQLContext(sc)spark.sql(select **)··· 现在的写法123456789101112131415from pyspark.sql import SparkSessionspark = SparkSession .builder .appName("Python Spark SQL basic example") .config("spark.some.config.option","some-value") .enableHiveSupport() .getOrCreate()df1 = spark.sql(select **) df2 = spark.read.csv('./python/test_support/sql/ages.csv') # 通过spark创建scsc = spark.sparkContextrdd1 = sc.parallelize([1,2,3,4,5]) 其中： 在pyspark sql中换行要 \ .getOrCreate() 指的是如果当前存在一个SparkSession就直接获取，否则新建。 .enableHiveSupport() 使我们可以从读取或写入数据到hive。.enableHiveSupport 函数的调用使得SparkSession支持hive，类似于HiveContext]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark生态介绍]]></title>
    <url>%2F2019%2F08%2F29%2FSpark%2Fspark%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[生态： Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的 Spark SQL：提供通过Apache Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。 Spark Streaming：对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据 MLlib：一个常用机器学习算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。 GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD API，包含控制图、创建子图、访问路径上所有顶点的操作 架构： Cluster Manager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器 Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。 Driver： 运行Application 的main()函数 Executor：执行器，是为某个Application运行在worker node上的一个进程 术语： Application: Appliction都是指用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码 Driver: Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用SparkContext代表Driver Executor: 某个Application运行在worker节点上的一个进程， 该进程负责运行某些Task， 并且负责将数据存到内存或磁盘上，每个Application都有各自独立的一批Executor， 在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象， 负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task， 这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数 Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型 Standalon : spark原生的资源管理，由Master负责资源的分配 Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架 Hadoop Yarn: 主要是指Yarn中的ResourceManager Worker: 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NoteManager节点 Task: 被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责 Job: 包含多个Task组成的并行计算，往往由Spark Action触发生成， 一个Application中往往会产生多个Job Stage: 每个Job会被拆分成多组Task， 作为一个TaskSet， 其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐算法]]></title>
    <url>%2F2018%2F12%2F05%2FMachineLearning%2F%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[推荐算法 基于内容的推荐算法 根据物品或内容的元数据，发现商品或内容的相关性，然后根据用户之前的喜好记录，推荐相似的物品 如：用户X购买过商品A，而A和B是相似的（iphone和小米手机），就可以把B推荐给X 协同过滤基于物品的协同过滤(Item-based Collaborative Filtering)首先从数据库里获取所有用户对物品的评价，根据评价，计算物品的相似度。然后从剩下的物品中找到和他历史兴趣近似的物品推荐给他。核心是要计算两个物品的相似度。基于用户的协同过滤(Item-based Collaborative Filtering)，基于用户的协同过滤推荐算法先使用统计技术寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。基本原理就是利用用户访问行为的相似性来互相推荐用户可能感兴趣的资源，区别： 可以注意到,基于物品的协同过滤和基于内容的推荐，两者的相同点都是要计算两个物品的相似度，但不同点是前者是根据两个物品被越多的人同时喜欢，这两个物品就越相似，而后者要根据物品的内容相似度来做推荐，给物品内容建模的方法很多，最著名的是向量空间模型，要计算两个向量的相似度。由此可以看到两种方法的不同点在于计算两个物品的相似度方法不同，一个根据外界环境计算，一个根据内容计算。 综上： 基于内容的推荐，只考虑了对象的本身性质，将对象按标签形成集合，如果你消费集合中的一个则向你推荐集合中的其他对象； 基于协调过滤，充分利用集体智慧，即在大量的人群的行为和数据中收集答案，以帮助我们对整个人群得到统计意义上的结论，推荐的个性化程度高，基于以下两个出发点： (1)兴趣相近的用户可能会对同样的东西感兴趣； (2)用户可能较偏爱与其已购买的东西相类似的商品。 也就是说考虑进了用户的历史习惯，对象客观上不一定相似，但由于人的行为可以认为其主观上是相似的，就可以产生推荐了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐算法</tag>
      </tags>
  </entry>
</search>
