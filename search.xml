<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java基础3]]></title>
    <url>%2F2021%2F06%2F01%2FJava%2FJava%E5%9F%BA%E7%A1%803%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql的join实现]]></title>
    <url>%2F2020%2F08%2F14%2FMySQL%2FMySql%E7%9A%84join%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[$Nested-Loop\ Join$ $MySql$只支持一种join算法：$Nested-Loop Join$（嵌套循环连接），但$Nested-Loop\ Join$有三种变种：$Simple\ Nested-Loop\ Join，Index\ Nested-Loop\ Join，Block\ Nested-Loop Join$(简单-索引-缓冲区) $Simple\ Nested-Loop\ Join：$如图，$R$为驱动表，$S$为匹配表，可以看到从r中分别取出$r_1、r_2、……、r_n$去匹配$S$表的左右列，然后再合并数据，对$S$表进行了$r*n$次访问，对数据库开销大: $Index\ Nested-Loop\ Join$（索引嵌套）：在查询时，如果被驱动表$S$的关联字段是有索引的，那么查询就要快很多 $Block\ Nested-Loop\ Join:$ 如果有索引，会选取第二种方式进行join，但如果join列没有索引，就会采用$Block\ Nested-Loop\ Join$。可以看到中间有个$join\ buffer$缓冲区，是将驱动表的所有join相关的列(这里不仅仅指用来join的列，还包含select的列)都先缓存到join buffer中，然后批量与匹配表进行匹配，将第一种多次比较合并为一次，降低了非驱动表$S$的访问频率。默认情况下$join_buffer_size=256K$，在查找的时候$MySQL$会将所有的需要的列缓存到$join\ buffer$当中。在一个有N个JOIN关联的SQL当中会在执行时候分配N-1个$join\ buffer$。 具体做法如下： 先从表$R$中取出满足条件的数据，放入$join\ buffer$中 遍历表$S$,每次取出一条数据与$join\ buffer$中的数据进行关联字段的比较。满足条件的进入结果集 如果$join\ buffer$中一次性放不下所有满足条件的数据，那么以上两步需要重复进行]]></content>
      <categories>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[$Spark$解析$DataFrame$中的$json\ string$字段]]></title>
    <url>%2F2020%2F08%2F03%2FSpark%2FHow%20to%20parse%20a%20column%20of%20json%20string%20in%20Pyspark%2F</url>
    <content type="text"><![CDATA[How to parse a column of json string in Pyspark 在用$spark.sql(\ )$从Table读入数据时，DataFrame的列有时是这样一种类型：json形式的string。此时，我们通常需要去解析这个json string，从而提取我们想要的数据。 数据准备12345# Sample Data Framejstr1 = u'&#123;"header":&#123;"id":12345,"foo":"bar"&#125;,"body":&#123;"id":111000,"name":"foobar","sub_json":&#123;"id":54321,"sub_sub_json":&#123;"col1":20,"col2":"somethong"&#125;&#125;&#125;&#125;'jstr2 = u'&#123;"header":&#123;"id":12346,"foo":"baz"&#125;,"body":&#123;"id":111002,"name":"barfoo","sub_json":&#123;"id":23456,"sub_sub_json":&#123;"col1":30,"col2":"something else"&#125;&#125;&#125;&#125;'jstr3 = u'&#123;"header":&#123;"id":43256,"foo":"foobaz"&#125;,"body":&#123;"id":20192,"name":"bazbar","sub_json":&#123;"id":39283,"sub_sub_json":&#123;"col1":50,"col2":"another thing"&#125;&#125;&#125;&#125;'df = spark.createDataFrame([Row(json=jstr1),Row(json=jstr2),Row(json=jstr3)]) 如上所示，我们模拟一个DataFrame，其中只有一列，列名为json，类型为string。可以看到，json中的值为json格式。我们如何从中取出我们关心的值，形成一个单独的列呢？例如：$df[‘header’][‘id’]$. from_json函数123456from pyspark import Rowfrom pyspark.sql.functions import from_json, coljson_schema = spark.read.json(df.select('json').rdd.map(lambda row: row.json)).schemadf_json = df.withColumn('json', from_json(col('json'), json_schema))print(json_schema) $Result:$ 123456789101112root |-- body: struct (nullable = true) | |-- id: long (nullable = true) | |-- name: string (nullable = true) | |-- sub_json: struct (nullable = true) | | |-- id: long (nullable = true) | | |-- sub_sub_json: struct (nullable = true) | | | |-- col1: long (nullable = true) | | | |-- col2: string (nullable = true) |-- header: struct (nullable = true) | |-- foo: string (nullable = true) | |-- id: long (nullable = true) 1df_json.select(col('json.header.id').alias('id')).show() $Result:$ 1234567+-----+| id|+-----+|12345||12346||43256|+-----+ 1df_json.select(col('json.header.id').alias('id'), col('json.body.name').alias('name')).show() $Result:$ 1234567+-----+-------+| id| name|+-----+-------+|12345| foobar||12346| barfoo||43256| bazbar|+-----+-------+ 参考链接]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础2]]></title>
    <url>%2F2020%2F07%2F25%2FJava%2FJava%E5%9F%BA%E7%A1%802%2F</url>
    <content type="text"><![CDATA[Java程序的结构首先，我们来看一段完整的Java程序： 123456789101112/** * 可以用来自动创建文档的注释 */public class Hello &#123; public static void main(String[] args) &#123; // 向屏幕输出文本: System.out.println("Hello, world!"); /* 多行注释开始 注释内容 注释结束 */ &#125;&#125; // class定义结束 $class$关键字因为Java是面向对象的语言，一个程序的基本单位就是class，class是关键字，这里定义的class名字就是Hello： 类名要求： 类名必须以英文字母开头，后接字母，数字和下划线的组合 习惯以大写字母开头 注意到public是访问修饰符，表示该class是公开的。 不写public，也能正确编译，但是这个类将无法从命令行执行。 方法在class内部，可以定义若干方法（method）： 12345public class Hello &#123; public static void main(String[] args) &#123; // 方法名是main // 方法代码... &#125; // 方法定义结束&#125; 在方法内部，语句才是真正的执行代码。Java的每一行语句必须以分号结。这里的方法名是main，返回值是void，表示没有任何返回值。 我们注意到public除了可以修饰class外，也可以修饰方法。而关键字static是另一个修饰符，它表示静态方法，后面我们会讲解方法的类型，目前，我们只需要知道，Java入口程序规定的方法必须是静态方法，方法名必须为main，括号内的参数必须是String数组。 注释Java有3种注释，第一种是单行注释，以双斜线开头，直到这一行的结尾结束： 1// 这是注释... 而多行注释以/*星号开头，以*/结束，可以有多行： 12345/*这是注释blablabla...这也是注释*/ 最后一种是文档注释: 12345678910/** * 可以用来自动创建文档的注释 * * @auther liaoxuefeng */public class Hello &#123; public static void main(String[] args) &#123; System.out.println("Hello, world!"); &#125;&#125; 这种特殊的多行注释需要写在类和方法的定义处，可以用于自动创建文档。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础1]]></title>
    <url>%2F2020%2F07%2F17%2FJava%2FJava%E5%9F%BA%E7%A1%801%2F</url>
    <content type="text"><![CDATA[Java简介 Java最早是由SUN公司（已被Oracle收购）的詹姆斯·高斯林（高司令，人称Java之父）在上个世纪90年代初开发的一种编程语言，最初被命名为Oak，目标是针对小型家电设备的嵌入式应用，结果市场没啥反响。谁料到互联网的崛起，让Oak重新焕发了生机，于是SUN公司改造了Oak，在1995年以Java的名称正式发布，原因是Oak已经被人注册了，因此SUN注册了Java这个商标。随着互联网的高速发展，Java逐渐成为最重要的网络编程语言。 Java介于编译型语言和解释型语言之间。编译型语言如C、C++，代码是直接编译成机器码执行，但是不同的平台（x86、ARM等）CPU的指令集不同，因此，需要编译出每一种平台的对应机器码。解释型语言如Python、Ruby没有这个问题，可以由解释器直接加载源码然后运行，代价是运行效率太低。而Java是将代码编译成一种“字节码”，它类似于抽象的CPU指令，然后，针对不同平台编写虚拟机，不同平台的虚拟机负责加载字节码并执行，这样就实现了“一次编写，到处运行”的效果。当然，这是针对Java开发者而言。对于虚拟机，需要为每个平台分别开发。为了保证不同平台、不同公司开发的虚拟机都能正确执行Java字节码，SUN公司制定了一系列的Java虚拟机规范。从实践的角度看，JVM的兼容性做得非常好，低版本的Java字节码完全可以正常运行在高版本的JVM上。 Java体系Java一共分为三个体系，分别应用于不同的场景; $JavaSE(J2SE)$ — （Java2 Platform Standard Edition，java平台标准版） $JavaEE(J2EE)$ — (Java 2 Platform Enterprise Edition，java平台企业版)​ $JavaME(J2ME)$ — (Java 2 Platform Micro Edition，java平台微型版) 补充：2005年6月，JavaOne大会召开，SUN公司公开Java SE 6。此时，Java的各种版本已经更名以取消其中的数字”2”：J2EE更名为Java EE, J2SE更名为Java SE，J2ME更名为Java ME。 三者之间的关系： 简单来说，Java SE就是标准版，包含标准的JVM和标准库，而Java EE是企业版，它只是在Java SE的基础上加上了大量的API和库，以便方便开发Web应用、数据库、消息服务等，Java EE的应用使用的虚拟机和Java SE完全相同。 Java ME就和Java SE不同，它是一个针对嵌入式设备的瘦身版，Java SE的标准库无法在Java ME上使用，Java ME的虚拟机也是“瘦身版”。 毫无疑问，Java SE是整个Java平台的核心，而Java EE是进一步学习Web应用所必须的。我们熟悉的Spring等框架都是Java EE开源生态系统的一部分。不幸的是，Java ME从来没有真正流行起来，反而是Android开发成为了移动平台的标准之一，因此，没有特殊需求，不建议学习Java ME。 Java学习路线图如下： 首先要学习Java SE，掌握Java语言本身、Java核心开发技术以及Java标准库的使用； 如果继续学习Java EE，那么Spring框架、数据库开发、分布式架构就是需要学习的； 如果要学习大数据开发，那么Hadoop、Spark、Flink这些大数据平台就是需要学习的，他们都基于Java或Scala开发； 如果想要学习移动开发，那么就深入Android平台，掌握Android App开发。 Java版本 版本 时间 1.0 1995 1.2 1998 1.3 2000 1.4 2002 1.5 / 5.0 2004 1.6 / 6.0 2005 1.7 / 7.0 2011 1.8 / 8.0 2014 1.9 / 9.0 2017/9 10 2018/3 11 2018/9 补充： 2004年9月30日18:00PM，J2SE1.5发布，成为Java语言发展史上的又一里程碑。为了表示该版本的重要性，J2SE1.5更名为Java SE 5.0 2005年6月，JavaOne大会召开，SUN公司公开Java SE 6。此时，Java的各种版本已经更名，以取消其中的数字”2”：J2EE更名为Java EE，J2SE更名为Java SE，J2ME更名为Java ME 2009年04月20日，甲骨文74亿美元收购Sun。取得java的版权。 JRE/JVM/JDK JDK：Java Develpment Kit java 开发工具 JRE：Java Runtime Environment java运行时环境 JVM：java Virtual Machine java 虚拟机 简单地说，JRE就是运行Java字节码的虚拟机。但是，如果只有Java源码，要编译成Java字节码，就需要JDK，因为JDK除了包含JRE，还提供了编译器、调试器等开发工具。 三者关系如下： 1234567891011 ┌─ ┌──────────────────────────────────┐ │ │ Compiler, debugger, etc. │ │ └──────────────────────────────────┘JDK ┌─ ┌──────────────────────────────────┐ │ │ │ │ │ JRE │ JVM + Runtime Library │ │ │ │ │ └─ └─ └──────────────────────────────────┘ ┌───────┐┌───────┐┌───────┐┌───────┐ │Windows││ Linux ││ macOS ││others │ └───────┘└───────┘└───────┘└───────┘ JDK、JRE和JVM的区别与相互之间的联系]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beta分布和Thompson采样]]></title>
    <url>%2F2020%2F07%2F16%2FStatistics%2FBeta%E5%88%86%E5%B8%83%E5%92%8CThompson%E9%87%87%E6%A0%B7%2F</url>
    <content type="text"><![CDATA[$Beta$分布$Beta$分布是一个定义在[0,1]区间上的连续概率分布族，它有两个正值参数，称为形状参数，一般用$\alpha$和$\beta$表示 $Beta$分布的概率密度为： f(x ; \alpha, \beta)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1} u^{\alpha-1}(1-u)^{\beta-1} d u}=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}随机变量$X$服从参数为$\alpha, \beta$的$beta$分布，一般记作： X \sim \operatorname \\{Beta} (\alpha, \beta)$Beta$分布的期望： \frac{\alpha}{\alpha + \beta}$Beta$分布的方差： \frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}$Beta$分布的概率密度图: 从$Beta$分布的概率密度函数的图形我们可以看出，$Beta$分布有很多种形状，但都是在$0~1$区间内，因此$Beta$分布可以描述各种$0~1$区间内的形状（事件）。因此，它特别适合为某件事发生或者成功的概率建模。同时，当$α=1，β=1$的时候，它就是一个均匀分布。 123456789101112131415from scipy.stats import beta import matplotlib.pyplot as plt import numpy as np x = np.linspace(0, 1, 100) a_array = [1, 2, 4, 8] b_array = [1, 2, 4, 8] for i, a in enumerate(a_array): for j, b in enumerate(b_array): plt.plot(x, beta.pdf(x, a, b), lw=1, alpha=0.6, label='a='+str(a)+',b='+str(b)) plt.legend(frameon=False) plt.show() 贝塔分布主要有 $α$和 $β$两个参数，这两个参数决定了分布的形状，从上图及其均值和方差的公式可以看出： $α/(α+β)$也就是均值，其越大，概率密度分布的中心位置越靠近1，依据此概率分布产生的随机数也多说都靠近1，反之则都靠近0。 $α+β$越大，则分布越窄，也就是集中度越高，这样产生的随机数更接近中心位置，从方差公式上也能看出来。 案例$Beta$分布可以看作是一个概率的概率分布(如硬币正面朝上的概率的分布)，当我们不知道一个东西的具体概率是多少时，它给出了所有概率出现的可能性大小，可以理解为概率的概率分布—贝叶斯的思维。 以棒球为例子： 熟悉棒球运动的都知道有一个指标就是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。 现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。 对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。 接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取$α=81,β=219$ 之所以取这两个参数是因为： beta分布的均值是$\frac{\alpha}{\alpha+\beta}=\frac{81}{81+219}=0.27$ 从图中可以看到这个分布主要落在了$(0.2,0.35)$间，这是从经验中得出的合理的范围 在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率。也就是说beta分布可以看作一个概率的概率分布。 那么有了先验信息后，现在我们考虑一个运动员只打一次球，那么他现在的数据就是1中-1击。这时候我们就可以更新我们的分布了，让这个曲线做一些移动去适应我们的新信息。beta分布在数学上就给我们提供了这一性质，他与二项分布是共轭先验的。所谓共轭先验就是先验分布是beta分布，而后验分布同样是beta分布。结果很简单： \operatorname{Beta}\left(\alpha_{0}+\text { hits }, \beta_{0}+\text { misses }\right)如果我们得到了更多的数据，假设一共打了300次，其中击中了100次，200次没击中，那么这一新分布就是：$\operatorname{beta}(81+100,219+200)$ 可以看出，曲线更窄而且往右移动了（击球率更高），由此我们对于运动员的击球率有了更好的了解。新的贝塔分布的期望值为0.303，比直接计算$100/(100+200)=0.333$要低，是比赛季开始时的预计0.27要高，所以贝塔分布能够抛出掉一些偶然因素，比直接计算击球率更能客观反映球员的击球水平。 $Beta$分布和二项分布二项分布 P(\text {data} \mid \theta) \propto \theta^{z}(1-\theta)^{N-z}贝叶斯定理 P(\theta \mid d a t a)=\frac{P(d a t a \mid \theta) P(\theta)}{P(d a t a)} \propto P(d a t a \mid \theta) P(\theta)$P(\theta)$为$beta$分布时： \\P(\theta) = \operatorname{Beta}(a, b)=\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a, b)} \propto \theta^{a-1}(1-\theta)^{b-1}则此时的后验分布为: \begin{array}{c} P(\theta \mid d a t a) \propto \theta^{z}(1-\theta)^{N-z} * \theta^{a-1}(1-\theta)^{b-1} \\ \propto \theta^{a+z-1}(1-\theta)^{b+N-z-1} \end{array}令$a′=a+z,b′=b+N−z$： P(\theta \mid \text { data })=\frac{\theta^{a^{\prime}-1}(1-\theta)^{b^{\prime}-1}}{B\left(a^{\prime}, b^{\prime}\right)}可见，后验分布依然是一个$Beta$分布。所以，我们将$Beta$分布和二项分布称之为共轭分布 $Thompson$采样 $Thompson$采样的背后原理正是上述所讲的$Beta$分布，将$Beta$分布的 $\alpha$参数看成是推荐后用户点击的次数，把分布的 $\beta$ 参数看成是推荐后用户未点击的次数，则汤普森采样过程如下： 1、取出每一个候选对应的参数 a 和 b； 2、为每个候选用 a 和 b 作为参数，用贝塔分布产生一个随机数； 3、按照随机数排序，输出最大值对应的候选； 4、观察用户反馈，如果用户点击则将对应候选的 a 加 1，否则 b 加 1； $Thompson$采样为什么有效呢？ 如果一个候选被选中的次数很多，也就是 $a+b$ 很大了，它的分布会很窄，换句话说这个候选的收益已经非常确定了，就是说不管分布中心接近0还是1都几乎比较确定了。用它产生随机数，基本上就在中心位置附近，接近平均收益。 如果一个候选不但 a+b 很大，即分布很窄，而且 $a/(a+b) $也很大，接近 1，那就确定这是个好的候选项，平均收益很好，每次选择很占优势，就进入利用阶段。反之则有可能平均分布比较接近与0，几乎再无出头之日。 如果一个候选的 $a+b$ 很小，分布很宽，也就是没有被选择太多次，说明这个候选是好是坏还不太确定，那么分布就是跳跃的，这次可能好，下次就可能坏，也就是还有机会存在，没有完全抛弃。那么用它产生随机数就有可能得到一个较大的随机数，在排序时被优先输出，这就起到了前面说的探索作用。 代码实现： 1choice = numpy.argmax(pymc.rbeta(1 + self.wins, 1 + self.trials - self.wins))]]></content>
      <categories>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>Thompson</tag>
        <tag>Beta</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一颗B+ Tree能存储多少条数据]]></title>
    <url>%2F2020%2F07%2F12%2FMySQL%2FInnoDB%E5%BC%95%E6%93%8E%E7%9A%84%E4%B8%80%E9%A2%97B%2B%20Tree%E8%83%BD%E5%AD%98%E5%82%A8%E5%A4%9A%E5%B0%91%E6%9D%A1%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[InnoDB中一颗B+ Tree能存储多少条数据磁盘的结构 在回答这个问题之前，我们先来复习一下计算机机械磁盘的结构，如下图： 磁头（head）：主要就是读取磁盘表面磁方向和改变其方向，每个盘面有一个磁头，它极其贴近地悬浮在盘面上，但是绝对不与盘面接触，否则会损坏磁头和盘面； 磁道（track）：磁道是单个盘面上的同心圆，当磁盘旋转时，磁头若保持在一个位置上，则每个磁头都会在磁盘表面划出一个圆形轨迹，这些圆形轨迹就叫做磁道，一个盘面上的磁道可以有成千上万个。相邻磁道之间并不是紧挨着的，这是因为磁化单元相隔太近时磁性会产生相互影响，同时也为磁头的读写带来困难。 柱面（cylinder）：在有多个盘片构成的盘组中，由不同盘片的面，但处于同一半径圆的多个磁道组成的一个圆柱面。 扇区（sector）：磁盘上的每个磁道被等分为若干个弧段，这些弧段便是硬盘的扇区$Sector$。硬盘的第一个扇区，叫做引导扇区。扇区是被间隙$gap$分割的圆的片段，间隙未被磁化成0或者1。注意，扇区是读写磁盘最基本的单位，如果一个扇区因为某种原因被破坏，那么整个扇区的数据都会受影响。一个机械硬盘扇区的容量一般为512字节 单盘面结构图： 磁盘容量 Megatron747磁盘是一个典型的vintage-2008的大容量驱动器，它具有以下特性：8个圆盘，16个盘面，每个盘面有65536个磁道，每个磁道（平均）有256个扇区，每个扇区可以存储4096个字节（byte） 那整个磁盘容量的算法是： 16个盘面，乘以65536个磁道，乘以256个扇区，再乘以4096字节，即1665536256*4096=2^40 byte，也就是1TB的容量。 InnoDB存储引擎 在计算机中，磁盘存储数据最小单元是扇区，一个扇区的大小是512字节，而文件系统（例如XFS/EXT4）的最小单元是块，一个块的大小是4k，而对于InnoDB存储引擎也有自己的最小储存单元，页（Page），一个页的大小是16K。 InnoDB的所有数据文件（后缀为ibd的文件），大小始终都是16384（16k）的整数倍。 磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。 ​ 考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。 事实1： 不同容量的存储器，访问速度差异悬殊。 磁盘(ms级别) &lt;&lt; 内存(ns级别)， 100000倍 若内存访问需要1s，则一次外存访问需要一天 为了避免1次外存访问，宁愿访问内存100次…所以将最常用的数据存储在最快的存储器中 事实2 ： 从磁盘中读 1 B，与读写 1KB 的时间成本几乎一样 从以上数据中可以总结出一个道理，索引查询的数据主要受限于硬盘的I/O速度，查询I/O次数越少，速度越快。 $B+\ Tree$ 在InnoDB存储引擎中，每一个page对应B+ Tree的一个节点(Node) 根节点：InnoDB树的根节点由索引字段值和指针组成 叶节点：InnoDB的叶节点由索引字段值和对应的完整记录(row)组成 一般来说，B+ Tree的高度为3 现在，我们回到最开始的问题：一颗InnoDB的B+树大概存储多少条数据？ 假设索引为主键索引，主键ID为bigint类型，长度为8字节，而指针大小在InnoDB源码中设置为6字节，这样一共14字节。 这样根节点大约可以存放的指针个数约为：$16384/14=1170$。即意味着内部节点一共有大约1170个。 对于每一个内部节点来说(上图第二层)，大约含有$16384/14=1170$个指针，意味着每一个内部节点对应大约1170个叶子节点。则一共有大约$1170*1170$个叶子节点 叶子节点，假设一条数据的大小为$1K$， 则每个叶子节点大约存放$16k / 1k =16$条数据。 至此，我们可以算出颗InnoDB的B+树大约能存数据为： 11170*1170*16=21902400 答案为：两千多万条！ 也就是说，对于一个B+ Tree，从两千万条的数据中查找一条数据，只需要进行3次$I/O$.效率极大的得到了提升！！]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>B+ Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[$PySpark$在遇到$map$类型的列的一些处理]]></title>
    <url>%2F2020%2F07%2F10%2FSpark%2FPySpark%20%E5%9C%A8%E9%81%87%E5%88%B0map%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%88%97%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[$PySpark$ 在遇到$map$类型的列的一些处理 在$spark$中，有时会遇到$column$的类型是$array$和$map$类型的，这时候需要将它们转换为多行数据 $Explode\ array\ and\ map\ columns\ to\ rows$123456789101112131415import pysparkfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()arrayData = [ ('James',['Java','Scala'],&#123;'hair':'black','eye':'brown'&#125;), ('Michael',['Spark','Java',None],&#123;'hair':'brown','eye':None&#125;), ('Robert',['CSharp',''],&#123;'hair':'red','eye':''&#125;), ('Washington',None,None), ('Jefferson',['1','2'],&#123;&#125;) ]df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])df.printSchema()df.show() 1234567891011121314151617root |-- name: string (nullable = true) |-- knownLanguages: array (nullable = true) | |-- element: string (containsNull = true) |-- properties: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true)+----------+--------------+--------------------+| name|knownLanguages| properties|+----------+--------------+--------------------+| James| [Java, Scala]|[eye -&gt; brown, ha...|| Michael|[Spark, Java,]|[eye -&gt;, hair -&gt; ...|| Robert| [CSharp, ]|[eye -&gt; , hair -&gt;...||Washington| null| null|| Jefferson| [1, 2]| []|+----------+--------------+--------------------+ $explode – array\ column\ example$ $PySpark\ function$ explode(e: Column) is used to explode or create array or map columns to rows. When an array is passed to this function, it creates a new default column “col1” and it contains all array elements. When a map is passed, it creates two new columns one for key and one for value and each element in map split into the rows. $spark$提供$explode$函数explode(e: Column)， 当传入的column是array类型时，它会新建一个列，默认列名为col；当传入的column是map类型时，则会新建两个列，一个列为key，另一个为value 1234from pyspark.sql.functions import explodedf3 = df.select(df.name, explode(df.knownLanguages))df3.printSchema()df3.show() $output$123456789101112131415161718root |-- name: string (nullable = true) |-- col: string (nullable = true)+---------+------+| name| col|+---------+------+| James| Java|| James| Scala|| Michael| Spark|| Michael| Java|| Michael| null|| Robert|CSharp|| Robert| ||Jefferson| 1||Jefferson| 2|+---------+------+ 注意： Washington对应的$knownLanguages$字段是null，explode会忽略这种值，可以看到，结果集里并没有Washington的记录，如果需要保留，使用explode_outer函数 $explode – map\ column\ example$1234from pyspark.sql.functions import explodedf3 = df.select(df.name,explode(df.properties))df3.printSchema()df3.show() $output$123456789101112131415root |-- name: string (nullable = true) |-- key: string (nullable = false) |-- value: string (nullable = true)+-------+----+-----+| name| key|value|+-------+----+-----+| James| eye|brown|| James|hair|black||Michael| eye| null||Michael|hair|brown|| Robert| eye| || Robert|hair| red|+-------+----+-----+ $How\ to\ covert\ Map\ into\ multiple\ columns$ 有时候需要把$Map$类型的$colum$n进行以$key$为列名，$value$为列值的处理。如下： 12345from pyspark.sql import functions as Fdf.select(F.col("name"), F.col("properties").getItem("hair").alias("hair_color"), F.col("properties").getItem("eye").alias("eye_color")).show() $output$123456789+----------+----------+---------+| name|hair_color|eye_color|+----------+----------+---------+| James| black| brown|| Michael| brown| null|| Robert| red| ||Washington| null| null|| Jefferson| null| null|+----------+----------+---------+]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL的分区]]></title>
    <url>%2F2020%2F06%2F29%2FMySQL%2FMySql%E7%9A%84%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[1. 什么是表分区？表分区，是指根据一定规则，将数据库中的一张表分解成多个更小的，容易管理的部分。从逻辑上看，只有一张表，但是底层却是由多个物理分区组成。 2. 表分区与分表的区别分表：指的是通过一定规则，将一张表分解成多张不同的表。比如将用户订单记录根据时间成多个表。分表与分区的区别在于：分区从逻辑上来讲只有一张表，而分表则是将一张表分解成多张表。 3. 表分区有什么好处？ 分区表的数据可以分布在不同的物理设备上，从而高效地利用多个硬件设备。 和单个磁盘或者文件系统相比，可以存储更多数据。 优化查询。在where语句中包含分区条件时，可以只扫描一个或多个分区表来提高查询效率;涉及sum和count语句时，也可以在多个分区上并行处理，最后汇总结果。 分区表更容易维护。例如：想批量删除大量数据可以清除整个分区。 可以使用分区表来避免某些特殊的瓶颈，例如InnoDB的单个索引的互斥访问，ext3问价你系统的inode锁竞争等。 4. MySQL支持的分区类型有哪些？ RANGE分区：按照数据的区间范围分区 LIST分区：按照List中的值分区，与RANGE的区别是，range分区的区间范围值是连续的。 HASH分区 KEY分区 A、 HASH分区根据$MOD$（分区键）的值，把数据行存储到表的不同分区中，键值必须为INT类型的值，或者转换为INT类型进行HASH函数运算。12CREATE TABLE `order` (***)PARTITION BY HASH(order_id) PARTITIONS 4; B、RANGE分区根据分区表键值的范围把数据行存储到表的不同分区中，默认情况下使用VALUES LESS THAN属性，也即[0,100)。 适用分区键为日期或者时间类型，数据分布均衡，容易归档。 1234567CREATE TABLE `order` (***)PARTITION BY RANGE(product_id) (PARTITION p0 VALUES LESS THAN (1000),PARTITION p1 VALUES LESS THAN (2000),PARTITION p2 VALUES LESS THAN (3000),PARTITION p3 VALUES LESS THAN MAXVALUE); 注意: 每个分区都是按顺序定义的，从最低到最高。 当插入的记录中对应的分区键的值不在分区定义的范围中的时候，插入语句会失败。 Range分区中，分区键的值如果是NULL，将被作为一个最小值来处理。 C、LIST分区按分区键取值的列表进行分区，每一行数据须找到对的分区列表，否则数据插入失败。12345CREATE TABLE `order` (***)PARTITION BY LIST(partner_id) (PARTITION p0 VALUES IN (1,3,5,7,9),PARTITION p1 VALUES IN (2,4,6,8,10)); 注意： MYSQL的分区字段，必须包含在主键字段内如果有主键，那么分区字段必须是主键(之一);如果没有主键，则可以指定任意字段作为分区字段。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务的的四大特性]]></title>
    <url>%2F2020%2F06%2F09%2FMySQL%2F%E4%BA%8B%E5%8A%A1%E7%9A%84ACID%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[事务的定义 数据库事务(事务)是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。 事务的四大特性—ACID 原子性(Atomicity):事务包含的操作全部成功或者全部失败 一致性(Consistency):是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 隔离性(Isolation): 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 持久性(Durability):是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 以A给B转账1000元为例子: 如何同时保证上述交易中，A账户总金额减少1000，B账户-总金额增加1000？ A A账户如果同时在和C账户交易(T2)，如何让这两笔交易互不影响？ I 如果交易完成时数据库突然崩溃，如何保证交易数据成功保存在数据库中？ D 如何在支持大量交易的同时，保证数据的合法性(没有钱凭空产生或消失) ？ C 一致性是基础，也是最终目的，其他三个特性（原子性、隔离性和持久性）都是为了保证一致性的 原子性和一致性事务中的操作必须作为一个整体,全部进行,或者一起回滚.就像原子最为物质的最小单位一样,无法再细分下去.比如:A给B转账,那么A-100和B+100必须全部进行. 但是,保证原子性的情况下是否就能满足一致性?答案是:否 例如，事务1需要将100元转入帐号A：先读取帐号A的值，然后在这个值上加上100。但是，在这两个操作之间，另一个事务2修改了帐号A的值，为它增加了100元。那么最后的结果应该是A增加了200元。但事实上，事务1最终完成后，帐号A只增加了100元，因为事务2的修改结果被事务1覆盖掉了。问题根源在于没有满足隔离性]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用函数构建抽象]]></title>
    <url>%2F2020%2F06%2F02%2FSCIP%2F%E4%BD%BF%E7%94%A8%E5%87%BD%E6%95%B0%E6%9E%84%E5%BB%BA%E6%8A%BD%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[Chapter1: 使用函数构建抽象高阶函数作为一般方法的函数 sum_naturals() 1234567&gt;&gt;&gt; def sum_naturals(n): total, k = 0, 1 while k &lt;= n: total, k = total + k, k + 1 return total&gt;&gt;&gt; sum_naturals(100)5050 sum_cube() 1234567&gt;&gt;&gt; def sum_cubes(n): total, k = 0, 1 while k &lt;= n: total, k = total + pow(k, 3), k + 1 return total&gt;&gt;&gt; sum_cubes(100)25502500 sum_pi() \pi=\frac{8}{1 \cdot 3}+\frac{8}{5 \cdot 7}+\frac{8}{9 \cdot 11}+\dots1234567&gt;&gt;&gt; def pi_sum(n): total, k = 0, 1 while k &lt;= n: total, k = total + 8 / (k * (k + 2)), k + 4 return total&gt;&gt;&gt; pi_sum(100)3.121594652591009 这三个函数在背后都具有相同模式。它们大部分相同，只是名字、用于计算被加项的k的函数，以及提供k的下一个值的函数不同。我们可以通过向相同的模板中填充槽位来生成每个函数： 12345def &lt;name&gt;(n): total, k = 0, 1 while k &lt;= n: total, k = total + &lt;term&gt;(k), &lt;next&gt;(k) return total 对于以上三个函数我们可以抽象成以下函数: 12345&gt;&gt;&gt; def summation(n, term, next): total, k = 0, 1 while k &lt;= n: total, k = total + term(k), next(k) return total 要注意summation接受上界n，以及函数term和next作为参数。我们可以像任何函数那样使用summation，它简洁地表达了求和。 sum_cube() 12345678&gt;&gt;&gt; def cube(k): return pow(k, 3)&gt;&gt;&gt; def successor(k): return k + 1&gt;&gt;&gt; def sum_cubes(n): return summation(n, cube, successor)&gt;&gt;&gt; sum_cubes(3) 36 sum_pi() 1234567891011def pi_term(k): return 8 / (k * (k + 2))def pi_next(k): return k+4def sum_pi(n): return summation(n, pi, next)&gt;&gt;&gt; pi_sum(1e6) 3.1415906535898936 计算黄金分割比(gold ratio) \phi = 1 + \frac{1}{\phi}123456&gt;&gt;&gt; def golden_update(guess): return 1/guess + 1&gt;&gt;&gt; def golden_test(guess): return near(guess, square, successor)&gt;&gt;&gt; iter_improve(golden_update, golden_test)1.6180371352785146 $Lambda$ 表达式 Python 中，我们可以使用 Lambda 表达式凭空创建函数，它会求值为匿名函数。Lambda 表达式是函数体具有单个返回表达式的函数，不允许出现赋值和控制语句。12 lambda x : f(g(x))&quot;A function that takes x and returns f(g(x))&quot; 1234561 def compose1(f, g):2 return lambda x: f(g(x))3 4 f = compose1(lambda x: x * x,5 lambda y: y + 1)6 result = f(12) 抽象和一等函数 作为程序员，我们应该留意识别程序中低级抽象的机会，在它们之上构建，并泛化它们来创建更加强大的抽象。这并不是说，一个人应该总是尽可能以最抽象的方式来编程；专家级程序员知道如何选择合适于他们任务的抽象级别。但是能够基于这些抽象来思考，以便我们在新的上下文中能使用它们十分重要。高阶函数的重要性是，它允许我们更加明显地将这些抽象表达为编程语言中的元素，使它们能够处理其它的计算元素。 通常，编程语言会限制操作计算元素的途径。带有最少限制的元素被称为具有一等地位。一些一等元素的“权利和特权”是： 它们可以绑定到名称。 它们可以作为参数向函数传递。 它们可以作为函数的返回值返回。 它们可以包含在数据结构中。 Python 总是给予函数一等地位，所产生的表现力的收益是巨大的。另一方面，控制结构不能做到：你不能像使用sum那样将if传给一个函数。 补充阅读如何正确理解Python函数是第一类对象]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>SICP</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[repartition和coalesce区别]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Frepartition%E5%92%8Ccoalesce%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[简介$repartition(numPartitions:Int)$ 和 $coalesce(numPartitions:Int，shuffle:Boolean=false)$ 作用：对RDD的分区进行重新划分，repartition内部调用了coalesce，参数shuffle=true分析例：RDD有N个分区，需要重新划分成M个分区12345678N小于M一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。N大于M且和M相差不多假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。N大于M且和M相差悬殊这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们在同一个Stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。 总结：返回一个减少到numPartitions个分区的新RDD，这会导致窄依赖例如：你将1000个分区转换成100个分区，这个过程不会发生shuffle，相反如果10个分区转换成100个分区将会发生shuffle。然而如果你想大幅度合并分区，例如所有partition合并成一个分区，这会导致计算在少数几个集群节点上进行（言外之意：并行度不够）。‘为了避免这种情况，你可以将第二个shuffle参数传递一个true，这样会在重新分区过程中多一步shuffle，这意味着上游的分区可以并行运行。 总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的partition数变多的。 进一步理解123N大于M且和M相差悬殊将shuffle设置为false，父子RDD是窄依赖关系，他们在同一个Stage中，就可能造成Spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以将shuffle设置为true 每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的！！1234和repartition有所区别的是，coalesce并不是一个shuffle算子。也就说coalesce不会触发shuffle操作，它是包含在当前的stage中的。由于，每个Stage里面的Task的数量是由该Stage中最后一个RDD的Partition的数量所决定的。就会引起这样一种现象：由于coalesce算子的存在，必然导致运算后的partition数目的减少。也就是说当前的stage的并行的task数目(并行度)会降低。每个task计算的数据(分区)会加大。可以将shuffle设置为true来触发shuffle，从而不会降低当前stage的task数量(并行度)。 由图可见，在coalesce操作之前，每个rdd有四个partition，如果没有soalesce操作,当前stage的并行度为4.但是由于coalesce操作的存在，导致分区数变为2。所以整个stage的并行度为2，在实际运行时，excutor只会为当前的stage启动2个核。也就是说输入的4个partition会被分布到两个task上运行，每个task上分到两个partion。每个task运算的数据量变大，运行速度就会被拖慢。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark运行内存超出]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2F%E5%85%B3%E4%BA%8ESpark%E8%BF%90%E8%A1%8C%E4%B8%AD%E5%86%85%E5%AD%98%E8%B6%85%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Container killed by YARN for exceeding memory limits？运行spark脚本时，经常会碰到Container killed by YARN for exceeding memory limits的错误，导致程序运行失败。 这个的意思是指executor的外堆内存超出了。默认情况下，这个值被设置为executor_memory的10%或者384M，以较大者为准，即max(executor_memory*.1, 384M). 解决办法 提高内存开销 减少执行程序内核的数量 增加分区数量 提高驱动程序和执行程序内存 提高内存开销 即直接指定堆外内存的大小1spark.conf.set(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;4g&quot;) 减少执行程序内核的数量 这可减少执行程序可以执行的最大任务数量，从而减少所需的内存量。 增加分区数量 要增加分区数量，请为原始弹性分布式数据集增加 spark.default.parallelism 的值，或执行.repartition() 操作。增加分区数量可减少每个分区所需的内存量。 提高驱动程序和执行程序内存 即通过增大executor.memory的值来增大堆外内存,但是可以看到，由于乘了10%，所以提升其实很有限。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark并行度理解]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FSpark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%9A%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE%E5%B9%B6%E8%A1%8C%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[Spark性能调优：合理设置并行度1. Spark的并行度指的是什么？并行度其实就是指的是spark作业中,各个stage的同时运行的task的数量,也就代表了spark作业在各个阶段stage的并行度！ 并行度 = executor\_number * executor\_cores理解：sparkApplication的划分： $job —&gt; stage —&gt; task$一般每个task一次处理一个分区。 可以将task理解为比赛中的跑道：每轮比赛中，每个跑道上都会有一位运动员(分区，即处理的数据)，并行度就是跑道的数量，一轮比赛就可以理解为一个stage。 2.如果不调节并行度，导致并行度过低会怎么样？假设现在已经在spark-submit脚本里面，给我们的spark作业分配了足够多的资源，比如有50个 executor，每个executor 有10G内存，每个 executor 有3个cpu core，基本已经达到了集群或者yarn队列的资源上限。 如果 task 没有设置，或者设置的很少，比如就设置了100个 task。现在50个 executor，每个executor 有3个cpu core，也就是说，你的Application任何一个 stage 运行的时候都有总数在150个 cpu core，可以并行运行。但是你现在只有100个task，平均分配一下，每个executor 分配到2个task，那么同时在运行的task只有100个，每个executor只会并行运行2个task，每个executor剩下的一个 cpu core 就浪费掉了。 你的资源虽然分配足够了，但是问题是，并行度没有与资源相匹配，导致你分配下去的资源都浪费掉了。 合理的并行度的设置，应该是要设置的足够大，大到可以完全合理的利用你的集群资源。比如上面的例子，总共集群有150个cpu core，可以并行运行150个task。那么就应该将你的Application 的并行度至少设置成150才能完全有效的利用你的集群资源，让150个task并行执行，而且task增加到150个以后，既可以同时并行运行，还可以让每个task要处理的数据量变少。比如总共150G的数据要处理，如果是100个task，每个task计算1.5G的数据，现在增加到150个task可以并行运行，而且每个task主要处理1G的数据就可以。 很简单的道理，只要合理设置并行度，就可以完全充分利用你的集群计算资源，并且减少每个task要处理的数据量，最终，就是提升你的整个Spark作业的性能和运行速度。 3. 如何去提高并行度？(1) task数量，至少设置成与spark Application 的总cpu core 数量相同（最理性情况，150个core，分配150task，一起运行，差不多同一时间运行完毕） 官方推荐，task数量，设置成spark Application 总cpu core数量的2~3倍 ，比如150个cpu core ，基本设置 task数量为 300~ 500， 与理性情况不同的，有些task 会运行快一点，比如50s 就完了，有些task 可能会慢一点，要一分半才运行完，所以如果你的task数量，刚好设置的跟cpu core 数量相同，可能会导致资源的浪费，因为 比如150task ，10个先运行完了，剩余140个还在运行，但是这个时候，就有10个cpu core空闲出来了，导致浪费。如果设置2~3倍，那么一个task运行完以后，另外一个task马上补上来，尽量让cpu core不要空闲。同时尽量提升spark运行效率和速度。提升性能。 (2) 如何设置一个Spark Application的并行度？123456对于RDD来说：可以通过设置spark.default.parallelism 参数来决定shuffle操作之后的partition数目，默认是没有值的，如果设置了值,比如说100，是在shuffle的过程才会起作用new SparkConf().set(“spark.default.parallelism”, “500”)对于sparksql来说：可以通过设置spark.sql.shuffle.partitions参数，默认值为200; (3) repartition算子repartiton算子通过传入想要的分区数目，来改变分区数。注意：repartion是一个shuffle算子 (4) 在算子中加入指定的参数，来指定分区数目]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark中的各种概念]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Fspark%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spark中的各种概念的理解Application：通俗讲，用户每次提交的所有的代码为一个application。 Job：一个application可以分为多个job。如何划分job？通俗讲，触发一个final RDD的实际计算（action）为一个job Stage：一个job可以分为多个stage。根据一个job中的RDD的宽依赖和窄依赖关系进行划分 Task：task是最小的基本的计算单位。一般是一个RDD的一个分区（partition）为一个task，大约是128M 并行度：是指指令并行执行的最大条数。在指令流水中，同时执行多条指令称为指令并行。 理论上：每一个stage下有多少的分区，就有多少的task，task的数量就是我们任务的最大的并行度。 （一般情况下，我们一个task运行的时候，使用一个cores） 实际上：最大的并行度，取决于我们的application任务运行时使用的executor拥有的cores的数量。 spark 基本概念解析 Spark运行流程 Application 首先被 Driver 构建 DAG 图并分解成 Stage。 然后 Driver 向 Cluster Manager 申请资源。 Cluster Manager 向某些 Work Node 发送征召信号。 被征召的 Work Node 启动 Executor 进程响应征召，并向 Driver 申请任务。 Driver 分配 Task 给 Work Node。 Executor 以 Stage 为单位执行 Task，期间 Driver 进行监控。 Driver 收到 Executor 任务完成的信号后向 Cluster Manager 发送注销信号。 Cluster Manager 向 Work Node 发送释放资源信号。 Work Node 对应 Executor 停止运行。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cache和persist比较]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Fcache%E7%AE%97%E5%AD%90%E5%92%8Cpersist%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[Spark中cache和persist的作用Spark开发高性能的大数据计算作业并不是那么简单。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。 有一些代码开发基本的原则，避免创建重复的RDD，尽可能复用同一个RDD，如下，我们可以直接用一个RDD进行多种操作：123val rdd1 = sc.textFile("xxx")rdd1.collectrdd1.show 但是Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。对于上面的代码，sc.textFile("xxx")会执行两次，这种方式的性能是很差的。 因此对于这种情况，我的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。 持久化如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。123val rdd1 = sc.textFile("xxx").cacherdd1.collectrdd1.show cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。 此时再对rdd1执行两次算子操作时，只有在第一次算子时，才会将这个rdd1从源头处计算一次。第二次执行算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。1234/** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */def cache(): this.type = persist() 通过源码可以看出cache()是persist()的简化方式，调用persist的无参版本，也就是调用persist(StorageLevel.MEMORY_ONLY)，cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中 persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。 1234/** * Persist this RDD with the default storage level (`MEMORY_ONLY`).*/def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) 默认缓存级别是StorageLevel.MEMORY\_ONLY.也就是说cache的默认级别就是MEMORY\_ONLY DataFrame的cache和persist的区别官网和上的教程说的都是RDD,但是没有讲df的缓存，通过源码发现df和rdd还是不太一样的:1234567891011121314151617181920212223242526272829/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */def cache(): this.type = persist()/** * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`). * * @group basic * @since 1.6.0 */def persist(): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this) this&#125;def persist(newLevel: StorageLevel): this.type = &#123; sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel) this&#125;def cacheQuery( query: Dataset[_], tableName: Option[String] = None, storageLevel: StorageLevel = MEMORY_AND_DISK): Unit = writeLock 可以到cache()依然调用的persist()，但是persist调用cacheQuery，而cacheQuery的默认存储级别为MEMORY_AND_DISK，这点和rdd是不一样的。 RDD的缓存级别每个持久化的 RDD 可以使用不同的存储级别进行缓存，例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。这些存储级别通过传递一个 StorageLevel 对象给 persist() 方法进行设置。详细的存储级别介绍如下： MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。 MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。 MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。 MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。 DISK_ONLY : 只在磁盘上缓存 RDD。 MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。 OFF_HEAP（实验中）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。 注意，在 Python 中，缓存的对象总是使用 Pickle 进行序列化，所以在 Python 中不关心你选择的是哪一种序列化级别。python 中的存储级别包括 MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY 和 DISK_ONLY_2 。 持久化策略的选择 默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。 如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。 如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。 通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。 使用Cache注意下面三点（1）cache之后一定不能立即有其它算子，不能直接去接算子。因为在实际工作的时候，cache后有算子的话，它每次都会重新触发这个计算过程。 （2）cache不是一个action，运行它的时候没有执行一个作业。 （3）cache缓存如何让它释放缓存：unpersist，它是立即执行的。persist是lazy级别的（没有计算）unpersist时eager级别的。意味着unpersist如果定义在action算子之前，则cache失效]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD算子总结]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FRDD%E7%AE%97%E5%AD%90%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[RDD算子总结从功能上分：转换算子(transformer)： lazy执行，生成新的rdd，只有在调用action算子时，才会真正的执行。如：map 、flatmap、filter、 union、 join、 ruduceByKey、 cache 行动算子(action)： 触发任务执行，产生job，返回值不再是rdd。如：count 、collect、top、 take、 reduce 从作用上分：通用的： map、 flatMap、 distinct、 union 作用于RDD[K,V]： mapValues、 reduceByKey、 groupByKey、 sortByKey、 转换算子是否有shuffleshuffle类: reduceByKey、 groupByKey、 groupBy、 join、 distinct、 repartition 非shuffle类: map、 filter、 union、flatMap、 coalesce Spark算子使用案例总结]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD基础入门]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FRDD%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[RDD简介 RDD—弹性分布式数据集（Resilient Distributed Dataset）是spark的核心概念。RDD其实就是分布式的元素集合。在Spark中，对数据的所有操作不外乎创建RDD，转化已有的RDD以及调用RDD操作进行求值。而在这一切的背后，spark会自动讲RDD中的数据分发到集群上，并将操作并行化执行。 RDD基础 RDD是一个不可变的分布式对象集合.每个RDD被分为多个分区，这些分区运行在集群中的不同节点上。RDD可以包含Python、Java、Scala中任意类型的对象。 每个spark程序无外乎都是下面的流程: 1.从外部数据创建输入RDD 2.使用诸如filter()这样的操作对RDD进行转化，定义新的RDD 3.告诉spark对需要被重用的中间RDD执行persisit()操作 4.使用行动操作(count(),first())触发一次并行计算，spark并不会立马执行，而是优化后再执行 1、创建RDD Spark提供了两种方法创建RDD的方法： 读取外部数据集 在驱动器程序中对一个集合进行并行化 12345#parallelize方法将集合转化为rddlines = sc.parallelize([&quot;pandas&quot;, &quot;i like pandas&quot;])#textFile方法lines=sc.textFile(&quot;README.md&quot;) 2、RDD操作spark支持两种操作：转化操作，行动操作 转化操作 转化操作执行时返回新的RDD的操作，转化出来的RDD是惰性求值的，只有在行动中用到这些RDD时才会被计算，许多转化操作只会操作RDD中的一个元素，并不是所有的转化操作都是这样 比如提取日志文件的错误信息 12345inputRDD=sc.testFile(&quot;log.txt&quot;)errorsRDD=inputRDD.filter(lambda x:&quot;error&quot; in x)warningsRDD = inputRDD.filter(lambda x: &quot;warning&quot; in x)# 将errorsRDD和warningsRDD合并成一个RDDbadLinesRDD = errorsRDD.union(warningsRDD) 通过转化操作，从已有的RDD中派生新的RDD，spark会使用谱系图记录这些RDD的依赖关系，spark在需要用这些信息的时候按需计算每个RDD，也可以依靠谱系图在丢失数据的情况下恢复丢失的数据 行动操作 行动操作需要实际的输出，它会强制执行哪些求值必须用到的RDD转化操作 示例：对badLinesRDD进行计数操作，并且打印前十条记录 1234print &quot;Input had &quot; + badLinesRDD.count() + &quot; concerning lines&quot;# take(num) 从RDD中取出num个元素for line in badLinesRDD.take(10): print line 这里使用了take()取出少量的数据集，也可以使用collect()函数获取整个RDD中的数据，但是使用collect需要注意内存是否够用。如果数据集特别大的时候，我们需要把数据写到诸如HDFS之类的分布式存储系统，当调用一个新的行动操作的时候整个RDD会从头计算，我们要将中间结果持久化 惰性求值 RDD的转化操作都是多心求值的，这意味着在被调用行动操作之前Spark不会开始计算。 惰性求值意味着我们对RDD调用转化操作（例如map()）时，操作不会立即执行，相反，Spark会在内部记录下所要求执行的操作的相关信息。我们不应该把RDD看作放着特定数据的数据集，而最好把每个RDD看作我们通过转化操作构建出来的、记录如何计算数据的指令列表。把数据读到RDD的操作也是惰性的，因此，当我们调用sc.textFile()时，数据并没有读取进来，而是在必要时才会读取。 3、函数传递1234567#使用lambda方法传递word = rdd.filter(lambda s: &quot;error&quot; in s)#定义一个函数然后传递def containsError(s): return &quot;error&quot; in sword = rdd.filter(containsError) 传递函数时要小心，python会在不经意间把函数所在的对象也序列化传出，有时如果传递的类里包含了python不知道如何序列化输出的对象，也可能导致程序失败。 如下是一个错误的函数传递示例； 1234567891011class SearchFunctions(object): def __init__(self, query): self.query = query def isMatch(self, s): return self.query in s def getMatchesFunctionReference(self, rdd): # 问题：在&quot;self.isMatch&quot;中引用了整个self return rdd.filter(self.isMatch) def getMatchesMemberReference(self, rdd): # 问题：在&quot;self.query&quot;中引用了整个self return rdd.filter(lambda x: self.query in x) 正确做法: 12345class WordFunctions(object): def getMatchesNoReference(self, rdd): # 安全：只把需要的字段提取到局部变量中 query = self.query return rdd.filter(lambda x: query in x) 4、RDD操作 常见的转化操作 常见的行动操作 5、持久化(缓存) 如前所述， SparkRDD是惰性求值的，而有时我们希望能多次使用同一个 RDD。如果简单地对 RDD 调用行动操作， Spark 每次都会重算 RDD 以及它的所有依赖。这在迭代算法中消耗格外大，因为迭代算法常常会多次使用同一组数据。 如下就是先对 RDD 作一次计数、再把该 RDD 输出的一个小例子。 123val result = input.map(x =&gt; x*x)println(result.count())println(result.collect().mkString(",")) 为了避免多次计算同一个 RDD，可以让 Spark 对数据进行持久化。当我们让 Spark 持久化存储一个 RDD 时，计算出 RDD 的节点会分别保存它们所求出的分区数据。如果一个有持久化数据的节点发生故障， Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。 出于不同的目的，我们可以为 RDD 选择不同的持久化级别（如表 3-6 所示）。在 Scala和 Java 中，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中。在 Python 中，我们会始终序列化要持久化存储的数据，所以持久化级别默认值就是以序列化后的对象存储在 JVM 堆空间中。当我们把数据写到磁盘或者堆外存储上时，也总是使用序列化后的数据。 1234val result = input.map(x =&gt; x * x)result.persist(StorageLevel.DISK_ONLY)println(result.count())println(result.collect().mkString(",")) 如果要缓存的数据太多， 内存中放不下， Spark 会自动利用最近最少使用（ LRU）的缓存策略把最老的分区从内存中移除。 对于仅把数据存放在内存中的缓存级别，下一次要用到已经被移除的分区时， 这些分区就需要重新计算。但是对于使用内存与磁盘的缓存级别的分区来说，被移除的分区都会写入磁盘。不论哪一种情况，都不必担心你的作业因为缓存了太多数据而被打断。 不过，缓存不必要的数据会导致有用的数据被移出内存，带来更多重算的时间开销]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce介绍]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2FMapReduce%2F</url>
    <content type="text"><![CDATA[Hadoop简介 Hadoop就是一个实现了Google云计算系统的开源系统，包括并行计算模型Map/Reduce，分布式文件系统HDFS，以及分布式数据库Hbase，同时Hadoop的相关项目也很丰富，包括ZooKeeper，Pig，Chukwa，Hive，Hbase，Mahout，flume等. 这里详细分解这里面的概念让大家通过这篇文章了解到底是什么hadoop： Map/Reduce： MapReduce是hadoop的核心组件之一，hadoop要分布式包括两部分，一是分布式文件系统hdfs,一部是分布式计算框架，就是mapreduce,缺一不可，也就是说，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程。 Mapreduce是一种编程模型，是一种编程方法，抽象理论。 下面是一个关于一个程序员是如何个妻子讲解什么是MapReduce？文章很长请耐心的看。 123456789101112131415161718192021222324252627282930313233343536373839404142434445我问妻子：“你真的想要弄懂什么是MapReduce？” 她很坚定的回答说“是的”。 因此我问道：我： 你是如何准备洋葱辣椒酱的？（以下并非准确食谱，请勿在家尝试）妻子： 我会取一个洋葱，把它切碎，然后拌入盐和水，最后放进混合研磨机里研磨。这样就能得到洋葱辣椒酱了。妻子： 但这和MapReduce有什么关系？我： 你等一下。让我来编一个完整的情节，这样你肯定可以在15分钟内弄懂MapReduce.妻子： 好吧。我：现在，假设你想用薄荷、洋葱、番茄、辣椒、大蒜弄一瓶混合辣椒酱。你会怎么做呢？妻子： 我会取薄荷叶一撮，洋葱一个，番茄一个，辣椒一根，大蒜一根，切碎后加入适量的盐和水，再放入混合研磨机里研磨，这样你就可以得到一瓶混合辣椒酱了。我： 没错，让我们把MapReduce的概念应用到食谱上。Map和Reduce其实是两种操作，我来给你详细讲解下。Map（映射）: 把洋葱、番茄、辣椒和大蒜切碎，是各自作用在这些物体上的一个Map操作。所以你给Map一个洋葱，Map就会把洋葱切碎。 同样的，你把辣椒，大蒜和番茄一一地拿给Map，你也会得到各种碎块。 所以，当你在切像洋葱这样的蔬菜时，你执行就是一个Map操作。 Map操作适用于每一种蔬菜，它会相应地生产出一种或多种碎块，在我们的例子中生产的是蔬菜块。在Map操作中可能会出现有个洋葱坏掉了的情况，你只要把坏洋葱丢了就行了。所以，如果出现坏洋葱了，Map操作就会过滤掉坏洋葱而不会生产出任何的坏洋葱块。Reduce（化简）:在这一阶段，你将各种蔬菜碎都放入研磨机里进行研磨，你就可以得到一瓶辣椒酱了。这意味要制成一瓶辣椒酱，你得研磨所有的原料。因此，研磨机通常将map操作的蔬菜碎聚集在了一起。妻子： 所以，这就是MapReduce?我： 你可以说是，也可以说不是。 其实这只是MapReduce的一部分，MapReduce的强大在于分布式计算。妻子： 分布式计算？ 那是什么？请给我解释下吧。我： 没问题。我： 假设你参加了一个辣椒酱比赛并且你的食谱赢得了最佳辣椒酱奖。得奖之后，辣椒酱食谱大受欢迎，于是你想要开始出售自制品牌的辣椒酱。假设你每天需要生产10000瓶辣椒酱，你会怎么办呢？妻子： 我会找一个能为我大量提供原料的供应商。我：是的..就是那样的。那你能否独自完成制作呢？也就是说，独自将原料都切碎？ 仅仅一部研磨机又是否能满足需要？而且现在，我们还需要供应不同种类的辣椒酱，像洋葱辣椒酱、青椒辣椒酱、番茄辣椒酱等等。妻子： 当然不能了，我会雇佣更多的工人来切蔬菜。我还需要更多的研磨机，这样我就可以更快地生产辣椒酱了。我：没错，所以现在你就不得不分配工作了，你将需要几个人一起切蔬菜。每个人都要处理满满一袋的蔬菜，而每一个人都相当于在执行一个简单的Map操作。每一个人都将不断的从袋子里拿出蔬菜来，并且每次只对一种蔬菜进行处理，也就是将它们切碎，直到袋子空了为止。这样，当所有的工人都切完以后，工作台（每个人工作的地方）上就有了洋葱块、番茄块、和蒜蓉等等。妻子：但是我怎么会制造出不同种类的番茄酱呢？我：现在你会看到MapReduce遗漏的阶段—搅拌阶段。MapReduce将所有输出的蔬菜碎都搅拌在了一起，这些蔬菜碎都是在以key为基础的 map操作下产生的。搅拌将自动完成，你可以假设key是一种原料的名字，就像洋葱一样。 所以全部的洋葱keys都会搅拌在一起，并转移到研磨洋葱的研磨器里。这样，你就能得到洋葱辣椒酱了。同样地，所有的番茄也会被转移到标记着番茄的研磨器里，并制造出番茄辣椒酱。 上面都是从理论上来说明什么是MapReduce，那么咱们在MapReduce产生的过程和代码的角度来理解这个问题。 如果想统计下过去10年计算机论文出现最多的几个单词，看看大家都在研究些什么，那收集好论文后，该怎么办呢？ 方法一： 我可以写一个小程序，把所有论文按顺序遍历一遍，统计每一个遇到的单词的出现次数，最后就可以知道哪几个单词最热门了。 这种方法在数据集比较小时，是非常有效的，而且实现最简单，用来解决这个问题很合适。 方法二：写一个多线程程序，并发遍历论文. 这个问题理论上是可以高度并发的，因为统计一个文件时不会影响统计另一个文件。当我们的机器是多核或者多处理器，方法二肯定比方法一高效。但是写一个多线程程序要比方法一困难多了，我们必须自己同步共享数据，比如要防止两个线程重复统计文件。 方法三： 把作业交给多个计算机去完成。我们可以使用方法一的程序，部署到N台机器上去，然后把论文集分成N份，一台机器跑一个作业。这个方法跑得足够快，但是部署起来很麻烦，我们要人工把程序copy到别的机器，要人工把论文集分开，最痛苦的是还要把N个运行结果进行整合（当然我们也可以再写一个程序）。 方法四： 让MapReduce来帮帮我们吧！ MapReduce本质上就是方法三，但是如何拆分文件集，如何copy程序，如何整合结果这些都是框架定义好的。我们只要定义好这个任务（用户程序），其它都交给MapReduce。 map函数和reduce函数 123map函数： 接受一个键值对（key-value pair），产生一组中间键值对。MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。 reduce函数： 接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。 在统计词频的例子里，map函数接受的键是文件名，值是文件的内容，map逐个遍历单词，每遇到一个单词w ，就产生一个中间键值对，这表示单词w咱又找到了一个；MapReduce将键相同（都是单词w）的键值对传给reduce函数，这样reduce函数接受的键就是单词w，值是一串”1”（最基本的实现是这样，但可以优化），个数等于键为w的键值对的个数，然后将这些“1”累加就得到单词w的出现次数。最后这些单词的出现次数会被写到用户定义的位置，存储在底层的分布式存储系统（GFS或HDFS）。 执行过程 图1展示了我们的实现中MapReduce操作的整体流程。当用户程序调用MapReduce函数时，会发生下面一系列动作（图1中的标号与下面列表顺序相同）： 12345678910111213141516171819201. 用户程序中的MapReduce库首先将输入文件切分为M块，每块的大小从16MB到64MB（用户可通过一个可选参数控制此大小）。然后MapReduce库会在一个集群的若干台机器上启动程序的多个副本。2. 程序的各个副本中有一个是特殊的——主节点，其它的则是工作节点。主节点将M个map任务和R个reduce任务分配给空闲的工作节点，每个节点一项任务。3. 被分配map任务的工作节点读取对应的输入区块内容。它从输入数据中解析出key/value对，然后将每个对传递给用户定义的map函数。由map函数产生的中间key/value对都缓存在内存中。4. 缓存的数据对会被周期性的由划分函数分成R块，并写入本地磁盘中。这些缓存对在本地磁盘中的位置会被传回给主节点，主节点负责将这些位置再传给reduce工作节点。5. 当一个reduce工作节点得到了主节点的这些位置通知后，它使用RPC调用去读map工作节点的本地磁盘中的缓存数据。当reduce工作节点读取完了所有的中间数据，它会将这些数据按中间key排序，这样相同key的数据就被排列在一起了。同一个reduce任务经常会分到有着不同key的数据，因此这个排序很有必要。如果中间数据数量过多，不能全部载入内存，则会使用外部排序。6. reduce工作节点遍历排序好的中间数据，并将遇到的每个中间key和与它关联的一组中间value传递给用户的reduce函数。reduce函数的输出会写到由reduce划分过程划分出来的最终输出文件的末尾。7. 当所有的map和reduce任务都完成后，主节点唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码中。 成功完成后，MapReduce执行的输出都在R个输出文件中（每个reduce任务产生一个，文件名由用户指定）。通常用户不需要合并这R个输出文件——他们经常会把这些文件当作另一个MapReduce调用的输入，或是用于另一个可以处理分成多个文件输入的分布式应用。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark2.0的新特性]]></title>
    <url>%2F2020%2F05%2F26%2FSpark%2Fspark2.0%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[Spark2.0Spark直接从1.6跨入2.0版本，带来一些新的特性。最大的变化便是SparkSession整合了各种环境。 Spark2.0中引入了SparkSession的概念，它为用户提供了一个统一的切入点来使用Spark的各项功能，用户不但可以使用DataFrame和Dataset的各种API，学习Spark的难度也会大大降低。 SparkSession在Spark的早期版本，SparkContext是进入Spark的切入点。我们都知道RDD是Spark中重要的API，然而它的创建和操作得使用sparkContext提供的API；对于RDD之外的其他东西，我们需要使用其他的Context。比如对于流处理来说，我们得使用StreamingContext；对于SQL得使用sqlContext；而对于hive得使用HiveContext。然而DataSet和Dataframe提供的API逐渐称为新的标准API，我们需要一个切入点来构建它们，所以在 Spark 2.0中我们引入了一个新的切入点(entry point)：SparkSession SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。 之前的写法：123456789from pyspark import SparkContext, SparkConffrom pyspark.sql import SQLContextconf = SparkConf().setMaster("local[*]").setAppName("PySparkShell")sc = SparkContext(conf=conf)sqlContest = SQLContext(sc)spark = SQLContext(sc)spark.sql(select **)··· 现在的写法123456789101112131415from pyspark.sql import SparkSessionspark = SparkSession .builder .appName("Python Spark SQL basic example") .config("spark.some.config.option","some-value") .enableHiveSupport() .getOrCreate()df1 = spark.sql(select **) df2 = spark.read.csv('./python/test_support/sql/ages.csv') # 通过spark创建scsc = spark.sparkContextrdd1 = sc.parallelize([1,2,3,4,5]) 其中： 在pyspark sql中换行要 \ .getOrCreate() 指的是如果当前存在一个SparkSession就直接获取，否则新建。 .enableHiveSupport() 使我们可以从读取或写入数据到hive。.enableHiveSupport 函数的调用使得SparkSession支持hive，类似于HiveContext]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark生态介绍]]></title>
    <url>%2F2019%2F08%2F29%2FSpark%2Fspark%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[生态： Spark Core：包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的 Spark SQL：提供通过Apache Hive的SQL变体Hive查询语言（HiveQL）与Spark进行交互的API。每个数据库表被当做一个RDD，Spark SQL查询被转换为Spark操作。 Spark Streaming：对实时数据流进行处理和控制。Spark Streaming允许程序能够像普通RDD一样处理实时数据 MLlib：一个常用机器学习算法库，算法被实现为对RDD的Spark操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。 GraphX：控制图、并行图操作和计算的一组算法和工具的集合。GraphX扩展了RDD API，包含控制图、创建子图、访问路径上所有顶点的操作 架构： Cluster Manager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器 Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。 Driver： 运行Application 的main()函数 Executor：执行器，是为某个Application运行在worker node上的一个进程 术语： Application: Appliction都是指用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码 Driver: Spark中的Driver即运行上述Application的main函数并创建SparkContext，创建SparkContext的目的是为了准备Spark应用程序的运行环境，在Spark中有SparkContext负责与ClusterManager通信，进行资源申请、任务的分配和监控等，当Executor部分运行完毕后，Driver同时负责将SparkContext关闭，通常用SparkContext代表Driver Executor: 某个Application运行在worker节点上的一个进程， 该进程负责运行某些Task， 并且负责将数据存到内存或磁盘上，每个Application都有各自独立的一批Executor， 在Spark on Yarn模式下，其进程名称为CoarseGrainedExecutor Backend。一个CoarseGrainedExecutor Backend有且仅有一个Executor对象， 负责将Task包装成taskRunner,并从线程池中抽取一个空闲线程运行Task， 这个每一个oarseGrainedExecutor Backend能并行运行Task的数量取决与分配给它的cpu个数 Cluter Manager：指的是在集群上获取资源的外部服务。目前有三种类型 Standalon : spark原生的资源管理，由Master负责资源的分配 Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架 Hadoop Yarn: 主要是指Yarn中的ResourceManager Worker: 集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slave文件配置的Worker节点，在Spark on Yarn模式下就是NoteManager节点 Task: 被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责 Job: 包含多个Task组成的并行计算，往往由Spark Action触发生成， 一个Application中往往会产生多个Job Stage: 每个Job会被拆分成多组Task， 作为一个TaskSet， 其名称为Stage，Stage的划分和调度是有DAGScheduler来负责的，Stage有非最终的Stage（Shuffle Map Stage）和最终的Stage（Result Stage）两种，Stage的边界就是发生shuffle的地方]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐算法]]></title>
    <url>%2F2018%2F12%2F05%2FMachineLearning%2F%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[推荐算法 基于内容的推荐算法 根据物品或内容的元数据，发现商品或内容的相关性，然后根据用户之前的喜好记录，推荐相似的物品 如：用户X购买过商品A，而A和B是相似的（iphone和小米手机），就可以把B推荐给X 协同过滤基于物品的协同过滤(Item-based Collaborative Filtering)首先从数据库里获取所有用户对物品的评价，根据评价，计算物品的相似度。然后从剩下的物品中找到和他历史兴趣近似的物品推荐给他。核心是要计算两个物品的相似度。基于用户的协同过滤(Item-based Collaborative Filtering)，基于用户的协同过滤推荐算法先使用统计技术寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。基本原理就是利用用户访问行为的相似性来互相推荐用户可能感兴趣的资源，区别： 可以注意到,基于物品的协同过滤和基于内容的推荐，两者的相同点都是要计算两个物品的相似度，但不同点是前者是根据两个物品被越多的人同时喜欢，这两个物品就越相似，而后者要根据物品的内容相似度来做推荐，给物品内容建模的方法很多，最著名的是向量空间模型，要计算两个向量的相似度。由此可以看到两种方法的不同点在于计算两个物品的相似度方法不同，一个根据外界环境计算，一个根据内容计算。 综上： 基于内容的推荐，只考虑了对象的本身性质，将对象按标签形成集合，如果你消费集合中的一个则向你推荐集合中的其他对象； 基于协调过滤，充分利用集体智慧，即在大量的人群的行为和数据中收集答案，以帮助我们对整个人群得到统计意义上的结论，推荐的个性化程度高，基于以下两个出发点： (1)兴趣相近的用户可能会对同样的东西感兴趣； (2)用户可能较偏爱与其已购买的东西相类似的商品。 也就是说考虑进了用户的历史习惯，对象客观上不一定相似，但由于人的行为可以认为其主观上是相似的，就可以产生推荐了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐算法</tag>
      </tags>
  </entry>
</search>
