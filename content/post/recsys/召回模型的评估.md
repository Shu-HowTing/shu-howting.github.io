+++
# author = "丁树浩"
title = "召回模型的评估"
date = "2023-07-02"
description = ""
tags = [
  "召回",
  "推荐"
]
categories = [
  "RecSys"
]
draft = false
math = true
+++


## 召回模型评测指标

### 为什么不用AUC指标

AUC指标不适用于衡量召回模型。原因有三：

- 计算AUC时，正样本容易获得，可以拿点击样本做正样本。但负样本从哪里来？照搬精排，用曝光未点击做负样本，行不行？不行。否则，测试样本都来自曝光物料，也就是从系统筛选过的、比较匹配用户爱好的优质物料，这样的测试数据明显与召回的实际应用场景（海量的、和用户毫不相关的物料）有着天壤之别。失真的测试环境只能产生失真的指标，不能反映召回模型的真实水平。（读到这里，细心的读者会意识到，其实粗排也面临类似的问题。严格来讲，凡是曝光过的样本，对粗排来说，也应该算正样本。尽管如此，在实践中，我们仍然拿点击当正样本，拿曝光未点击当负样本，计算GAUC来评估粗排模型。大家都认为，在流程上，粗排比召回离精排更近，因此靠精排的标准来严格要求粗排，也不算太离谱。）
- 那么靠召回结果中，除点击之外的其他物料当作负样本，行不行？假设我们为一个用户召回了三个物料，按召回模型的打分降序排列分别为A,B,C。历史记录显示只有C被该用户点击，算正样本。我们认为A和B是负样本，从而计算出AUC=0，是否合理？答案也是否定的。A、B未曾被用户点击，可能是因为它俩从未向用户曝光过，所以我们不能肯定用户就一定不喜欢它俩。把A、B当负样本不太靠谱。
- 即便我们能够证明用户真的不喜欢A和B，从而计算出AUC=0，难道我们就能得出该召回模型毫无价值的结论吗？答案仍然是否定的。毕竟召回算法找到了用户喜欢的物料C，确实发挥了作用。至于把C排名靠后，这一点根本不是问题。毕竟召回的顺序并非非要呈现给用户的顺序，召回的位置信息，筛选掉不召用户喜欢的A和B，那是粗排、精排的责任。

基于以上三点原因，在评估召回模型时，我们一般不再用AUC这样强调排序性能的指标，也避免直接统计负样本，而是从预测正样本与否的“命中率”，“覆盖度”视角出发来进行评估。


### 用召回率指标

![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/recall1.png)

![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/recall2.png)