+++
# author = "丁树浩"
title = "动态权重在推荐中的应用"
date = "2023-12-17"
description = ""
tags = [
  "rank",
  "推荐"
]
categories = [
  "RecSys"
]
draft = false
math = true
+++

## 动态权重

### 1. 从LHUC说起

语音识别领域2016年一项开创性工作提出了**LHUC**(Learning Hidden Unit Contribution)算法, 在DNN网络中为每个speaker学习对应的hidden unit contribution， 然后与common hidden layer相结合，以此提升不同speaker的语音识别准确率。这项工作属于domain adaptation领域，LHUC方法相比之前工作最重要的改进点是模型实现domain adaptation的过程不依赖target domain样本re-training，因此可以拓展到任意多个domain，之前方法由于re-training成本较高，仅适用于有限个domain。

![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/dw1.png)

## 2. LHUC和动态权重的关系

近几年国内一些头部公司将LHUC算法运用到推荐系统领域取得了不错的效果，下面选择一些代表性的工作进行介绍。论文中对LHUC思想有各种改进，称呼也不尽相同，如dynamic weights，adaptive parameter generation等，本文统一以动态权重指代这一类方法。

## 2.1 PPNet

![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/dw2.png)

需要注意一下蓝色部分的 Gate NN 部分，其中 uid，pid，aid 分别表示 user id，photo id，author id，进行梯度回传；但是左侧其他特征，虽然也会和这 3 个 id 特征的 embedding 拼接到一起作为所有 Gate NN 的输入，但并不接受 Gate NN 的反传梯度，**减少 Gate NN 对现有特征 embedding 收敛产生的影响**。

Gate NN 的数量同左侧神经网络层数一致 (上图中灰色网络共四层，因此一共有四个 Gate NN)，其输出同每一层神经网络的输入做 element-wise product 来做用户的个性化偏置。Gate NN 采用的是一个 2 层神经网络，其中第二层网络的激活函数是 $2 \* sigmoid$，这一部分与 LHUC 原文中设置是相同的。

### 2.1.1 动态权重思想的体现

1.  PPNet通过Gate NN结构达到增强用户个性化表达的能力。
    
2.  GateNN结构共两层，第二层网络的激活函数是**2 \* sigmoid**，默认值为1。
    
3.  GateNN的输入中额外增加了三个独有特征uid，pid，aid 分别表示 user id，photo id和author id。
    
4.  训练过程中左侧所有sparse特征**不接受**Gate NN 的反传梯度，这样操作的目的是减少 Gate NN 对现有特征 embedding 收敛产生的影响。
    

## 2.2 POSO

- MLP结构
![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/202407191114640.png)

- **MMoE结构**
    
![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/202407191121236.png)

## 2.3 PEPNet

![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/dw5.png)
**PEPNet和PPNet的结构类似。添加了对不同场景(Domain)的个性化特征**


## 2.4 阿里M2M

![](https://markdown-1258220306.cos.ap-shenzhen-fsi.myqcloud.com/img/dw6.png)

### 2.4.1 动态权重思想的体现

1. 场景scenario相关特征生成动态参数Weight和Bias。 
2. 模型输入input(同时包括场景相关特征和其它特征)直接使用上面的输出作为MLP的参数。与LHUC的一个小区别是，论文中没有直接采用场景相关动态权重从所有input生成的公共hidden layer中提取场景个性化信息的操作
    

**思考：**

LHUC和ppnet和POSO的区别？

和CAN的对比：

- CAN和DW针对的问题很像，都是针对“合不上，分不开”的问题
    
  - `合不上`：如果每个特征只有一套embedding，需要与其他所有embedding交叉，可能相互干扰。
        
    - 这和DW将所有场景数据合一起训练，面临的“模型被数据多的场景带偏”问题，很相似。
            
  - `分不开`：如果每对儿交叉特征都有自己独立的embedding，特征空间太稀疏不好训，而且也占用太多资源。
        
    - 这和DW为每个场景单独建模，面临的“数据少场景不好训、占用资源多、不好维护”问题，很相似。
            
- CAN和DW解决的方法很像
    
  - CAN把target item id/category embedding reshape成一个MLP，与user feature交叉时，就把user feature喂入这个dynamic generated MLP
        
  - DW利用“特征敏感”特征动态生成一个MLP，把其他所有特征喂入这个dynamic generated MLP
        

**Reference**

1. Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation 
2. [快手落地万亿参数推荐精排模型](https://www.51cto.com/article/644214.html)
3. POSO: Personalized Cold Start Modules for Large-scale Recommender Systems
4. [推荐系统难题挑战（7）：POSO，从模型角度解决用户冷启动问题](https://zhuanlan.zhihu.com/p/472726462)
5. [POSO方法的实际应用和分析思考](https://zhuanlan.zhihu.com/p/536499073)
6. [「2023 | 快手」PEPNet: 脱胎于LHUC的极致个性化](https://zhuanlan.zhihu.com/p/617478217)
7. [PEPNet: Parameter and Embedding Personalized Network for Infusing with Personalized Prior Information](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2302.01115.pdf)
8. Leaving No One Behind: A Multi-Scenario Multi-Task Meta Learning Approach for Advertiser Modeling